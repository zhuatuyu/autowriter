"""
ç»©æ•ˆåˆ†ææŠ¥å‘ŠçŸ¥è¯†å›¾è°±æœåŠ¡
ä¸“é—¨é’ˆå¯¹ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„çŸ¥è¯†å›¾è°±æ„å»ºå’ŒæŸ¥è¯¢

åŸºäºLlamaIndex KnowledgeGraphIndexï¼Œæ„å»ºé¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±ï¼š
- é¡¹ç›® -> æŒ‡æ ‡ä½“ç³» -> å…·ä½“æŒ‡æ ‡
- é¡¹ç›® -> è¡Œä¸šç±»å‹ -> æœ€ä½³å®è·µ
- æ”¿ç­–æ³•è§„ -> é€‚ç”¨é¡¹ç›®ç±»å‹
- æ¡ˆä¾‹ -> è§£å†³æ–¹æ¡ˆ -> æ”¹è¿›å»ºè®®
"""

from typing import List, Dict, Any, Optional
from pathlib import Path
import json

from metagpt.logs import logger
try:
    from llama_index.core import KnowledgeGraphIndex, Settings
    from llama_index.core.storage.storage_context import StorageContext
    from llama_index.core.graph_stores import SimpleGraphStore
    from llama_index.core.query_engine import KnowledgeGraphQueryEngine
    KG_AVAILABLE = True
    logger.info("âœ… çŸ¥è¯†å›¾è°±ä¾èµ–åŠ è½½æˆåŠŸ")
except ImportError as e:
    KG_AVAILABLE = False
    logger.warning(f"âš ï¸ çŸ¥è¯†å›¾è°±ä¾èµ–ä¸å¯ç”¨: {e}ï¼Œå°†ä½¿ç”¨ä¼ ç»ŸRAGæ£€ç´¢")

from .hybrid_search import hybrid_search
from metagpt.config2 import Config
from pathlib import Path

# é…ç½®é©±åŠ¨
from backend.config.global_prompts import (
    QUERY_INTENT_MAPPING,
    KG_CONF,
    KG_ENTITY_TYPES,
    KG_RELATION_TYPES,
)


class PerformanceKnowledgeGraph:
    """ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçŸ¥è¯†å›¾è°±"""
    
    def __init__(self):
        self._kg_index = None
        self._kg_storage_path = "workspace/vector_storage/global_graph"
        # ğŸ¯ ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„å®ä½“/å…³ç³»ï¼ˆå¸¸é‡åŒ–é»˜è®¤é›†ï¼‰
        # ä»å…¨å±€å¸¸é‡è¯»å–ï¼ˆå¯åœ¨ global_prompts ä¸­è°ƒæ•´ï¼‰
        self.entity_types = KG_ENTITY_TYPES
        self.relation_types = KG_RELATION_TYPES
    
    async def build_knowledge_graph(self, project_vector_storage_path: str) -> bool:
        """
        æ„å»ºç»©æ•ˆåˆ†ææŠ¥å‘ŠçŸ¥è¯†å›¾è°±
        
        Args:
            project_vector_storage_path: é¡¹ç›®çŸ¥è¯†åº“è·¯å¾„
        """
        if not KG_AVAILABLE:
            logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡æ„å»º")
            return False
            
        try:
            logger.info("ğŸ§  å¼€å§‹æ„å»ºç»©æ•ˆåˆ†ææŠ¥å‘ŠçŸ¥è¯†å›¾è°±...")
            
            # 1. æ”¶é›†æ–‡æ¡£
            documents = self._collect_documents(project_vector_storage_path)
            if not documents:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºçŸ¥è¯†å›¾è°±")
                return False
            
            # 2. åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = self._create_storage_context()
            
            # 3. è®¾ç½®ä¸“ç”¨çš„çŸ¥è¯†å›¾è°±LLMå’Œembeddingæ¨¡å‹
            llm, embed_model = self._create_kg_llm_and_embed_model()
            Settings.llm = llm
            Settings.embed_model = embed_model
            Settings.chunk_size = 512  # çŸ¥è¯†å›¾è°±é€‚åˆè¾ƒå°çš„chunk
            
            logger.info(f"ğŸ§  çŸ¥è¯†å›¾è°±ä½¿ç”¨LLM: {type(llm).__name__}")
            logger.info(f"ğŸ§  çŸ¥è¯†å›¾è°±ä½¿ç”¨Embedding: {type(embed_model).__name__}")
            logger.info(f"ğŸ”‘ LLM API Key: {llm.api_key[:8]}...")
            logger.info(f"ğŸ¤– LLM Model: {getattr(llm, 'model', getattr(llm, 'model_name', 'Unknown'))}")
            
            # 4. æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•
            self._kg_index = KnowledgeGraphIndex.from_documents(
                documents=documents,
                storage_context=storage_context,
                max_triplets_per_chunk=10,  # æ¯ä¸ªchunkæœ€å¤šæå–10ä¸ªä¸‰å…ƒç»„
                show_progress=True,
            )
            
            # 5. ä¿å­˜çŸ¥è¯†å›¾è°±
            self._kg_index.storage_context.persist(self._kg_storage_path)
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {self._kg_storage_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return False
    
    async def query_knowledge_graph(
        self, 
        query: str, 
        mode: str = "keyword",
        max_knowledge_sequence: int = 5
    ) -> str:
        """
        ğŸ§  æ™ºèƒ½æ¨ç†å¼æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        
        Args:
            query: æŸ¥è¯¢é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ ("keyword", "embedding", "keyword_embedding") - æ³¨æ„ï¼šä¸æ”¯æŒ"hybrid"
            max_knowledge_sequence: æœ€å¤§çŸ¥è¯†åºåˆ—é•¿åº¦
        """
        if not KG_AVAILABLE:
            logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±åŠŸèƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°å‘é‡æ£€ç´¢")
            # é™çº§åˆ°ä¼ ç»ŸRAGæ£€ç´¢
            from .hybrid_search import hybrid_search
            results = await hybrid_search.hybrid_search(
                query=query,
                project_vector_storage_path="",
                enable_global=True,
                global_top_k=max_knowledge_sequence
            )
            return "\n".join(results) if results else "æ— å¯ç”¨ç»“æœ"
            
        try:
            if not self._kg_index:
                logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±æœªæ„å»ºï¼Œå°è¯•åŠ è½½...")
                if not await self._load_knowledge_graph():
                    return "çŸ¥è¯†å›¾è°±ä¸å¯ç”¨ï¼Œè¯·å…ˆæ„å»ºçŸ¥è¯†å›¾è°±"
            
            # åˆ›å»ºçŸ¥è¯†å›¾è°±æŸ¥è¯¢å¼•æ“ - é…ç½®é©±åŠ¨ï¼ˆæ”¯æŒ kg_profiles æŒ‰æ„å›¾åˆ‡æ¡£ï¼‰
            retriever_mode = "keyword"
            response_mode = "tree_summarize"
            similarity_top_k_cfg = max_knowledge_sequence
            # ç®€åŒ–ï¼šä½¿ç”¨é»˜è®¤ retriever/response é…ç½®ï¼Œå¯æŒ‰éœ€è¦æ‰©å±•


            kg_query_engine = self._kg_index.as_query_engine(
                include_text=True,
                retriever_mode=retriever_mode,  # "keyword", "embedding"
                response_mode=response_mode,
                similarity_top_k=similarity_top_k_cfg,
                verbose=True,
            )
            
            # ğŸ§  æ™ºèƒ½æŸ¥è¯¢å¢å¼º - æ ¹æ®é¢†åŸŸç‰¹ç‚¹ä¼˜åŒ–æŸ¥è¯¢
            enhanced_query = await self._enhance_domain_query(query)

            # ğŸš¦ å¢å¼ºåå†æ¬¡é™è¯ï¼ˆå¯é…ç½®ï¼‰ï¼Œé¿å…å¢å¼ºæ–‡æœ¬å¼•å…¥è¿‡å¤šå…³é”®è¯å¯¼è‡´KGå†—é•¿æ¨ç†
            # ä»å…¨å±€é…ç½®è¯»å–å¢å¼ºåé™è¯ç­–ç•¥
            try:
                limit_after = bool(KG_CONF.get("limit_keywords_after_enhance", True))
                max_after = int(KG_CONF.get("max_keywords_after_enhance", 5))
            except Exception:
                limit_after, max_after = True, 5
            if limit_after and max_after > 0:
                enhanced_query = self._limit_keywords_in_text(enhanced_query, max_after)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = await kg_query_engine.aquery(enhanced_query)
            
            # ğŸ§  åå¤„ç† - æå–å…³ç³»æ¨ç†ç»“æœ
            enhanced_response = await self._post_process_kg_response(str(response), query)
            
            logger.info(f"ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†æŸ¥è¯¢å®Œæˆ: {query}")
            return enhanced_response
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æŸ¥è¯¢å¤±è´¥: {e}")
            return f"æŸ¥è¯¢å¤±è´¥: {e}"

    def _limit_keywords_in_text(self, text: str, max_keywords: int) -> str:
        """å°†æ–‡æœ¬æŒ‰ç©ºç™½/æ ‡ç‚¹åˆ‡åˆ†åå–å‰Nä¸ªæ ‡è®°å†æ‹¼æ¥ï¼Œæ§åˆ¶æŸ¥è¯¢é•¿åº¦ã€‚"""
        if not text or max_keywords <= 0:
            return text
        import re
        tokens = [t for t in re.split(r"[\s,;ï¼Œï¼›ã€‚.!ï¼ï¼Ÿã€]+", text) if t]
        if len(tokens) <= max_keywords:
            return text
        return " ".join(tokens[:max_keywords])
    
    async def _enhance_domain_query(self, query: str) -> str:
        """ğŸ¯ åŸºäºé…ç½®çš„æŸ¥è¯¢å¢å¼ºï¼šä½¿ç”¨ intelligent_search.query_intent_mapping åŠ¨æ€åˆ†ææ„å›¾"""
        intents = self._analyze_intents_by_config(query)
        if not intents:
            return query
        context = self._build_intent_context(intents)
        # æç¤ºå®ä½“ç±»å‹ä¸å…³ç³»ç±»å‹ï¼Œåˆ©äºKGæ¨ç†
        entity_type_list = ", ".join(list(self.entity_types.keys())[:8])
        relation_type_list = ", ".join(self.relation_types[:8])
        preface = (
            f"è¯·ç»“åˆé¢†åŸŸçŸ¥è¯†å›¾è°±è¿›è¡Œå›ç­”ï¼›ä¼˜å…ˆä½¿ç”¨å·²çŸ¥å®ä½“ç±»å‹ï¼ˆ{entity_type_list} ç­‰ï¼‰ä¸å…³ç³»ï¼ˆ{relation_type_list} ç­‰ï¼‰è¿›è¡Œæ¨ç†ä¸å¼•ç”¨ã€‚\n"
        )
        return f"{context}\n{preface}{query}"

    def _analyze_intents_by_config(self, query: str) -> List[str]:
        """æ ¹æ®é…ç½®çš„æ„å›¾å…³é”®è¯æ˜ å°„åˆ†ææŸ¥è¯¢æ„å›¾"""
        try:
            mapping: Dict[str, List[str]] = QUERY_INTENT_MAPPING or {}
            matched: List[str] = []
            q = query or ""
            for intent, keywords in mapping.items():
                if not isinstance(keywords, list):
                    continue
                if any(kw for kw in keywords if kw and kw in q):
                    matched.append(intent)
            return matched
        except Exception:
            return []

    def _build_intent_context(self, intents: List[str]) -> str:
        """æŠŠæ„å›¾æ˜ å°„ä¸ºé¢†åŸŸä¸Šä¸‹æ–‡è¯´æ˜ï¼Œé¿å…ç¡¬ç¼–ç  if/elseï¼Œé‡‡ç”¨æ„å›¾åˆ°æ¨¡æ¿çš„æ˜ å°„"""
        intent_to_template: Dict[str, str] = {
            "policy": "å›´ç»•æ”¿ç­–/æ³•è§„æ¡æ¬¾è¿›è¡Œå›ç­”ï¼Œæ³¨æ˜æ¡æ¬¾å‡ºå¤„ï¼Œé˜æ˜é€‚ç”¨èŒƒå›´ä¸åˆè§„è¦æ±‚ã€‚",
            "method": "å›´ç»•è¯„ä»·/åˆ†ææ–¹æ³•ä¸æ­¥éª¤ï¼ˆå¦‚AHPã€è®¡åˆ†è§„åˆ™ï¼‰è¿›è¡Œè¯´æ˜ï¼Œå¹¶ç»™å‡ºå¯å¤ç”¨çš„æ“ä½œè·¯å¾„ã€‚",
            "case": "å›´ç»•ç›¸ä¼¼é¡¹ç›®/æ¡ˆä¾‹è¿›è¡Œå¯¹æ ‡ï¼Œæ€»ç»“å…³é”®åšæ³•ä¸æˆæ•ˆï¼Œæä¾›å¯å¼•ç”¨çš„è¯æ®æ¥æºã€‚",
            "metric": "å›´ç»•ç»©æ•ˆæŒ‡æ ‡ä½“ç³»è¿›è¡Œå›ç­”ï¼Œä¼˜å…ˆæŒ‰å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šå››ä¸ªç»´åº¦ç»„ç»‡ï¼Œæ˜ç¡®è¯„ä»·è¦ç‚¹ä¸è®¡åˆ†æ–¹æ³•ã€‚",
            # å…è®¸é€ä¼ æœªçŸ¥æ„å›¾
        }
        parts = [intent_to_template.get(i, f"å›´ç»•{i} ç›¸å…³çŸ¥è¯†è¿›è¡Œè¯´æ˜ï¼Œå¹¶æä¾›è¯æ®ä¸å¯æ‰§è¡Œè·¯å¾„ã€‚") for i in intents]
        return "\n".join(parts)
    
    async def _post_process_kg_response(self, response: str, original_query: str) -> str:
        """ğŸ§  çŸ¥è¯†å›¾è°±å“åº”çš„æ™ºèƒ½åå¤„ç†"""
        # æ·»åŠ æ¨ç†è·¯å¾„è¯´æ˜
        enhanced_response = f"### ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†ç»“æœ\n\n{response}\n\n"
        
        # å°è¯•æå–å®ä½“å…³ç³»
        entities = self.extract_domain_entities(response)
        if entities:
            enhanced_response += "### ğŸ•¸ï¸ å‘ç°çš„å®ä½“å…³ç³»\n"
            for entity_type, entity_list in entities.items():
                enhanced_response += f"- **{entity_type}**: {', '.join(entity_list)}\n"
        
        # ç”Ÿæˆé¢†åŸŸæ´å¯Ÿ
        insights = self.generate_performance_insights(entities)
        if insights:
            enhanced_response += "\n### ğŸ’¡ ç»©æ•ˆåˆ†ææ´å¯Ÿ\n"
            for insight in insights:
                enhanced_response += f"- {insight}\n"
        
        return enhanced_response
    
    def extract_domain_entities(self, text: str) -> Dict[str, List[str]]:
        """
        æå–ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„ç‰¹å®šå®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            Dict[å®ä½“ç±»å‹, å®ä½“åˆ—è¡¨]
        """
        entities = {}
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨NERæ¨¡å‹ï¼‰
        for entity_type, keywords in self.entity_types.items():
            found_entities = []
            for keyword in keywords:
                if keyword in text:
                    found_entities.append(keyword)
            if found_entities:
                entities[entity_type] = found_entities
        
        return entities
    
    def generate_performance_insights(self, entities: Dict[str, List[str]]) -> List[str]:
        """
        åŸºäºæå–çš„å®ä½“ç”Ÿæˆç»©æ•ˆåˆ†ææ´å¯Ÿ
        
        Args:
            entities: æå–çš„å®ä½“å­—å…¸
            
        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        insights = []
        
        # ç”Ÿæˆé¢†åŸŸç‰¹å®šçš„æ´å¯Ÿ
        if "é¡¹ç›®" in entities and "æŒ‡æ ‡ä½“ç³»" in entities:
            insights.append("ğŸ¯ è¯¥é¡¹ç›®åº”å»ºç«‹å®Œæ•´çš„ç»©æ•ˆæŒ‡æ ‡ä½“ç³»ï¼Œæ¶µç›–å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šå››ä¸ªç»´åº¦")
        
        if "æ”¿ç­–æ³•è§„" in entities:
            insights.append("ğŸ“‹ é¡¹ç›®å®æ–½åº”ä¸¥æ ¼éµå¾ªç›¸å…³æ”¿ç­–æ³•è§„è¦æ±‚")
        
        if "æœ€ä½³å®è·µ" in entities:
            insights.append("â­ å¯å€Ÿé‰´ç›¸å…³æœ€ä½³å®è·µç»éªŒï¼Œæå‡é¡¹ç›®å®æ–½æ•ˆæœ")
        
        return insights
    
    def _create_kg_llm_and_embed_model(self):
        """
        ğŸ¯ åˆ›å»ºçŸ¥è¯†å›¾è°±ä¸“ç”¨çš„LLMå’ŒåµŒå…¥æ¨¡å‹
        ä½¿ç”¨config2.yamlä¸­çš„knowledge_graphé…ç½®
        """
        import yaml
        # åªè¯»å–æˆ‘ä»¬åº”ç”¨ä¾§çš„ config/config2.yamlï¼Œé¿å… example/MetaGPT_bak ç”Ÿæ•ˆ
        with open('config/config2.yaml', 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)
        
        kg_config = yaml_config.get('knowledge_graph', {})
        embed_config = yaml_config.get('embedding', {})
        
        # ğŸ”§ æŒ‰ç…§é˜¿é‡Œäº‘å®˜æ–¹æ–‡æ¡£ä½¿ç”¨OpenAI-Likeæ–¹å¼åˆ›å»ºå›¾è°±ä¸“ç”¨LLM
        from llama_index.embeddings.dashscope import DashScopeEmbedding
        from llama_index.llms.openai_like import OpenAILike
        
        # åˆ›å»ºçŸ¥è¯†å›¾è°±ä¸“ç”¨LLM - ä½¿ç”¨qwen-flashå¿«é€Ÿä½æˆæœ¬æ¨¡å‹
        kg_llm = OpenAILike(
            model=kg_config.get('model', 'qwen-flash'),  # qwen-flash
            api_base=kg_config.get('base_url', 'https://dashscope.aliyuncs.com/compatible-mode/v1'),
            api_key=kg_config.get('api_key', ''),
            is_chat_model=True
        )
        
        # åˆ›å»ºEmbeddingæ¨¡å‹ - å¤ç”¨embeddingé…ç½®
        embed_model = DashScopeEmbedding(
            model_name=embed_config.get('model', 'text-embedding-v3'),  # text-embedding-v3
            api_key=embed_config.get('api_key', ''),
            dashscope_api_key=embed_config.get('api_key', '')
        )
        embed_model.embed_batch_size = 8
        
        return kg_llm, embed_model
    
    def _collect_documents(self, project_vector_storage_path: str) -> List[Any]:
        """æ”¶é›†é¡¹ç›®æ–‡æ¡£"""
        from llama_index.core import Document
        
        documents = []
        project_path = Path(project_vector_storage_path)
        
        if not project_path.exists():
            return documents
        
        # æ”¶é›†æ‰€æœ‰.mdå’Œ.txtæ–‡ä»¶
        for pattern in ["*.md", "*.txt"]:
            for file_path in project_path.glob(pattern):
                try:
                    content = file_path.read_text(encoding='utf-8')
                    doc = Document(text=content, metadata={"filename": file_path.name})
                    documents.append(doc)
                except Exception as e:
                    logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"ğŸ“š æ”¶é›†åˆ° {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±")
        return documents
    
    def _create_storage_context(self):
        """åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡"""
        if not KG_AVAILABLE:
            return None
            
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        Path(self._kg_storage_path).mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå›¾å­˜å‚¨
        graph_store = SimpleGraphStore()
        storage_context = StorageContext.from_defaults(graph_store=graph_store)
        
        return storage_context
    
    def _configure_settings(self):
        """é…ç½®å…¨å±€è®¾ç½®"""
        # ä½¿ç”¨ä¸hybrid_searchç›¸åŒçš„LLMå’Œembeddingé…ç½®
        llm, embed_model = hybrid_search._create_llm_and_embed_model()
        Settings.llm = llm
        Settings.embed_model = embed_model
        Settings.chunk_size = 512
    
    async def _load_knowledge_graph(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†å›¾è°±"""
        if not KG_AVAILABLE:
            return False
            
        try:
            if not Path(self._kg_storage_path).exists():
                return False
            
            # é…ç½®å…¨å±€è®¾ç½®
            self._configure_settings()
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(persist_dir=self._kg_storage_path)
            
            # åŠ è½½çŸ¥è¯†å›¾è°± - ä¿®å¤åŠ è½½æ–¹æ³•
            from llama_index.core import load_index_from_storage
            self._kg_index = load_index_from_storage(
                storage_context=storage_context,
            )
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸ: {self._kg_storage_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return False


# å…¨å±€å®ä¾‹
performance_kg = PerformanceKnowledgeGraph()