"""
ç»©æ•ˆåˆ†ææŠ¥å‘ŠçŸ¥è¯†å›¾è°±æœåŠ¡
ä¸“é—¨é’ˆå¯¹ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„çŸ¥è¯†å›¾è°±æ„å»ºå’ŒæŸ¥è¯¢

åŸºäºLlamaIndex KnowledgeGraphIndexï¼Œæ„å»ºé¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±ï¼š
- é¡¹ç›® -> æŒ‡æ ‡ä½“ç³» -> å…·ä½“æŒ‡æ ‡
- é¡¹ç›® -> è¡Œä¸šç±»å‹ -> æœ€ä½³å®è·µ
- æ”¿ç­–æ³•è§„ -> é€‚ç”¨é¡¹ç›®ç±»å‹
- æ¡ˆä¾‹ -> è§£å†³æ–¹æ¡ˆ -> æ”¹è¿›å»ºè®®
"""

from typing import List, Dict, Any, Optional
from pathlib import Path
import json

from metagpt.logs import logger
try:
    from llama_index.core import KnowledgeGraphIndex, Settings
    from llama_index.core.storage.storage_context import StorageContext
    from llama_index.core.graph_stores import SimpleGraphStore
    from llama_index.core.query_engine import KnowledgeGraphQueryEngine
    KG_AVAILABLE = True
    logger.info("âœ… çŸ¥è¯†å›¾è°±ä¾èµ–åŠ è½½æˆåŠŸ")
except ImportError as e:
    KG_AVAILABLE = False
    logger.warning(f"âš ï¸ çŸ¥è¯†å›¾è°±ä¾èµ–ä¸å¯ç”¨: {e}ï¼Œå°†ä½¿ç”¨ä¼ ç»ŸRAGæ£€ç´¢")

from .hybrid_search import hybrid_search
from metagpt.config2 import Config
from pathlib import Path


class PerformanceKnowledgeGraph:
    """ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçŸ¥è¯†å›¾è°±"""
    
    def __init__(self):
        self._kg_index = None
        self._kg_storage_path = "workspace/vector_storage/global_graph"
        
        # ğŸ¯ ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„å®ä½“ç±»å‹å®šä¹‰
        self.entity_types = {
            "é¡¹ç›®": ["é¡¹ç›®åç§°", "é¡¹ç›®ç±»å‹", "å®æ–½åœ°ç‚¹", "èµ„é‡‘è§„æ¨¡"],
            "æŒ‡æ ‡ä½“ç³»": ["å†³ç­–æŒ‡æ ‡", "è¿‡ç¨‹æŒ‡æ ‡", "äº§å‡ºæŒ‡æ ‡", "æ•ˆç›ŠæŒ‡æ ‡"],
            "å…·ä½“æŒ‡æ ‡": ["æŒ‡æ ‡åç§°", "è®¡ç®—æ–¹æ³•", "ç›®æ ‡å€¼", "æƒé‡"],
            "æ”¿ç­–æ³•è§„": ["æ³•è§„åç§°", "é€‚ç”¨èŒƒå›´", "å‘å¸ƒæœºæ„", "ç”Ÿæ•ˆæ—¶é—´"],
            "æœ€ä½³å®è·µ": ["å®è·µåç§°", "é€‚ç”¨åœºæ™¯", "å®æ–½è¦ç‚¹", "é¢„æœŸæ•ˆæœ"],
            "é—®é¢˜æ¡ˆä¾‹": ["é—®é¢˜ç±»å‹", "åŸå› åˆ†æ", "è§£å†³æ–¹æ¡ˆ", "æ”¹è¿›å»ºè®®"],
            "è¡Œä¸šç±»å‹": ["åŸºç¡€è®¾æ–½", "å…¬ç›Šäº‹ä¸š", "æ°‘ç”Ÿä¿éšœ", "ç¯å¢ƒæ²»ç†"],
        }
        
        # ğŸ¯ å…³ç³»ç±»å‹å®šä¹‰
        self.relation_types = [
            "åŒ…å«", "å±äº", "é€‚ç”¨äº", "éµå¾ª", "å‚è€ƒ",
            "å¯¼è‡´", "è§£å†³", "æ”¹è¿›", "å…³è”", "å½±å“"
        ]
    
    async def build_knowledge_graph(self, project_vector_storage_path: str) -> bool:
        """
        æ„å»ºç»©æ•ˆåˆ†ææŠ¥å‘ŠçŸ¥è¯†å›¾è°±
        
        Args:
            project_vector_storage_path: é¡¹ç›®çŸ¥è¯†åº“è·¯å¾„
        """
        if not KG_AVAILABLE:
            logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡æ„å»º")
            return False
            
        try:
            logger.info("ğŸ§  å¼€å§‹æ„å»ºç»©æ•ˆåˆ†ææŠ¥å‘ŠçŸ¥è¯†å›¾è°±...")
            
            # 1. æ”¶é›†æ–‡æ¡£
            documents = self._collect_documents(project_vector_storage_path)
            if not documents:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºçŸ¥è¯†å›¾è°±")
                return False
            
            # 2. åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = self._create_storage_context()
            
            # 3. è®¾ç½®ä¸“ç”¨çš„çŸ¥è¯†å›¾è°±LLMå’Œembeddingæ¨¡å‹
            llm, embed_model = self._create_kg_llm_and_embed_model()
            Settings.llm = llm
            Settings.embed_model = embed_model
            Settings.chunk_size = 512  # çŸ¥è¯†å›¾è°±é€‚åˆè¾ƒå°çš„chunk
            
            logger.info(f"ğŸ§  çŸ¥è¯†å›¾è°±ä½¿ç”¨LLM: {type(llm).__name__}")
            logger.info(f"ğŸ§  çŸ¥è¯†å›¾è°±ä½¿ç”¨Embedding: {type(embed_model).__name__}")
            logger.info(f"ğŸ”‘ LLM API Key: {llm.api_key[:8]}...")
            logger.info(f"ğŸ¤– LLM Model: {getattr(llm, 'model', getattr(llm, 'model_name', 'Unknown'))}")
            
            # 4. æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•
            self._kg_index = KnowledgeGraphIndex.from_documents(
                documents=documents,
                storage_context=storage_context,
                max_triplets_per_chunk=10,  # æ¯ä¸ªchunkæœ€å¤šæå–10ä¸ªä¸‰å…ƒç»„
                show_progress=True,
            )
            
            # 5. ä¿å­˜çŸ¥è¯†å›¾è°±
            self._kg_index.storage_context.persist(self._kg_storage_path)
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {self._kg_storage_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return False
    
    async def query_knowledge_graph(
        self, 
        query: str, 
        mode: str = "keyword",
        max_knowledge_sequence: int = 5
    ) -> str:
        """
        ğŸ§  æ™ºèƒ½æ¨ç†å¼æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        
        Args:
            query: æŸ¥è¯¢é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ ("keyword", "embedding", "keyword_embedding") - æ³¨æ„ï¼šä¸æ”¯æŒ"hybrid"
            max_knowledge_sequence: æœ€å¤§çŸ¥è¯†åºåˆ—é•¿åº¦
        """
        if not KG_AVAILABLE:
            logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±åŠŸèƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°å‘é‡æ£€ç´¢")
            # é™çº§åˆ°ä¼ ç»ŸRAGæ£€ç´¢
            from .hybrid_search import hybrid_search
            results = await hybrid_search.hybrid_search(
                query=query,
                project_vector_storage_path="",
                enable_global=True,
                global_top_k=max_knowledge_sequence
            )
            return "\n".join(results) if results else "æ— å¯ç”¨ç»“æœ"
            
        try:
            if not self._kg_index:
                logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±æœªæ„å»ºï¼Œå°è¯•åŠ è½½...")
                if not await self._load_knowledge_graph():
                    return "çŸ¥è¯†å›¾è°±ä¸å¯ç”¨ï¼Œè¯·å…ˆæ„å»ºçŸ¥è¯†å›¾è°±"
            
            # åˆ›å»ºçŸ¥è¯†å›¾è°±æŸ¥è¯¢å¼•æ“ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
            kg_query_engine = self._kg_index.as_query_engine(
                include_text=True,
                retriever_mode="keyword",  # "keyword", "embedding", "keyword_embedding"
                response_mode="tree_summarize",  # ä½¿ç”¨å®˜æ–¹æ¨èçš„å“åº”æ¨¡å¼
                similarity_top_k=max_knowledge_sequence,
                verbose=True,
            )
            
            # ğŸ§  æ™ºèƒ½æŸ¥è¯¢å¢å¼º - æ ¹æ®é¢†åŸŸç‰¹ç‚¹ä¼˜åŒ–æŸ¥è¯¢
            enhanced_query = await self._enhance_domain_query(query)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = await kg_query_engine.aquery(enhanced_query)
            
            # ğŸ§  åå¤„ç† - æå–å…³ç³»æ¨ç†ç»“æœ
            enhanced_response = await self._post_process_kg_response(str(response), query)
            
            logger.info(f"ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†æŸ¥è¯¢å®Œæˆ: {query}")
            return enhanced_response
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æŸ¥è¯¢å¤±è´¥: {e}")
            return f"æŸ¥è¯¢å¤±è´¥: {e}"
    
    async def _enhance_domain_query(self, query: str) -> str:
        """ğŸ¯ ç»©æ•ˆåˆ†æé¢†åŸŸçš„æŸ¥è¯¢å¢å¼º"""
        # æ£€æµ‹æŸ¥è¯¢ç±»å‹å¹¶æ·»åŠ é¢†åŸŸç‰¹å®šçš„ä¸Šä¸‹æ–‡
        if any(keyword in query for keyword in ["æŒ‡æ ‡", "è¯„ä»·", "ç»©æ•ˆ"]):
            return f"åœ¨ç»©æ•ˆåˆ†æè¯„ä»·ä½“ç³»ä¸­ï¼Œ{query}ã€‚è¯·é‡ç‚¹å…³æ³¨å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šå››ä¸ªç»´åº¦çš„ç›¸å…³ä¿¡æ¯ã€‚"
        elif any(keyword in query for keyword in ["é¡¹ç›®", "å®æ–½", "ç®¡ç†"]):
            return f"å…³äºé¡¹ç›®å®æ–½å’Œç®¡ç†ï¼Œ{query}ã€‚è¯·åˆ†æé¡¹ç›®èƒŒæ™¯ã€å®æ–½è¿‡ç¨‹ã€ç»„ç»‡ç®¡ç†ç­‰æ–¹é¢çš„ä¿¡æ¯ã€‚"
        elif any(keyword in query for keyword in ["é—®é¢˜", "é£é™©", "æŒ‘æˆ˜"]):
            return f"é’ˆå¯¹é¡¹ç›®é£é™©å’Œé—®é¢˜ï¼Œ{query}ã€‚è¯·è¯†åˆ«æ½œåœ¨é£é™©å› ç´ åŠå…¶å½±å“ã€‚"
        elif any(keyword in query for keyword in ["å»ºè®®", "æ”¹è¿›", "ä¼˜åŒ–"]):
            return f"å…³äºæ”¹è¿›å»ºè®®ï¼Œ{query}ã€‚è¯·ç»“åˆæœ€ä½³å®è·µå’ŒæˆåŠŸæ¡ˆä¾‹æä¾›å»ºè®®ã€‚"
        else:
            return query
    
    async def _post_process_kg_response(self, response: str, original_query: str) -> str:
        """ğŸ§  çŸ¥è¯†å›¾è°±å“åº”çš„æ™ºèƒ½åå¤„ç†"""
        # æ·»åŠ æ¨ç†è·¯å¾„è¯´æ˜
        enhanced_response = f"### ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†ç»“æœ\n\n{response}\n\n"
        
        # å°è¯•æå–å®ä½“å…³ç³»
        entities = self.extract_domain_entities(response)
        if entities:
            enhanced_response += "### ğŸ•¸ï¸ å‘ç°çš„å®ä½“å…³ç³»\n"
            for entity_type, entity_list in entities.items():
                enhanced_response += f"- **{entity_type}**: {', '.join(entity_list)}\n"
        
        # ç”Ÿæˆé¢†åŸŸæ´å¯Ÿ
        insights = self.generate_performance_insights(entities)
        if insights:
            enhanced_response += "\n### ğŸ’¡ ç»©æ•ˆåˆ†ææ´å¯Ÿ\n"
            for insight in insights:
                enhanced_response += f"- {insight}\n"
        
        return enhanced_response
    
    def extract_domain_entities(self, text: str) -> Dict[str, List[str]]:
        """
        æå–ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„ç‰¹å®šå®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            Dict[å®ä½“ç±»å‹, å®ä½“åˆ—è¡¨]
        """
        entities = {}
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨NERæ¨¡å‹ï¼‰
        for entity_type, keywords in self.entity_types.items():
            found_entities = []
            for keyword in keywords:
                if keyword in text:
                    found_entities.append(keyword)
            if found_entities:
                entities[entity_type] = found_entities
        
        return entities
    
    def generate_performance_insights(self, entities: Dict[str, List[str]]) -> List[str]:
        """
        åŸºäºæå–çš„å®ä½“ç”Ÿæˆç»©æ•ˆåˆ†ææ´å¯Ÿ
        
        Args:
            entities: æå–çš„å®ä½“å­—å…¸
            
        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        insights = []
        
        # ç”Ÿæˆé¢†åŸŸç‰¹å®šçš„æ´å¯Ÿ
        if "é¡¹ç›®" in entities and "æŒ‡æ ‡ä½“ç³»" in entities:
            insights.append("ğŸ¯ è¯¥é¡¹ç›®åº”å»ºç«‹å®Œæ•´çš„ç»©æ•ˆæŒ‡æ ‡ä½“ç³»ï¼Œæ¶µç›–å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šå››ä¸ªç»´åº¦")
        
        if "æ”¿ç­–æ³•è§„" in entities:
            insights.append("ğŸ“‹ é¡¹ç›®å®æ–½åº”ä¸¥æ ¼éµå¾ªç›¸å…³æ”¿ç­–æ³•è§„è¦æ±‚")
        
        if "æœ€ä½³å®è·µ" in entities:
            insights.append("â­ å¯å€Ÿé‰´ç›¸å…³æœ€ä½³å®è·µç»éªŒï¼Œæå‡é¡¹ç›®å®æ–½æ•ˆæœ")
        
        return insights
    
    def _create_kg_llm_and_embed_model(self):
        """
        ğŸ¯ åˆ›å»ºçŸ¥è¯†å›¾è°±ä¸“ç”¨çš„LLMå’ŒåµŒå…¥æ¨¡å‹
        ä½¿ç”¨config2.yamlä¸­çš„knowledge_graphé…ç½®
        """
        import yaml
        
        # ç›´æ¥è¯»å–YAMLæ–‡ä»¶æ¥è·å–knowledge_graphé…ç½®
        with open('config/config2.yaml', 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)
        
        kg_config = yaml_config.get('knowledge_graph', {})
        embed_config = yaml_config.get('embedding', {})
        
        # ğŸ”§ æŒ‰ç…§é˜¿é‡Œäº‘å®˜æ–¹æ–‡æ¡£ä½¿ç”¨OpenAI-Likeæ–¹å¼åˆ›å»ºå›¾è°±ä¸“ç”¨LLM
        from llama_index.embeddings.dashscope import DashScopeEmbedding
        from llama_index.llms.openai_like import OpenAILike
        
        # åˆ›å»ºçŸ¥è¯†å›¾è°±ä¸“ç”¨LLM - ä½¿ç”¨qwen-flashå¿«é€Ÿä½æˆæœ¬æ¨¡å‹
        kg_llm = OpenAILike(
            model=kg_config.get('model', 'qwen-flash'),  # qwen-flash
            api_base=kg_config.get('base_url', 'https://dashscope.aliyuncs.com/compatible-mode/v1'),
            api_key=kg_config.get('api_key', ''),
            is_chat_model=True
        )
        
        # åˆ›å»ºEmbeddingæ¨¡å‹ - å¤ç”¨embeddingé…ç½®
        embed_model = DashScopeEmbedding(
            model_name=embed_config.get('model', 'text-embedding-v3'),  # text-embedding-v3
            api_key=embed_config.get('api_key', ''),
            dashscope_api_key=embed_config.get('api_key', '')
        )
        embed_model.embed_batch_size = 8
        
        return kg_llm, embed_model
    
    def _collect_documents(self, project_vector_storage_path: str) -> List[Any]:
        """æ”¶é›†é¡¹ç›®æ–‡æ¡£"""
        from llama_index.core import Document
        
        documents = []
        project_path = Path(project_vector_storage_path)
        
        if not project_path.exists():
            return documents
        
        # æ”¶é›†æ‰€æœ‰.mdå’Œ.txtæ–‡ä»¶
        for pattern in ["*.md", "*.txt"]:
            for file_path in project_path.glob(pattern):
                try:
                    content = file_path.read_text(encoding='utf-8')
                    doc = Document(text=content, metadata={"filename": file_path.name})
                    documents.append(doc)
                except Exception as e:
                    logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"ğŸ“š æ”¶é›†åˆ° {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±")
        return documents
    
    def _create_storage_context(self):
        """åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡"""
        if not KG_AVAILABLE:
            return None
            
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        Path(self._kg_storage_path).mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå›¾å­˜å‚¨
        graph_store = SimpleGraphStore()
        storage_context = StorageContext.from_defaults(graph_store=graph_store)
        
        return storage_context
    
    def _configure_settings(self):
        """é…ç½®å…¨å±€è®¾ç½®"""
        # ä½¿ç”¨ä¸hybrid_searchç›¸åŒçš„LLMå’Œembeddingé…ç½®
        llm, embed_model = hybrid_search._create_llm_and_embed_model()
        Settings.llm = llm
        Settings.embed_model = embed_model
        Settings.chunk_size = 512
    
    async def _load_knowledge_graph(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†å›¾è°±"""
        if not KG_AVAILABLE:
            return False
            
        try:
            if not Path(self._kg_storage_path).exists():
                return False
            
            # é…ç½®å…¨å±€è®¾ç½®
            self._configure_settings()
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(persist_dir=self._kg_storage_path)
            
            # åŠ è½½çŸ¥è¯†å›¾è°± - ä¿®å¤åŠ è½½æ–¹æ³•
            from llama_index.core import load_index_from_storage
            self._kg_index = load_index_from_storage(
                storage_context=storage_context,
            )
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸ: {self._kg_storage_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return False


# å…¨å±€å®ä¾‹
performance_kg = PerformanceKnowledgeGraph()