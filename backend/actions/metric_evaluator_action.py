#!/usr/bin/env python
"""
æŒ‡æ ‡è¯„åˆ†Action - ä»writer_actionä¸­å½»åº•ç‹¬ç«‹ï¼ˆSOP1 æŒ‡æ ‡è¯„ä»·ï¼Œå†…è”å®ç°ï¼‰
"""
from metagpt.actions import Action
from metagpt.logs import logger
from backend.tools.json_utils import extract_json_from_llm_response

from backend.config.evaluation_standards import EVALUATION_TYPES  # è¯„ä»·ç±»å‹é…ç½®ï¼ˆæè¿°/è¯„åˆ†æŒ‡å¯¼/æ„è§å†™æ³•è¦æ±‚ï¼‰
from backend.config.evaluator_prompts import (
    EVALUATOR_BASE_SYSTEM,     # è¯„ä»·ä¸“å®¶ç³»ç»Ÿæç¤º
    EVALUATION_PROMPT_TEMPLATE, # ç»Ÿä¸€è¯„ä»·æç¤ºè¯æ¨¡æ¿ï¼ˆä»…è¾“å‡ºJSON: score/opinionï¼‰
    METRIC_PROMPT_SPEC,        # æŒ‡æ ‡çº§æç¤ºè¯ç»„åˆè§„èŒƒï¼ˆä¸åŒè¯„ä»·ç±»å‹çš„è¦ç‚¹/è®¡åˆ†è¯´æ˜ï¼‰
)
from pathlib import Path
import re
from backend.tools.project_info import get_project_info_text


class EvaluateMetrics(Action):
    """
    æŒ‡æ ‡è¯„åˆ†Action - æŒ‰ç…§æ ‡å‡†åŒ–è¯„ä»·ç±»å‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†
    ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆè¯„ä»·æ„è§å’Œå…·ä½“å¾—åˆ†
    """

    async def run(self, metric_table_json: str, vector_store_path: str, metric_table_md_path: str | None = None) -> dict:
        """
        å¯¹æ‰€æœ‰æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼Œè¿”å›è¯„åˆ†ç»“æœ

        Returns:
            dict: {
                "metrics_scores": [{"metric": {}, "score": 85.5, "opinion": "è¯„ä»·æ„è§"}],
                "level1_summary": {"å†³ç­–": 12.5, "è¿‡ç¨‹": 22.3, "äº§å‡º": 30.1, "æ•ˆç›Š": 20.6},
                "total_score": 85.5,
            }
        """
        logger.info("ğŸ“Š å¼€å§‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†...")

        try:
            # ä¼˜å…ˆä»æ–‡ä»¶è¯»å–ï¼Œé¿å…ç»ç”±æ¶ˆæ¯ä¸Šä¸‹æ–‡ä¼ é€’é€ æˆæ±¡æŸ“
            metrics_data = None
            if metric_table_md_path:
                path = Path(metric_table_md_path)
                if path.exists():
                    text = path.read_text(encoding="utf-8")
                    m = re.search(r"```json\s*(.*?)\s*```", text, flags=re.DOTALL)
                    if m:
                        metrics_data = extract_json_from_llm_response(m.group(1))
                # è®°å½• docs ç›®å½•ï¼Œä¾›äº‹å®ä¾æ®/ç®€æŠ¥å›å†™ä½¿ç”¨
                try:
                    self._docs_dir = path.parent
                except Exception:
                    self._docs_dir = None
            if metrics_data is None:
                # å›é€€ï¼šè§£æä¼ å…¥çš„ JSON å­—ç¬¦ä¸²ï¼ˆå…¼å®¹ä»£ç å—/æ¾æ•£JSONï¼‰
                metrics_data = extract_json_from_llm_response(metric_table_json)

            if isinstance(metrics_data, dict) and "error" in metrics_data:
                return {"error": "æŒ‡æ ‡ä½“ç³»æ„å»ºå¤±è´¥", "details": metrics_data}

            # extract_json_from_llm_response å·²åšå½’ä¸€åŒ–ï¼›æ­¤å¤„åªåšæœ€ç»ˆå…œåº•
            if isinstance(metrics_data, dict):
                metrics_data = [metrics_data]
            elif not isinstance(metrics_data, list):
                metrics_data = []

            # ä¸ºæ¯ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼ˆä¸¥æ ¼å¯¹é½ SOP1 æ‰å¹³è‹±æ–‡å­—æ®µç»“æ„ï¼‰
            metrics_scores = []

            for metric in metrics_data:
                try:
                    # æ‰§è¡Œå•ä¸ªæŒ‡æ ‡è¯„åˆ†
                    score, opinion = await self._evaluate_single_metric(metric, vector_store_path)

                    metrics_scores.append({
                        "metric": metric,
                        "score": score,
                        "opinion": opinion,
                    })

                    logger.info(f"âœ… å®ŒæˆæŒ‡æ ‡è¯„åˆ†: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {score}åˆ†")

                except Exception as e:
                    logger.error(f"æŒ‡æ ‡è¯„åˆ†å¤±è´¥: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {e}")


            # å°†æ„è§ä¸å¾—åˆ†å›å†™è‡³æŒ‡æ ‡è¡¨mdï¼ˆè‹¥æä¾›è·¯å¾„ï¼‰
            if metric_table_md_path:
                try:
                    self._update_metric_table_md(metric_table_md_path, metrics_scores)
                    logger.info(f"ğŸ“ å·²å›å†™è¯„åˆ†ä¸æ„è§è‡³: {metric_table_md_path}")
                except Exception as e:
                    logger.error(f"å›å†™metric_analysis_table.mdå¤±è´¥: {e}")

            # å°†è¯„åˆ†æ‘˜è¦æ³¨å…¥ç ”ç©¶ç®€æŠ¥é™„å½•åŒºï¼ˆä¸æ”¹å˜å…­é”®JSONç»“æ„ï¼‰
            try:
                if getattr(self, "_docs_dir", None):
                    brief_path = self._docs_dir / "research_brief.md"
                    if brief_path.exists():
                        self._update_research_brief_with_metrics(str(brief_path), metrics_scores)
                        logger.info(f"ğŸ“ å·²å°†è¯„åˆ†æ‘˜è¦æ³¨å…¥ç®€æŠ¥: {brief_path}")
            except Exception as e:
                logger.error(f"æ³¨å…¥ç ”ç©¶ç®€æŠ¥è¯„åˆ†æ‘˜è¦å¤±è´¥: {e}")

            result = {"metrics_scores": metrics_scores}
            logger.info("ğŸ“Š æŒ‡æ ‡è¯„åˆ†å®Œæˆï¼Œå·²å›å†™æ¯é¡¹ score/opinion")
            return result

        except Exception as e:
            logger.error(f"æŒ‡æ ‡è¯„åˆ†è¿‡ç¨‹å¤±è´¥: {e}")
            return {"error": "æŒ‡æ ‡è¯„åˆ†å¤±è´¥", "details": str(e)}

    def _update_metric_table_md(self, md_path: str, metrics_scores: list[dict]) -> None:
        """åœ¨ metric_analysis_table.md çš„ JSON é‡Œä¸ºåŒ¹é…çš„æŒ‡æ ‡è¡¥å…… opinion ä¸ scoreï¼ˆä¼˜å…ˆæŒ‰ metric_idï¼Œå…¶æ¬¡æŒ‰ nameï¼‰ã€‚"""
        path = Path(md_path)
        if not path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶: {md_path}")

        text = path.read_text(encoding="utf-8")

        # æå– ```json ... ``` ä»£ç å—
        code_block_match = re.search(r"```json\s*(.*?)\s*```", text, flags=re.DOTALL)
        if not code_block_match:
            raise ValueError("metric_analysis_table.md å†…æœªæ‰¾åˆ°JSONä»£ç å—")

        json_str = code_block_match.group(1)
        data = extract_json_from_llm_response(json_str)

        # å½’ä¸€ä¸ºåˆ—è¡¨
        if isinstance(data, dict):
            metrics_list = [data]
        elif isinstance(data, list):
            metrics_list = data
        else:
            raise ValueError("metric_analysis_table.md ä¸­JSONä¸æ˜¯å¯¹è±¡æˆ–æ•°ç»„")

        # å»ºç«‹è¯„åˆ†æ˜ å°„ï¼ˆä¼˜å…ˆ metric_idï¼Œå…¶æ¬¡ nameï¼‰
        score_map_id: dict[str, dict] = {}
        score_map_name: dict[str, dict] = {}
        for item in metrics_scores:
            metric = item.get("metric", {})
            mid = metric.get("metric_id")
            mname = metric.get("name")
            payload = {"score": item.get("score", 0), "opinion": item.get("opinion", "")}
            if mid:
                score_map_id[mid] = payload
            if mname:
                score_map_name[mname] = payload

        # å›å†™è‡³åŸåˆ—è¡¨ï¼ˆæŒ‰ metric_idï¼‰
        matched_ids = set()
        matched_names = set()
        for m in metrics_list:
            if not isinstance(m, dict):
                continue
            key_id = m.get("metric_id")
            key_name = m.get("name")
            if key_id and key_id in score_map_id:
                m["score"] = score_map_id[key_id]["score"]
                m["opinion"] = score_map_id[key_id]["opinion"]
                matched_ids.add(key_id)
            elif key_name and key_name in score_map_name:
                m["score"] = score_map_name[key_name]["score"]
                m["opinion"] = score_map_name[key_name]["opinion"]
                matched_names.add(key_name)

        # ä¸å†è¿½åŠ æ–°è®°å½•ï¼šä»…æ›´æ–°åŒ¹é…é¡¹ï¼Œä¿æŒåˆå§‹ç»“æ„ç¨³å®š

        # å†™å› mdï¼ˆä¿æŒ```json å—ï¼‰
        from json import dumps
        new_json = dumps(metrics_list, ensure_ascii=False, indent=2)
        new_text = text[:code_block_match.start(1)] + new_json + text[code_block_match.end(1):]
        path.write_text(new_text, encoding="utf-8")

    async def _evaluate_single_metric(self, metric: dict, vector_store_path: str) -> tuple:
        """ç»Ÿä¸€çš„é…ç½®é©±åŠ¨è¯„ä»·ï¼šæ ¹æ®è¯„ä»·ç±»å‹ä»é…ç½®è¯»å–æ¨¡æ¿å¹¶æ‰§è¡Œ"""
        metric_name = metric.get("name", "æœªçŸ¥æŒ‡æ ‡")
        evaluation_type = metric.get("evaluation_type", "")
        evaluation_points = metric.get("evaluation_points", "")
        scoring_method = metric.get("scoring_method", "")

        logger.info(f"ğŸ” å¼€å§‹è¯„ä»·æŒ‡æ ‡: {metric_name} (ç±»å‹: {evaluation_type})")

        # æ³¨å…¥ç ”ç©¶ç®€æŠ¥äº‹å®ï¼ˆç¦ç”¨RAGæ£€ç´¢ï¼‰
        facts = await self._retrieve_metric_facts(metric_name, vector_store_path)
        logger.info(f"ğŸ“š æ³¨å…¥çš„äº‹å®ä¾æ®: {len(facts)} å­—ç¬¦")

        # ä»é…ç½®è·å–è¯¥è¯„ä»·ç±»å‹çš„è¯¦ç»†è¯´æ˜
        eval_cfg = EVALUATION_TYPES.get(evaluation_type, {})
        type_description = eval_cfg.get("description", "")
        scoring_guidance = eval_cfg.get("scoring_guidance", "")
        opinion_requirements = eval_cfg.get("opinion_requirements", "")

        logger.info(f"ğŸ“‹ è¯„ä»·ç±»å‹é…ç½®: {type_description}")
        logger.info(f"ğŸ“ è¯„ä»·æ„è§è¦æ±‚: {len(opinion_requirements)} å­—ç¬¦")

        # æŒ‡æ ‡çº§æç¤ºè¯ç»„åˆè§„èŒƒ
        spec_default = METRIC_PROMPT_SPEC.get('default', {})
        spec_type = METRIC_PROMPT_SPEC.get('by_evaluation_type', {}).get(evaluation_type, {})
        points_intro = spec_type.get('points_intro', spec_default.get('points_intro', 'è¯„ä»·è¦ç‚¹ï¼š'))
        point_bullet = spec_type.get('point_bullet', spec_default.get('point_bullet', 'â‘  '))
        scoring_method_intro = spec_type.get('scoring_method_intro', spec_default.get('scoring_method_intro', 'è¯„åˆ†æ–¹æ³•ï¼š'))
        max_points = int(spec_default.get('max_points', 10))

        # è§„èŒƒåŒ–è¯„ä»·è¦ç‚¹ - evaluation_pointsç°åœ¨æ˜¯åŒ…å«è¯„ä»·è¦ç´ å’Œè®¡åˆ†è§„åˆ™çš„æ–‡æœ¬
        if evaluation_points:
            # ç›´æ¥ä½¿ç”¨evaluation_pointsæ–‡æœ¬ï¼Œå› ä¸ºå®ƒå·²ç»åŒ…å«äº†å®Œæ•´çš„è¯„ä»·è¦ç´ å’Œè®¡åˆ†è§„åˆ™
            points_str = points_intro + "\n" + evaluation_points
            logger.info(f"ğŸ“Š è¯„ä»·è¦ç‚¹æ ¼å¼åŒ–å®Œæˆ: {len(points_str)} å­—ç¬¦")
        else:
            points_str = points_intro + " æ— "
            logger.warning("âš ï¸ è¯„ä»·è¦ç‚¹ä¸ºç©º")

        # è§„èŒƒåŒ–è¯„åˆ†æ–¹æ³• - å¦‚æœscoring_methodä¸ºç©ºï¼Œå¯ä»¥ä»evaluation_pointsä¸­æå–
        if scoring_method:
            scoring_method = f"{scoring_method_intro}{scoring_method}"
        else:
            # å¦‚æœscoring_methodä¸ºç©ºï¼Œå°è¯•ä»evaluation_pointsä¸­æå–è®¡åˆ†è§„åˆ™
            if "å¾—åˆ†" in evaluation_points or "åˆ†å€¼" in evaluation_points:
                scoring_method = f"{scoring_method_intro}è®¡åˆ†è§„åˆ™å·²åœ¨è¯„ä»·è¦ç‚¹ä¸­æ˜ç¡®"
            else:
                scoring_method = f"{scoring_method_intro}æ— "

        if EVALUATION_PROMPT_TEMPLATE:
            prompt = EVALUATION_PROMPT_TEMPLATE.format(
                evaluation_type=evaluation_type or "æ ‡å‡†è¯„ä»·",
                evaluation_points=points_str,
                facts=facts,
                scoring_method=scoring_method,
                type_description=type_description,
                scoring_guidance=scoring_guidance,
                opinion_requirements=opinion_requirements,
            )
            logger.info(f"ğŸ“ æç¤ºè¯æ¨¡æ¿ç”Ÿæˆå®Œæˆ: {len(prompt)} å­—ç¬¦")
        else:
            # å…œåº•æ¨¡æ¿
            prompt = f"è¯·åŸºäºä»¥ä¸‹äº‹å®ï¼ŒæŒ‰{evaluation_type or 'æ ‡å‡†è¯„ä»·'}è¿›è¡Œè¯„åˆ†ã€‚\n\nè¦ç‚¹:\n{points_str}\n\näº‹å®:\n{facts}\n\nè¯„åˆ†æ–¹æ³•:{scoring_method}"
            logger.warning("âš ï¸ ä½¿ç”¨å…œåº•æç¤ºè¯æ¨¡æ¿")

        try:
            project_info_text = get_project_info_text()
            logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨LLMè¿›è¡Œè¯„åˆ†...")
            result = await self._aask(prompt, [EVALUATOR_BASE_SYSTEM, project_info_text])
            logger.info(f"ğŸ¤– LLMå“åº”å®Œæˆ: {len(result)} å­—ç¬¦")
            
            # è§£æJSONå“åº”
            parsed = extract_json_from_llm_response(result)
            logger.info(f"ğŸ” JSONè§£æç»“æœç±»å‹: {type(parsed)}")
            
            # åªæ¥å—å•å¯¹è±¡ï¼›è‹¥è¿”å›åˆ—è¡¨/åŒ…è£¹ï¼Œåˆ™åˆ¤å®šä¸ºéæ³•ï¼Œç›´æ¥ç½®0å¹¶å›å†™è§£æå¤±è´¥è¯´æ˜
            if isinstance(parsed, dict):
                item = parsed
                logger.info(f"âœ… æˆåŠŸè§£æJSONå¯¹è±¡ï¼ŒåŒ…å«é”®: {list(item.keys())}")
            else:
                raise ValueError(f"è¯„åˆ†ç»“æœå¿…é¡»ä¸ºå•ä¸ªJSONå¯¹è±¡ï¼Œä¸”ä»…åŒ…å«score/opinion ä¸¤ä¸ªé”®ï¼Œå®é™…ç±»å‹: {type(parsed)}")
            
            score_val = item.get("score")
            if not isinstance(score_val, (int, float)):
                raise ValueError(f"scoreå¿…é¡»ä¸ºæ•°å€¼(0-100)ï¼Œç¦æ­¢å­—ç¬¦ä¸²/ç™¾åˆ†å·/æ–‡å­—ï¼Œå®é™…å€¼: {score_val}")
            
            score = float(score_val)
            opinion = item.get("opinion") or ""
            
            logger.info(f"âœ… æŒ‡æ ‡è¯„åˆ†æˆåŠŸ: {metric_name} - {score}åˆ†")
            logger.info(f"ğŸ“ è¯„ä»·æ„è§: {opinion[:100]}...")
            
            return score, opinion
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€è¯„ä»·å¤±è´¥: {e}")
            logger.error(f"ğŸ“ åŸå§‹LLMå“åº”: {result[:200]}...")
            return 0, f"ç»Ÿä¸€è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"

    async def _retrieve_metric_facts(self, metric_name: str, vector_store_path: str) -> str:
        """æ³¨å…¥ç ”ç©¶ç®€æŠ¥ä¸ºäº‹å®ä¾æ®ï¼Œé¿å…åœ¨è¯„ä»·é˜¶æ®µå†æ¬¡è¿›è¡ŒRAGæ£€ç´¢ã€‚"""
        try:
            # ä¼˜å…ˆä» docs/research_brief.md è¯»å–å…­é”®å†…å®¹
            docs_dir = getattr(self, "_docs_dir", None)
            if docs_dir:
                brief_path = docs_dir / "research_brief.md"
                if brief_path.exists():
                    raw = brief_path.read_text(encoding="utf-8").strip()
                    brief = extract_json_from_llm_response(raw)
                    brief = brief if isinstance(brief, dict) else {}
                    # é€‰æ‹©ä¸æŒ‡æ ‡äº‹å®æœ€ç›¸å…³çš„é”®æ‹¼è£…ä¸ºäº‹å®
                    parts = []
                    for key in ["é¡¹ç›®æƒ…å†µ", "èµ„é‡‘æƒ…å†µ", "é‡è¦äº‹ä»¶", "æ”¿ç­–å¼•ç”¨", "å¯å€Ÿé‰´ç½‘ç»œæ¡ˆä¾‹"]:
                        val = brief.get(key)
                        if isinstance(val, str) and val.strip():
                            parts.append(f"ã€{key}ã€‘\n{val.strip()}")
                    if parts:
                        return "\n\n".join(parts)
            # ç®€æŠ¥ä¸å¯ç”¨æ—¶ï¼Œè¿”å›æç¤ºæ–‡æœ¬ï¼Œä¸å†åšRAGæ£€ç´¢
            return f"ä¾æ®ç ”ç©¶ç®€æŠ¥ä¿¡æ¯è¿›è¡Œè¯„ä»·ã€‚è‹¥äº‹å®ä¸è¶³ï¼Œè¯·åœ¨ç®€æŠ¥ä¸­è¡¥å……ä¸â€˜{metric_name}â€™ç›¸å…³çš„è¯æ®ã€‚"
        except Exception as e:
            logger.error(f"è¯»å–ç ”ç©¶ç®€æŠ¥å¤±è´¥: {e}")
            return f"è¯»å–ç ”ç©¶ç®€æŠ¥å¤±è´¥ï¼Œè¯„ä»·å°†ä»…åŸºäºæŒ‡æ ‡è¦ç‚¹æ‰§è¡Œã€‚"

    def _update_research_brief_with_metrics(self, brief_md_path: str, metrics_scores: list[dict]) -> None:
        """åœ¨ä¸æ”¹å˜å…­é”®JSONç»“æ„çš„å‰æä¸‹ï¼Œå°†è¯„åˆ†æ‘˜è¦ä»¥é™„å½•å½¢å¼è¿½åŠ åˆ°ç®€æŠ¥æ–‡ä»¶å°¾éƒ¨ã€‚"""
        path = Path(brief_md_path)
        if not path.exists():
            return
        try:
            original = path.read_text(encoding="utf-8")
        except Exception:
            return

        # æ„å»ºç®€è¦æ‘˜è¦ï¼ˆæ§åˆ¶é•¿åº¦ï¼Œopinion å‹ç¼©ä¸ºå•è¡Œï¼‰
        lines = []
        for item in metrics_scores[:30]:  # æœ€å¤šæ‘˜è¦å‰30é¡¹ï¼Œé¿å…è¿‡é•¿
            metric = item.get("metric", {})
            name = metric.get("name") or metric.get("metric_id") or "æœªçŸ¥æŒ‡æ ‡"
            score = item.get("score", 0)
            opinion = str(item.get("opinion", "")).replace("\n", " ").replace("\r", " ")
            if len(opinion) > 180:
                opinion = opinion[:180] + "â€¦"
            lines.append(f"- {name}: {score} åˆ†ï¼›æ„è§ï¼š{opinion}")

        appendix = "\n\n## æŒ‡æ ‡ä½“ç³»ä¸è¯„åˆ†æ‘˜è¦\n" + "\n".join(lines) if lines else "\n\n## æŒ‡æ ‡ä½“ç³»ä¸è¯„åˆ†æ‘˜è¦\nï¼ˆæš‚æ— å¯ç”¨è¯„åˆ†æ‘˜è¦ï¼‰"

        # è‹¥å·²å­˜åœ¨è¯¥ç« èŠ‚æ ‡é¢˜ï¼Œåˆ™æ›¿æ¢ï¼›å¦åˆ™è¿½åŠ 
        if "\n## æŒ‡æ ‡ä½“ç³»ä¸è¯„åˆ†æ‘˜è¦\n" in original:
            # ç®€å•æ›¿æ¢åˆ°æ–‡æœ«çš„åŒåæ®µè½
            head, _sep, _tail = original.partition("\n## æŒ‡æ ‡ä½“ç³»ä¸è¯„åˆ†æ‘˜è¦\n")
            new_content = head + appendix
        else:
            new_content = original.rstrip() + appendix

        path.write_text(new_content, encoding="utf-8")


