#!/usr/bin/env python
"""
æ¶æ„å¸ˆActioné›†åˆ - æŠ¥å‘Šç»“æ„è®¾è®¡å’ŒæŒ‡æ ‡åˆ†æ
é‡æ„å®ç°ä¸‰ç¯èŠ‚é€»è¾‘ï¼šåˆ†æç®€æŠ¥ -> RAGæ£€ç´¢ -> ç»¼åˆè®¾è®¡
"""
import pandas as pd
import json
import re
from typing import List, Tuple, Optional
from pydantic import BaseModel, Field
from metagpt.actions import Action
from metagpt.logs import logger
from backend.actions.research_action import ResearchData


class Section(BaseModel):
    """æŠ¥å‘Šç« èŠ‚çš„ç»“æ„åŒ–æ¨¡å‹"""
    section_title: str = Field(..., description="ç« èŠ‚æ ‡é¢˜")
    metric_ids: List[str] = Field(default_factory=list, description="æœ¬ç« èŠ‚å…³è”çš„æŒ‡æ ‡IDåˆ—è¡¨")
    description_prompt: str = Field(..., description="æŒ‡å¯¼æœ¬ç« èŠ‚å†™ä½œçš„æ ¸å¿ƒè¦ç‚¹æˆ–é—®é¢˜")


class ReportStructure(BaseModel):
    """æŠ¥å‘Šæ•´ä½“æ¶æ„çš„ç»“æ„åŒ–æ¨¡å‹"""
    title: str = Field(..., description="æŠ¥å‘Šä¸»æ ‡é¢˜")
    sections: List[Section] = Field(..., description="æŠ¥å‘Šçš„ç« èŠ‚åˆ—è¡¨")


class MetricAnalysisTable(BaseModel):
    """æŒ‡æ ‡åˆ†æè¡¨çš„ç»“æ„åŒ–æ¨¡å‹"""
    data_json: str = Field(..., description="å­˜å‚¨æŒ‡æ ‡åˆ†æç»“æœçš„DataFrame (JSONæ ¼å¼)")


class ArchitectOutput(BaseModel):
    """Architectè¾“å‡ºçš„å¤åˆæ•°æ®ç»“æ„"""
    report_structure: ReportStructure = Field(..., description="æŠ¥å‘Šç»“æ„è®¾è®¡")
    metric_analysis_table: MetricAnalysisTable = Field(..., description="æŒ‡æ ‡åˆ†æè¡¨")


class DesignReportStructure(Action):
    """
    è®¾è®¡æŠ¥å‘Šç»“æ„Action - Architectçš„æ ¸å¿ƒèƒ½åŠ›
    å®ç°ä¸‰ç¯èŠ‚é€»è¾‘ï¼šåˆ†æç®€æŠ¥ -> RAGæ£€ç´¢ -> ç»¼åˆè®¾è®¡
    """
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._research_data: Optional[ResearchData] = None
    
    async def run(self, enhanced_research_context: str, research_data: Optional[ResearchData] = None) -> Tuple[ReportStructure, MetricAnalysisTable]:
        """
        åŸºäºæ ‡å‡†ç»©æ•ˆè¯„ä»·æ¨¡æ¿è®¾è®¡æŠ¥å‘Šç»“æ„ï¼Œå†…å®¹æ ¹æ®é¡¹ç›®ç‰¹ç‚¹å®šåˆ¶
        
        Args:
            enhanced_research_context: å¯èƒ½å·²ç»ç»è¿‡RAGå¢å¼ºçš„ç ”ç©¶ä¸Šä¸‹æ–‡
            research_data: ProductManageræä¾›çš„ç ”ç©¶æ•°æ®ï¼ˆåŒ…å«å‘é‡çŸ¥è¯†åº“ï¼‰
        """
        logger.info("ğŸ—ï¸ å¼€å§‹åŸºäºæ ‡å‡†æ¨¡æ¿çš„æŠ¥å‘Šç»“æ„è®¾è®¡...")
        self._research_data = research_data
        
        # ä»å¢å¼ºä¸Šä¸‹æ–‡ä¸­æå–åŸå§‹ç ”ç©¶ç®€æŠ¥
        research_brief = self._extract_original_brief(enhanced_research_context)
        
        # æ­¥éª¤ä¸€ï¼šé¡¹ç›®ä¿¡æ¯æå– - ä»ç ”ç©¶ç®€æŠ¥å’ŒRAGä¸­æå–é¡¹ç›®æ ¸å¿ƒä¿¡æ¯
        logger.info("ğŸ“‹ æ­¥éª¤ä¸€ï¼šæå–é¡¹ç›®æ ¸å¿ƒä¿¡æ¯...")
        project_info = await self._extract_project_info(research_brief)
        
        # æ­¥éª¤äºŒï¼šRAGå¢å¼º - æŸ¥è¯¢è¯¦ç»†èµ„æ–™ä¸°å¯Œé¡¹ç›®ä¿¡æ¯
        logger.info("ğŸ” æ­¥éª¤äºŒï¼šRAGæ£€ç´¢å¢å¼ºé¡¹ç›®ä¿¡æ¯...")
        enriched_info = await self._enrich_with_rag(project_info)
        
        # æ­¥éª¤ä¸‰ï¼šæ ‡å‡†ç»“æ„å®šåˆ¶ - åŸºäºå›ºå®šæ¨¡æ¿ç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹
        logger.info("ğŸ—ï¸ æ­¥éª¤ä¸‰ï¼šåŸºäºæ ‡å‡†æ¨¡æ¿ç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹...")
        report_structure, metric_table = await self._generate_customized_template(enriched_info)
        
        logger.info(f"âœ… æŠ¥å‘Šè“å›¾è®¾è®¡å®Œæˆ: {report_structure.title}")
        logger.info(f"ğŸ“Š æŒ‡æ ‡ä½“ç³»: {len(json.loads(metric_table.data_json))} ä¸ªæŒ‡æ ‡")
        
        return report_structure, metric_table
    
    def _extract_original_brief(self, enhanced_context: str) -> str:
        """ä»å¢å¼ºä¸Šä¸‹æ–‡ä¸­æå–åŸå§‹ç ”ç©¶ç®€æŠ¥"""
        # å¦‚æœåŒ…å«RAGå¢å¼ºå†…å®¹ï¼Œæå–åŸå§‹éƒ¨åˆ†
        if "### RAGæ£€ç´¢å¢å¼ºå†…å®¹" in enhanced_context:
            parts = enhanced_context.split("### RAGæ£€ç´¢å¢å¼ºå†…å®¹")
            return parts[0].strip()
        return enhanced_context
    
    async def _extract_project_info(self, research_brief: str) -> dict:
        """
        æ­¥éª¤ä¸€ï¼šä»ç ”ç©¶ç®€æŠ¥ä¸­æå–é¡¹ç›®æ ¸å¿ƒä¿¡æ¯
        """
        extraction_prompt = f"""
ä½ æ˜¯ç»©æ•ˆè¯„ä»·æŠ¥å‘Šçš„æ¶æ„å¸ˆã€‚è¯·ä»ä»¥ä¸‹ç ”ç©¶ç®€æŠ¥ä¸­æå–é¡¹ç›®çš„æ ¸å¿ƒä¿¡æ¯ï¼Œç”¨äºåç»­åŸºäºæ ‡å‡†æ¨¡æ¿çš„æŠ¥å‘Šç»“æ„è®¾è®¡ã€‚

ç ”ç©¶ç®€æŠ¥ï¼š
{research_brief}

è¯·è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
1. project_name: é¡¹ç›®å…¨ç§°
2. project_type: é¡¹ç›®ç±»å‹ï¼ˆå¦‚ï¼šè´¢æ”¿æ”¯å‡ºé¡¹ç›®ã€ä¸“é¡¹èµ„é‡‘é¡¹ç›®ç­‰ï¼‰
3. budget_amount: é¡¹ç›®é¢„ç®—é‡‘é¢ï¼ˆå¦‚æœæœ‰ï¼‰
4. implementation_period: å®æ–½æœŸé—´
5. target_beneficiaries: ä¸»è¦å—ç›Šå¯¹è±¡
6. main_objectives: ä¸»è¦ç›®æ ‡ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰
7. key_activities: ä¸»è¦æ´»åŠ¨å†…å®¹ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰
8. performance_focus: ç»©æ•ˆé‡ç‚¹å…³æ³¨é¢†åŸŸï¼ˆå¦‚ï¼šç»æµæ•ˆç›Šã€ç¤¾ä¼šæ•ˆç›Šã€ç”Ÿæ€æ•ˆç›Šç­‰ï¼‰

è¦æ±‚ï¼š
- ä¿¡æ¯è¦å‡†ç¡®ã€å®Œæ•´
- å¦‚æœæŸäº›ä¿¡æ¯ä¸æ˜ç¡®ï¼Œæ ‡æ³¨ä¸º"å¾…è¡¥å……"
- é‡ç‚¹å…³æ³¨ä¸ç»©æ•ˆè¯„ä»·ç›¸å…³çš„ä¿¡æ¯
"""
        
        try:
            extraction_result = await self._aask(extraction_prompt)
            
            # ä»LLMå›å¤ä¸­æå–JSONå†…å®¹
            project_info = self._extract_json_from_llm_response(extraction_result)
            
            logger.info(f"ğŸ“‹ é¡¹ç›®åç§°: {project_info.get('project_name', 'æœªçŸ¥é¡¹ç›®')}")
            logger.info(f"ğŸ“‹ é¡¹ç›®ç±»å‹: {project_info.get('project_type', 'å¾…è¡¥å……')}")
            return project_info
        except Exception as e:
            logger.error(f"é¡¹ç›®ä¿¡æ¯æå–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®¾è®¡: {e}")
            raise ValueError(f"æ— æ³•ä»ç ”ç©¶ç®€æŠ¥ä¸­æå–æœ‰æ•ˆé¡¹ç›®ä¿¡æ¯: {e}")
    
    def _extract_json_from_llm_response(self, response: str) -> dict:
        """
        ä»LLMå›å¤ä¸­æå–JSONå†…å®¹ï¼Œå¤„ç†markdownæ ¼å¼å’Œé¢å¤–è¯´æ˜
        """
        try:
            # æ–¹æ³•1ï¼šå°è¯•ç›´æ¥è§£æï¼ˆå¦‚æœæ˜¯çº¯JSONï¼‰
            return json.loads(response)
        except:
            pass
        
        try:
            # æ–¹æ³•2ï¼šæå–```jsonä»£ç å—ä¸­çš„å†…å®¹
            import re
            json_pattern = r'```json\s*(.*?)\s*```'
            match = re.search(json_pattern, response, re.DOTALL)
            if match:
                json_str = match.group(1).strip()
                return json.loads(json_str)
        except:
            pass
        
        try:
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾å¤§æ‹¬å·åŒ…å›´çš„JSONå†…å®¹
            start_idx = response.find('{')
            if start_idx != -1:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ª{ï¼Œç„¶åæ‰¾åˆ°åŒ¹é…çš„}
                brace_count = 0
                end_idx = start_idx
                for i, char in enumerate(response[start_idx:], start_idx):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end_idx = i
                            break
                
                if brace_count == 0:
                    json_str = response[start_idx:end_idx+1]
                    return json.loads(json_str)
        except:
            pass
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise ValueError(f"æ— æ³•ä»LLMå›å¤ä¸­æå–æœ‰æ•ˆJSON: {response[:200]}...")
    
    async def _enrich_with_rag(self, project_info: dict) -> dict:
        """
        æ­¥éª¤äºŒï¼šé€šè¿‡RAGæ£€ç´¢ä¸°å¯Œé¡¹ç›®ä¿¡æ¯ - åŠ¨æ€ç”Ÿæˆæ£€ç´¢å…³é”®è¯
        """
        if not self._research_data or not self._research_data.content_chunks:
            logger.warning("å‘é‡çŸ¥è¯†åº“ä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹é¡¹ç›®ä¿¡æ¯")
            return project_info
        
        # åŠ¨æ€ç”Ÿæˆæ£€ç´¢å…³é”®è¯
        search_keywords = await self._generate_rag_search_keywords(project_info)
        
        enriched_info = project_info.copy()
        enriched_info["rag_evidence"] = {}
        
        logger.info(f"ğŸ” å¼€å§‹å¯¹ {len(search_keywords)} ä¸ªåŠ¨æ€å…³é”®è¯è¿›è¡ŒRAGæ£€ç´¢ï¼ˆè¯·ç¨å€™ï¼‰...")
        
        for keyword_group in search_keywords:
            category = keyword_group["category"]
            keywords = keyword_group["keywords"]
            
            # å¯¹æ¯ä¸ªå…³é”®è¯ç»„è¿›è¡Œæ£€ç´¢
            category_evidence = []
            for keyword in keywords:
                relevant_chunks = await self._search_chunks(keyword, self._research_data.content_chunks)
                if relevant_chunks:
                    category_evidence.extend(relevant_chunks[:2])  # æ¯ä¸ªå…³é”®è¯å–å‰2ä¸ªæœ€ç›¸å…³çš„
            
            if category_evidence:
                enriched_info["rag_evidence"][category] = category_evidence
                # ç®€åŒ–å•ä¸ªç±»åˆ«çš„æ—¥å¿—è¾“å‡º
                logger.debug(f"ğŸ“‹ {category}: æ£€ç´¢åˆ° {len(category_evidence)} æ¡ç›¸å…³è¯æ®")
        
        logger.info(f"ğŸ“‹ RAGæ£€ç´¢å®Œæˆï¼Œä¸°å¯Œäº† {len(enriched_info['rag_evidence'])} ä¸ªä¿¡æ¯ç±»åˆ«")
        return enriched_info
    
    async def _generate_rag_search_keywords(self, project_info: dict) -> List[dict]:
        """
        åŠ¨æ€ç”ŸæˆRAGæ£€ç´¢å…³é”®è¯ï¼ˆç±»ä¼¼PMçš„å…³é”®è¯ç”Ÿæˆé€»è¾‘ï¼‰
        """
        project_name = project_info.get('project_name', 'é¡¹ç›®')
        
        keyword_generation_prompt = f"""
ä½ æ˜¯æ¶æ„å¸ˆçš„RAGæ£€ç´¢åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹é¡¹ç›®ä¿¡æ¯ï¼Œç”Ÿæˆç”¨äºæ£€ç´¢å‘é‡çŸ¥è¯†åº“çš„å…³é”®è¯ç»„ã€‚

é¡¹ç›®ä¿¡æ¯ï¼š
{json.dumps(project_info, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆ6ä¸ªç±»åˆ«çš„æ£€ç´¢å…³é”®è¯ï¼Œæ¯ä¸ªç±»åˆ«åŒ…å«3-5ä¸ªå…·ä½“çš„æ£€ç´¢è¯ï¼š

è¿”å›JSONæ ¼å¼ï¼š
[
  {{
    "category": "é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡",
    "keywords": ["é¡¹ç›®ç«‹é¡¹èƒŒæ™¯", "ä¸»è¦ç›®æ ‡", "é¢„æœŸæˆæœ"]
  }},
  {{
    "category": "èµ„é‡‘ä¸é¢„ç®—",
    "keywords": ["é¢„ç®—æ€»é¢", "èµ„é‡‘æ¥æº", "èµ„é‡‘åˆ†é…"]
  }},
  {{
    "category": "å®æ–½æ–¹æ¡ˆ",
    "keywords": ["å®æ–½æ­¥éª¤", "æŠ€æœ¯æ–¹æ¡ˆ", "ç®¡ç†æªæ–½"]
  }},
  {{
    "category": "æ•ˆæœä¸æˆæ•ˆ",
    "keywords": ["å®æ–½æ•ˆæœ", "äº§å‡ºæŒ‡æ ‡", "æ•ˆç›Šåˆ†æ"]
  }},
  {{
    "category": "æ”¿ç­–ä¾æ®",
    "keywords": ["æ”¿ç­–æ–‡ä»¶", "æ³•è§„ä¾æ®", "æ ‡å‡†è§„èŒƒ"]
  }},
  {{
    "category": "é£é™©ä¸æŒ‘æˆ˜",
    "keywords": ["å­˜åœ¨é—®é¢˜", "é£é™©å› ç´ ", "æ”¹è¿›å»ºè®®"]
  }}
]

è¦æ±‚ï¼šå…³é”®è¯è¦å…·ä½“ã€å‡†ç¡®ï¼Œèƒ½åœ¨{project_name}ç›¸å…³èµ„æ–™ä¸­æ‰¾åˆ°å¯¹åº”ä¿¡æ¯ã€‚
"""
        
        try:
            keywords_result = await self._aask(keyword_generation_prompt)
            
            # ä½¿ç”¨åŒæ ·çš„JSONæå–é€»è¾‘
            search_keywords = self._extract_json_from_llm_response(keywords_result)
            
            logger.info(f"ğŸ” åŠ¨æ€ç”Ÿæˆäº† {len(search_keywords)} ä¸ªå…³é”®è¯ç»„")
            return search_keywords
        except Exception as e:
            logger.warning(f"åŠ¨æ€å…³é”®è¯ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å…³é”®è¯: {e}")
            # åŸºç¡€å…³é”®è¯ä½œä¸ºå¤‡ç”¨
            return [
                {"category": "é¡¹ç›®åŸºæœ¬ä¿¡æ¯", "keywords": ["é¡¹ç›®åç§°", "é¡¹ç›®èƒŒæ™¯", "ä¸»è¦ç›®æ ‡"]},
                {"category": "èµ„é‡‘é¢„ç®—", "keywords": ["é¢„ç®—é‡‘é¢", "èµ„é‡‘æ¥æº", "æ”¯å‡ºæ˜ç»†"]},
                {"category": "å®æ–½å†…å®¹", "keywords": ["å®æ–½æ–¹æ¡ˆ", "æŠ€æœ¯æªæ–½", "ç®¡ç†æµç¨‹"]},
                {"category": "ç»©æ•ˆæŒ‡æ ‡", "keywords": ["è¯„ä»·æŒ‡æ ‡", "æˆæœäº§å‡º", "æ•ˆç›Šåˆ†æ"]}
            ]
    
    async def _search_chunks(self, query: str, content_chunks: List[str]) -> List[str]:
        """åœ¨å†…å®¹å—ä¸­æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œå°è¯•ä½¿ç”¨å‘é‡æ£€ç´¢"""
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨å‘é‡æ£€ç´¢
            if self._research_data and hasattr(self._research_data, 'vector_store_path'):
                vector_results = await self._vector_search(query, self._research_data.vector_store_path)
                if vector_results:
                    return vector_results[:3]  # è¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„
        except Exception as e:
            logger.warning(f"å‘é‡æ£€ç´¢å¤±è´¥ï¼Œé™çº§åˆ°å…³é”®è¯æ£€ç´¢: {e}")
        
        # é™çº§åˆ°å…³é”®è¯æ£€ç´¢
        query_keywords = self._extract_search_keywords(query)
        
        # è®¡ç®—æ¯ä¸ªå—çš„ç›¸å…³åº¦
        chunk_scores = []
        for chunk in content_chunks:
            score = self._calculate_chunk_relevance(chunk, query_keywords)
            if score > 0:
                chunk_scores.append((score, chunk))
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        chunk_scores.sort(reverse=True)
        return [chunk for score, chunk in chunk_scores[:3]]  # è¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„
    
    async def _vector_search(self, query: str, vector_store_path: str) -> List[str]:
        """ä½¿ç”¨å‘é‡æ£€ç´¢æœç´¢ç›¸å…³å†…å®¹"""
        try:
            from metagpt.rag.engines.simple import SimpleEngine
            from metagpt.rag.schema import FAISSRetrieverConfig, VectorIndexConfig
            import os
            
            if not os.path.exists(vector_store_path):
                logger.warning(f"å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨: {vector_store_path}")
                return []
            
            # æ£€æŸ¥å¹¶åŠ è½½å·²æœ‰çš„å‘é‡åº“
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨PMåˆ›å»ºçš„ç›¸åŒæ–‡ä»¶æ¥åˆå§‹åŒ–
            vector_files = []
            if os.path.isdir(vector_store_path):
                vector_files = [os.path.join(vector_store_path, f) for f in os.listdir(vector_store_path) if f.endswith('.txt')]
            
            if not vector_files:
                logger.warning(f"å‘é‡åº“ç›®å½•ä¸ºç©º: {vector_store_path}")
                return []
            
            # ä½¿ç”¨MetaGPTåŸç”Ÿçš„RAG embeddingå·¥å‚ - è¿™æ˜¯æ­£ç¡®çš„æ–¹å¼ï¼
            from llama_index.llms.openai import OpenAI as LlamaOpenAI
            from pathlib import Path
            from metagpt.config2 import Config
            from metagpt.rag.factories.embedding import get_rag_embedding
            
            # æ‰‹åŠ¨åŠ è½½å®Œæ•´é…ç½®ï¼Œç¡®ä¿embeddingé…ç½®è¢«æ­£ç¡®è¯»å–
            full_config = Config.from_yaml_file(Path('config/config2.yaml'))
            
            # è·å–LLMé…ç½® - ä½¿ç”¨å…¼å®¹çš„æ¨¡å‹å
            llm_config = full_config.llm
            llm = LlamaOpenAI(
                api_key=llm_config.api_key,
                base_url=llm_config.base_url,
                model="gpt-3.5-turbo"  # ä½¿ç”¨llama_indexè®¤è¯†çš„æ¨¡å‹åï¼Œå®é™…ä¼šè°ƒç”¨é˜¿é‡Œäº‘API
            )
            
            # ä½¿ç”¨MetaGPTåŸç”Ÿembeddingå·¥å‚ - è¿™ä¼šæ­£ç¡®å¤„ç†model_nameå‚æ•°
            embed_model = get_rag_embedding(config=full_config)
            # é˜¿é‡Œäº‘DashScope embedding APIé™åˆ¶æ‰¹å¤„ç†å¤§å°ä¸èƒ½è¶…è¿‡10
            embed_model.embed_batch_size = 10
            
            engine = SimpleEngine.from_docs(
                input_files=vector_files,  # ä½¿ç”¨å·²å­˜åœ¨çš„æ–‡ä»¶
                llm=llm,  # çœŸå®çš„LLMé…ç½®
                embed_model=embed_model  # çœŸå®çš„åµŒå…¥æ¨¡å‹
            )
            
            # æ‰§è¡Œæ£€ç´¢
            results = await engine.aretrieve(query)
            
            # æå–å†…å®¹
            retrieved_texts = []
            for result in results:
                if hasattr(result, 'text') and result.text:
                    retrieved_texts.append(result.text.strip())
            
            logger.debug(f"ğŸ” å‘é‡æ£€ç´¢æ‰¾åˆ° {len(retrieved_texts)} æ¡ç›¸å…³å†…å®¹")
            return retrieved_texts
            
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢æ‰§è¡Œå¤±è´¥: {e}")
            return []
    
    def _extract_search_keywords(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯"""
        # å»é™¤åœç”¨è¯ï¼Œæå–æœ‰æ„ä¹‰çš„è¯æ±‡
        stopwords = {'çš„', 'äº†', 'å’Œ', 'ä¸', 'åŠ', 'ä»¥åŠ', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'å“ªäº›', 'æ€æ ·'}
        words = re.findall(r'[\u4e00-\u9fff]+', query)
        keywords = [word for word in words if len(word) > 1 and word not in stopwords]
        return keywords
    
    def _calculate_chunk_relevance(self, chunk: str, keywords: List[str]) -> float:
        """è®¡ç®—å†…å®¹å—ä¸å…³é”®è¯çš„ç›¸å…³åº¦"""
        score = 0
        chunk_lower = chunk.lower()
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in chunk_lower:
                # è®¡ç®—å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„å¯†åº¦
                count = chunk_lower.count(keyword_lower)
                score += count * len(keyword)  # é•¿å…³é”®è¯æƒé‡æ›´é«˜
        
        # æ ‡å‡†åŒ–åˆ†æ•°
        return score / max(len(chunk), 1)
    
    async def _generate_customized_template(self, enriched_info: dict) -> Tuple[ReportStructure, MetricAnalysisTable]:
        """
        æ­¥éª¤ä¸‰ï¼šåŸºäºæ ‡å‡†ç»©æ•ˆè¯„ä»·æ¨¡æ¿ç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹
        """
        # æ ‡å‡†ç»©æ•ˆè¯„ä»·æŠ¥å‘Šç»“æ„ï¼ˆåŸºäºreportmodel.yamlï¼‰
        standard_sections = [
            {
                "title": "ä¸€ã€é¡¹ç›®æ¦‚è¿°",
                "key": "overview",
                "prompt_template": "è¯·å›´ç»•ä»¥ä¸‹æ–¹é¢è¯¦ç»†æè¿°é¡¹ç›®æ¦‚å†µï¼š1. é¡¹ç›®ç«‹é¡¹èƒŒæ™¯åŠç›®çš„ã€é¡¹ç›®ä¸»è¦å†…å®¹ï¼›2. èµ„é‡‘æŠ•å…¥å’Œä½¿ç”¨æƒ…å†µã€é¡¹ç›®å®æ–½æƒ…å†µï¼›3. é¡¹ç›®ç»„ç»‡ç®¡ç†ï¼›4. é¡¹ç›®ç»©æ•ˆç›®æ ‡ï¼šé€šè¿‡çŸ¥è¯†åº“æœç´¢ç»©æ•ˆç›®æ ‡è¡¨å¤åˆ¶ç›¸å…³å†…å®¹ï¼ŒåŠ¡å¿…ä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºé¡¹ç›®ç»©æ•ˆæŒ‡æ ‡"
            },
            {
                "title": "äºŒã€ç»¼åˆç»©æ•ˆè¯„ä»·ç»“è®º",
                "key": "conclusion", 
                "prompt_template": "è¯·åŸºäºå¯¹é¡¹ç›®å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºå’Œæ•ˆç›Šå››ä¸ªç»´åº¦çš„å…¨é¢ç»©æ•ˆåˆ†æï¼Œç»™å‡ºé¡¹ç›®çš„ç»¼åˆè¯„ä»·ç»“è®ºã€‚åº”åŒ…å«é¡¹ç›®æ€»å¾—åˆ†ã€è¯„ä»·ç­‰çº§ï¼Œå¹¶åŠ¡å¿…ä»¥è¡¨æ ¼å½¢å¼æ¸…æ™°å±•ç¤ºå„ä¸€çº§æŒ‡æ ‡ï¼ˆå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šï¼‰çš„è®¡åˆ’åˆ†å€¼ã€å®é™…å¾—åˆ†å’Œå¾—åˆ†ç‡"
            },
            {
                "title": "ä¸‰ã€ä¸»è¦æˆæ•ˆåŠç»éªŒ",
                "key": "achievements",
                "prompt_template": "è¯·è¯¦ç»†æ€»ç»“é¡¹ç›®å®æ–½è¿‡ç¨‹ä¸­æ‰€å–å¾—çš„å„é¡¹ä¸»è¦æˆæ•ˆï¼Œéœ€ç»“åˆå…·ä½“æ•°æ®å’Œäº‹å®è¿›è¡Œé˜è¿°ã€‚åŒæ—¶ï¼Œæç‚¼å‡ºé¡¹ç›®åœ¨æ”¿ç­–æ‰§è¡Œã€èµ„é‡‘ç®¡ç†ã€éƒ¨é—¨ååŒã€æœåŠ¡ä¼˜åŒ–ç­‰æ–¹é¢å¯ä¾›å…¶ä»–åœ°åŒºæˆ–ç±»ä¼¼é¡¹ç›®å€Ÿé‰´çš„æˆåŠŸç»éªŒå’Œæœ‰æ•ˆåšæ³•"
            },
            {
                "title": "å››ã€å­˜åœ¨çš„é—®é¢˜å’ŒåŸå› åˆ†æ",
                "key": "problems",
                "prompt_template": "è¯·æ ¹æ®è°ƒç ”ï¼ˆå¦‚é—®å·è°ƒæŸ¥ã€è®¿è°ˆï¼‰å’Œæ•°æ®åˆ†æï¼Œå®¢è§‚ã€å‡†ç¡®åœ°æŒ‡å‡ºé¡¹ç›®åœ¨å®æ–½è¿‡ç¨‹ä¸­å­˜åœ¨çš„ä¸»è¦é—®é¢˜ã€‚å¯¹äºæ¯ä¸ªè¯†åˆ«å‡ºçš„é—®é¢˜ï¼Œéƒ½åº”æ·±å…¥å‰–æå…¶äº§ç”Ÿçš„å†…å¤–éƒ¨åŸå› "
            },
            {
                "title": "äº”ã€æ”¹è¿›å»ºè®®",
                "key": "suggestions",
                "prompt_template": "é’ˆå¯¹åœ¨'å­˜åœ¨çš„é—®é¢˜å’ŒåŸå› åˆ†æ'éƒ¨åˆ†æŒ‡å‡ºçš„å„é¡¹ä¸»è¦é—®é¢˜ï¼Œè¯·é€æ¡æå‡ºå…·ä½“çš„ã€æœ‰é’ˆå¯¹æ€§çš„ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®ã€‚å»ºè®®åº”æ˜ç¡®æ”¹è¿›æ–¹å‘ã€è´£ä»»ä¸»ä½“å’Œé¢„æœŸæ•ˆæœ"
            }
        ]
        
        # åŸºäºé¡¹ç›®ä¿¡æ¯å®šåˆ¶å†…å®¹æè¿°
        customized_sections = await self._customize_section_content(standard_sections, enriched_info)
        
        # ç”Ÿæˆæ ‡å‡†æŒ‡æ ‡ä½“ç³»
        metric_table = await self._generate_standard_metrics(enriched_info)
        
        # æ„é€ ReportStructure
        sections = []
        for section_data in customized_sections:
            section = Section(
                section_title=section_data["title"],
                metric_ids=section_data.get("metric_ids", []),
                description_prompt=section_data["description_prompt"]
            )
            sections.append(section)
        
        project_name = enriched_info.get('project_name', 'é¡¹ç›®')
        report_structure = ReportStructure(
            title=f"{project_name}ç»©æ•ˆè¯„ä»·æŠ¥å‘Š",
            sections=sections
        )
        
        return report_structure, metric_table
    
    async def _customize_section_content(self, standard_sections: List[dict], enriched_info: dict) -> List[dict]:
        """
        å®šåˆ¶åŒ–ç« èŠ‚å†…å®¹æè¿°
        """
        customized_sections = []
        project_name = enriched_info.get('project_name', 'é¡¹ç›®')
        
        for section in standard_sections:
            # åŸºäºé¡¹ç›®ä¿¡æ¯è°ƒæ•´prompt
            customized_prompt = await self._generate_section_prompt(section, enriched_info)
            
            customized_section = {
                "title": section["title"],
                "description_prompt": customized_prompt,
                "metric_ids": []
            }
            
            # ä¸º"é¡¹ç›®æ¦‚è¿°"ç« èŠ‚æ·»åŠ æŒ‡æ ‡å…³è”
            if "æ¦‚è¿°" in section["title"]:
                customized_section["metric_ids"] = ["project_scope", "budget_execution", "target_completion"]
            elif "è¯„ä»·ç»“è®º" in section["title"]:
                customized_section["metric_ids"] = ["overall_score", "decision_score", "process_score", "output_score", "benefit_score"]
                
            customized_sections.append(customized_section)
        
        return customized_sections
    
    async def _generate_section_prompt(self, section: dict, enriched_info: dict) -> str:
        """
        ç”Ÿæˆç‰¹å®šç« èŠ‚çš„å†™ä½œæŒ‡å¯¼prompt - åŸºäºRAGè¯æ®ç»™å‡ºå…·ä½“æ£€ç´¢æŒ‡å¯¼
        """
        base_prompt = section["prompt_template"]
        project_name = enriched_info.get('project_name', 'é¡¹ç›®')
        section_title = section["title"]
        
        # æ ¹æ®ç« èŠ‚ç‰¹ç‚¹ç”Ÿæˆå…·ä½“çš„RAGæ£€ç´¢æŒ‡å¯¼
        rag_instructions = await self._generate_chapter_rag_instructions(section_title, enriched_info)
        
        customized_prompt = f"""
é’ˆå¯¹{project_name}ï¼Œ{base_prompt}

### ğŸ“‹ å…·ä½“å†™ä½œæŒ‡å¯¼ä¸æ£€ç´¢è¦æ±‚ï¼š

{rag_instructions}

### ğŸ” RAGæ£€ç´¢ç­–ç•¥ï¼š
å†™ä½œæ—¶è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
1. é¦–å…ˆæ£€ç´¢ä¸Šè¿°å…³é”®ä¿¡æ¯é¡¹ï¼Œè·å–å…·ä½“æ•°æ®å’Œäº‹å®
2. åŸºäºæ£€ç´¢åˆ°çš„çœŸå®ä¿¡æ¯è¿›è¡Œåˆ†æå’Œè®ºè¿°
3. é¿å…æ³›æ³›è€Œè°ˆï¼Œç¡®ä¿æ¯ä¸ªè®ºç‚¹éƒ½æœ‰å…·ä½“çš„æ•°æ®æ”¯æ’‘
4. å¦‚æœæŸé¡¹ä¿¡æ¯æ£€ç´¢ä¸åˆ°ï¼Œæ˜ç¡®æ ‡æ³¨"ä¿¡æ¯å¾…è¡¥å……"

### ğŸ“Š è´¨é‡è¦æ±‚ï¼š
- æ•°æ®å‡†ç¡®ï¼šæ‰€æœ‰æ•°å­—ã€æ—¶é—´ã€åç§°å¿…é¡»æ¥è‡ªæ£€ç´¢åˆ°çš„åŸå§‹èµ„æ–™
- é€»è¾‘æ¸…æ™°ï¼šæŒ‰ç…§æ£€ç´¢æŒ‡å¯¼çš„é¡ºåºç»„ç»‡å†…å®¹ç»“æ„
- æ·±åº¦åˆ†æï¼šä¸ä»…è¦åˆ—å‡ºäº‹å®ï¼Œè¿˜è¦åˆ†æåŸå› å’Œå½±å“
"""
        
        return customized_prompt
    
    async def _generate_chapter_rag_instructions(self, section_title: str, enriched_info: dict) -> str:
        """
        ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆå…·ä½“çš„RAGæ£€ç´¢æŒ‡å¯¼
        """
        rag_evidence = enriched_info.get("rag_evidence", {})
        
        # æ ¹æ®ç« èŠ‚æ ‡é¢˜ç”Ÿæˆå…·ä½“çš„æ£€ç´¢æŒ‡å¯¼
        if "é¡¹ç›®æ¦‚è¿°" in section_title:
            instructions = f"""
**1. é¡¹ç›®ç«‹é¡¹èƒŒæ™¯åŠç›®çš„**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šæ”¿ç­–æ–‡ä»¶å¼•ç”¨ã€ç«‹é¡¹ä¾æ®ã€ç›®æ ‡è®¾å®š
   
**2. èµ„é‡‘æŠ•å…¥å’Œä½¿ç”¨æƒ…å†µ**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "èµ„é‡‘ä¸é¢„ç®—")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šé¢„ç®—æ€»é¢ã€èµ„é‡‘æ¥æºã€åˆ†é…æ˜ç»†ã€æ‰§è¡Œè¿›åº¦
   
**3. é¡¹ç›®ç»„ç»‡ç®¡ç†**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "å®æ–½æ–¹æ¡ˆ")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šç®¡ç†æœºæ„ã€èŒè´£åˆ†å·¥ã€æµç¨‹åˆ¶åº¦
   
**4. é¡¹ç›®ç»©æ•ˆç›®æ ‡**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "æ•ˆæœä¸æˆæ•ˆ")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šç»©æ•ˆç›®æ ‡è¡¨ã€æŒ‡æ ‡è®¾å®šã€é¢„æœŸæˆæœï¼ˆåŠ¡å¿…ä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºï¼‰
"""
        elif "ç»¼åˆç»©æ•ˆè¯„ä»·ç»“è®º" in section_title:
            instructions = f"""
**å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šå››ä¸ªç»´åº¦åˆ†æ**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "æ•ˆæœä¸æˆæ•ˆ")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šå„é¡¹æŒ‡æ ‡å®Œæˆæƒ…å†µã€è¯„åˆ†ç»“æœã€ç»¼åˆå¾—åˆ†
   - å¿…é¡»è¾“å‡ºï¼šæŒ‡æ ‡å¾—åˆ†æƒ…å†µè¡¨ï¼ˆä¸€çº§æŒ‡æ ‡ã€åˆ†å€¼ã€å¾—åˆ†ã€å¾—åˆ†ç‡ï¼‰
"""
        elif "ä¸»è¦æˆæ•ˆåŠç»éªŒ" in section_title:
            instructions = f"""
**å…·ä½“æˆæ•ˆæ•°æ®**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "æ•ˆæœä¸æˆæ•ˆ")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šé‡åŒ–æˆæœæ•°æ®ã€å—ç›Šäººç¾¤ç»Ÿè®¡ã€æ•ˆæœå¯¹æ¯”
   
**æˆåŠŸç»éªŒæ€»ç»“**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "å®æ–½æ–¹æ¡ˆ")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šåˆ›æ–°åšæ³•ã€ç®¡ç†ç»éªŒã€æŠ€æœ¯äº®ç‚¹
"""
        elif "å­˜åœ¨çš„é—®é¢˜å’ŒåŸå› åˆ†æ" in section_title:
            instructions = f"""
**é—®é¢˜è¯†åˆ«**
   - æ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "é£é™©ä¸æŒ‘æˆ˜")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šè°ƒç ”å‘ç°çš„é—®é¢˜ã€æ•°æ®åæ˜ çš„ä¸è¶³ã€åé¦ˆæ„è§
   
**åŸå› æ·±åº¦åˆ†æ**
   - æ£€ç´¢å…³é”®è¯ï¼šæ”¿ç­–æ‰§è¡Œã€ç®¡ç†åˆ¶åº¦ã€æŠ€æœ¯æ¡ä»¶ã€å¤–éƒ¨ç¯å¢ƒ
   - é‡ç‚¹æŸ¥æ‰¾ï¼šé—®é¢˜äº§ç”Ÿçš„å†…åœ¨æœºåˆ¶å’Œå¤–éƒ¨å› ç´ 
"""
        elif "æ”¹è¿›å»ºè®®" in section_title:
            instructions = f"""
**é’ˆå¯¹æ€§å»ºè®®**
   - åŸºäºå‰è¿°é—®é¢˜åˆ†æï¼Œæ£€ç´¢å…³é”®è¯ï¼š{self._get_evidence_summary(rag_evidence, "é£é™©ä¸æŒ‘æˆ˜")}
   - é‡ç‚¹æŸ¥æ‰¾ï¼šæ”¹è¿›æªæ–½ã€æ”¿ç­–å»ºè®®ã€æŠ€æœ¯ä¼˜åŒ–æ–¹æ¡ˆ
   
**å¯æ“ä½œæ€§éªŒè¯**
   - æ£€ç´¢å…³é”®è¯ï¼šæˆåŠŸæ¡ˆä¾‹ã€æœ€ä½³å®è·µã€æ”¿ç­–æ”¯æŒ
   - é‡ç‚¹æŸ¥æ‰¾ï¼šç±»ä¼¼é¡¹ç›®çš„æ”¹è¿›ç»éªŒã€æ”¿ç­–å¯è¡Œæ€§åˆ†æ
"""
        else:
            # é€šç”¨æŒ‡å¯¼
            instructions = f"""
**é€šç”¨æ£€ç´¢æŒ‡å¯¼**
   - ä¼˜å…ˆæ£€ç´¢ï¼šé¡¹ç›®ç›¸å…³çš„å…·ä½“æ•°æ®ã€æ”¿ç­–æ–‡ä»¶ã€å®æ–½æ•ˆæœ
   - é‡ç‚¹å…³æ³¨ï¼šæ•°é‡åŒ–æŒ‡æ ‡ã€æ—¶é—´èŠ‚ç‚¹ã€è´£ä»»ä¸»ä½“ã€å…·ä½“æªæ–½
"""
        
        return instructions
    
    def _get_evidence_summary(self, rag_evidence: dict, category: str) -> str:
        """
        è·å–ç‰¹å®šç±»åˆ«çš„RAGè¯æ®æ‘˜è¦ï¼Œç”¨äºæŒ‡å¯¼æ£€ç´¢
        """
        if category in rag_evidence and rag_evidence[category]:
            # ä»è¯æ®ä¸­æå–å…³é”®è¯ä½œä¸ºæ£€ç´¢æŒ‡å¯¼
            evidence_text = " ".join(rag_evidence[category][:2])  # å–å‰2æ¡è¯æ®
            # ç®€å•æå–å…³é”®æ¦‚å¿µ
            keywords = []
            if "é¢„ç®—" in evidence_text or "èµ„é‡‘" in evidence_text:
                keywords.append("é¢„ç®—èµ„é‡‘æ•°æ®")
            if "ç›®æ ‡" in evidence_text or "æŒ‡æ ‡" in evidence_text:
                keywords.append("ç›®æ ‡æŒ‡æ ‡è®¾å®š")
            if "å®æ–½" in evidence_text or "ç®¡ç†" in evidence_text:
                keywords.append("å®æ–½ç®¡ç†æªæ–½")
            if "æ•ˆæœ" in evidence_text or "æˆæœ" in evidence_text:
                keywords.append("å®æ–½æ•ˆæœæ•°æ®")
            
            return ", ".join(keywords) if keywords else "ç›¸å…³é¡¹ç›®ä¿¡æ¯"
        return "é¡¹ç›®ç›¸å…³ä¿¡æ¯ï¼ˆå¾…æ£€ç´¢ï¼‰"
    
    async def _generate_standard_metrics(self, enriched_info: dict) -> MetricAnalysisTable:
        """
        åŸºäºé¡¹ç›®ç‰¹ç‚¹åŠ¨æ€ç”Ÿæˆç»©æ•ˆæŒ‡æ ‡ä½“ç³»
        ä¸€çº§æŒ‡æ ‡å›ºå®šä¸ºï¼šå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Š
        äºŒçº§ã€ä¸‰çº§æŒ‡æ ‡æ ¹æ®é¡¹ç›®ç‰¹ç‚¹ç”±LLMåŠ¨æ€ç”Ÿæˆ
        """
        project_name = enriched_info.get('project_name', 'é¡¹ç›®')
        project_type = enriched_info.get('project_type', 'è´¢æ”¿æ”¯å‡ºé¡¹ç›®')
        
        # æ„é€ æŒ‡æ ‡è®¾è®¡prompt
        metrics_design_prompt = f"""
ä½ æ˜¯ç»©æ•ˆè¯„ä»·æŒ‡æ ‡ä½“ç³»çš„æ¶æ„å¸ˆã€‚è¯·åŸºäºä»¥ä¸‹é¡¹ç›®ä¿¡æ¯ï¼Œè®¾è®¡ä¸€å¥—å®Œæ•´çš„ç»©æ•ˆè¯„ä»·æŒ‡æ ‡ä½“ç³»ã€‚

é¡¹ç›®ä¿¡æ¯ï¼š
{json.dumps(enriched_info, ensure_ascii=False, indent=2)}

æŒ‡æ ‡ä½“ç³»è®¾è®¡è¦æ±‚ï¼š
1. ä¸€çº§æŒ‡æ ‡å›ºå®šä¸ºï¼šå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šï¼ˆæ¯ä¸ªä¸€çº§æŒ‡æ ‡ä¸‹éœ€è¦2-3ä¸ªäºŒçº§æŒ‡æ ‡ï¼‰
2. æ¯ä¸ªäºŒçº§æŒ‡æ ‡ä¸‹è®¾ç½®1-2ä¸ªä¸‰çº§æŒ‡æ ‡
3. åˆ†å€¼åˆ†é…ï¼šå†³ç­–(25åˆ†)ã€è¿‡ç¨‹(25åˆ†)ã€äº§å‡º(25åˆ†)ã€æ•ˆç›Š(25åˆ†)
4. æŒ‡æ ‡è¦ç¬¦åˆé¡¹ç›®ç‰¹ç‚¹ï¼Œå…·ä½“ã€å¯è¡¡é‡
5. è¯„åˆ†è§„åˆ™è¦æ˜ç¡®ã€å¯æ“ä½œ
6. è¯„åˆ†è¿‡ç¨‹è¦ç»™å‡ºå…·ä½“çš„è¯„ä»·æ–¹æ³•æŒ‡å¯¼

è¯·è¿”å›JSONæ ¼å¼ï¼Œæ¯ä¸ªæŒ‡æ ‡åŒ…å«ï¼š
- metric_id: å”¯ä¸€æ ‡è¯†ï¼ˆè‹±æ–‡ï¼‰
- name: æŒ‡æ ‡åç§°ï¼ˆä¸­æ–‡ï¼‰
- category: æŒ‡æ ‡åˆ†ç±»
- ä¸€çº§æŒ‡æ ‡: "å†³ç­–"/"è¿‡ç¨‹"/"äº§å‡º"/"æ•ˆç›Š"
- äºŒçº§æŒ‡æ ‡: å…·ä½“çš„äºŒçº§æŒ‡æ ‡åç§°
- ä¸‰çº§æŒ‡æ ‡: å…·ä½“çš„ä¸‰çº§æŒ‡æ ‡åç§°
- åˆ†å€¼: æ•°å€¼ï¼ˆæ€»è®¡100åˆ†ï¼‰
- è¯„åˆ†è§„åˆ™: è¯¥æŒ‡æ ‡çš„è¯„ä»·æ ‡å‡†å’Œè¦æ±‚
- è¯„åˆ†è¿‡ç¨‹: è¯¥æŒ‡æ ‡çš„è¯„ä»·æ–¹æ³•æŒ‡å¯¼ï¼ˆå‘Šè¯‰Writerå¦‚ä½•è¿›è¡Œè¯„ä»·ï¼‰

ç¤ºä¾‹æ ¼å¼ï¼š
[
  {{
    "metric_id": "project_necessity",
    "name": "é¡¹ç›®ç«‹é¡¹å¿…è¦æ€§",
    "category": "å†³ç­–æŒ‡æ ‡",
    "ä¸€çº§æŒ‡æ ‡": "å†³ç­–",
    "äºŒçº§æŒ‡æ ‡": "ç«‹é¡¹å†³ç­–",
    "ä¸‰çº§æŒ‡æ ‡": "ç«‹é¡¹å¿…è¦æ€§",
    "åˆ†å€¼": 8.0,
    "è¯„åˆ†è§„åˆ™": "è¯„ä¼°é¡¹ç›®ç«‹é¡¹çš„å¿…è¦æ€§å’Œè¿«åˆ‡æ€§ï¼Œæ˜¯å¦ç¬¦åˆæ”¿ç­–å¯¼å‘å’Œå®é™…éœ€æ±‚",
    "è¯„åˆ†è¿‡ç¨‹": "Writeréœ€æ£€æŸ¥é¡¹ç›®èƒŒæ™¯åˆ†æã€éœ€æ±‚è°ƒç ”æŠ¥å‘Šã€æ”¿ç­–ä¾æ®ç­‰ææ–™çš„å®Œæ•´æ€§å’Œå……åˆ†æ€§è¿›è¡Œè¯„åˆ†"
  }}
]

è¯·ä¸º{project_name}ï¼ˆ{project_type}ï¼‰è®¾è®¡8-12ä¸ªæŒ‡æ ‡ï¼Œç¡®ä¿è¦†ç›–å››ä¸ªä¸€çº§æŒ‡æ ‡ç»´åº¦ã€‚
"""
        
        try:
            metrics_result = await self._aask(metrics_design_prompt)
            
            # ä»LLMå›å¤ä¸­æå–JSONå†…å®¹
            metrics_data = self._extract_json_from_llm_response(metrics_result)
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œä¸€çº§æŒ‡æ ‡åˆ†å¸ƒ
            validated_metrics = self._validate_metrics_structure(metrics_data)
            
            logger.info(f"ğŸ“Š åŠ¨æ€ç”Ÿæˆäº† {len(validated_metrics)} ä¸ªç»©æ•ˆæŒ‡æ ‡")
            logger.info(f"ğŸ“Š æŒ‡æ ‡åˆ†å¸ƒ - å†³ç­–:{self._count_metrics_by_level1(validated_metrics, 'å†³ç­–')}ä¸ª, "
                       f"è¿‡ç¨‹:{self._count_metrics_by_level1(validated_metrics, 'è¿‡ç¨‹')}ä¸ª, "
                       f"äº§å‡º:{self._count_metrics_by_level1(validated_metrics, 'äº§å‡º')}ä¸ª, "
                       f"æ•ˆç›Š:{self._count_metrics_by_level1(validated_metrics, 'æ•ˆç›Š')}ä¸ª")
            
            return MetricAnalysisTable(data_json=json.dumps(validated_metrics, ensure_ascii=False))
            
        except Exception as e:
            logger.error(f"LLMæŒ‡æ ‡ç”Ÿæˆå¤±è´¥ï¼Œæ¡ä»¶ä¸è¶³æ— æ³•æ„å»ºæŒ‡æ ‡ä½“ç³»: {e}")
            # ä¸ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼Œç›´æ¥è¿”å›ç©ºæŒ‡æ ‡è¡¨ç¤ºæ— æ³•æ„å»º
            empty_metrics = {
                "error": "æ¡ä»¶ä¸è¶³ï¼Œæ— æ³•æ„å»ºæŒ‡æ ‡ä½“ç³»",
                "reason": str(e),
                "suggestion": "è¯·ç¡®ä¿é¡¹ç›®ä¿¡æ¯å®Œæ•´åé‡æ–°ç”Ÿæˆ"
            }
            return MetricAnalysisTable(data_json=json.dumps(empty_metrics, ensure_ascii=False))
    
    def _validate_metrics_structure(self, metrics_data: List[dict]) -> List[dict]:
        """
        éªŒè¯æŒ‡æ ‡æ•°æ®ç»“æ„çš„å®Œæ•´æ€§
        """
        validated_metrics = []
        required_fields = ['metric_id', 'name', 'category', 'ä¸€çº§æŒ‡æ ‡', 'äºŒçº§æŒ‡æ ‡', 'ä¸‰çº§æŒ‡æ ‡', 'åˆ†å€¼', 'è¯„åˆ†è§„åˆ™', 'è¯„åˆ†è¿‡ç¨‹']
        
        for metric in metrics_data:
            if all(field in metric for field in required_fields):
                # ç¡®ä¿ä¸€çº§æŒ‡æ ‡åªèƒ½æ˜¯å›ºå®šçš„å››ä¸ªå€¼
                if metric['ä¸€çº§æŒ‡æ ‡'] in ['å†³ç­–', 'è¿‡ç¨‹', 'äº§å‡º', 'æ•ˆç›Š']:
                    validated_metrics.append(metric)
                else:
                    logger.warning(f"æŒ‡æ ‡ {metric.get('name', 'æœªçŸ¥')} çš„ä¸€çº§æŒ‡æ ‡ä¸ç¬¦åˆè¦æ±‚: {metric.get('ä¸€çº§æŒ‡æ ‡', '')}")
            else:
                logger.warning(f"æŒ‡æ ‡æ•°æ®ä¸å®Œæ•´: {metric}")
        
        return validated_metrics
    
    def _count_metrics_by_level1(self, metrics: List[dict], level1: str) -> int:
        """
        ç»Ÿè®¡æŒ‡å®šä¸€çº§æŒ‡æ ‡ä¸‹çš„æŒ‡æ ‡æ•°é‡
        """
        return len([m for m in metrics if m.get('ä¸€çº§æŒ‡æ ‡') == level1])
    
