#!/usr/bin/env python3
"""
æœ¬åœ°å‘é‡åŒ–æœåŠ¡
å€Ÿé‰´ç°æœ‰æœåŠ¡çš„å®ç°æ–¹å¼ï¼Œä¸ºxsearchæä¾›æœ¬åœ°æ–‡æ¡£å‘é‡åŒ–èƒ½åŠ›
å‚è€ƒå·²éªŒè¯çš„backend/servicesä»£ç 
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import asyncio

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    # ğŸ”§ å‚è€ƒå·²éªŒè¯çš„ä»£ç ï¼šä½¿ç”¨MetaGPTçš„SimpleEngine
    from metagpt.rag.engines.simple import SimpleEngine
    from metagpt.rag.schema import FAISSRetrieverConfig
    from metagpt.config2 import Config
    from llama_index.llms.openai_like import OpenAILike
    from llama_index.embeddings.dashscope import DashScopeEmbedding
    VECTOR_SERVICES_AVAILABLE = True
except ImportError as e:
    VECTOR_SERVICES_AVAILABLE = False
    print(f"âš ï¸ å‘é‡åŒ–æœåŠ¡ä¾èµ–ä¸å¯ç”¨: {e}")


class LocalVectorService:
    """æœ¬åœ°å‘é‡åŒ–æœåŠ¡ - ä½¿ç”¨MetaGPT SimpleEngine"""
    
    def __init__(self, project_config: Dict[str, Any]):
        self.project_config = project_config
        self.doc_dir = Path(project_config['project_root']) / 'xsearch' / 'doc'
        self.index_dir = Path(project_config['project_root']) / 'xsearch' / 'vector_index'
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.doc_dir.mkdir(parents=True, exist_ok=True)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®
        self._config = None
        self._engine = None
    
    def _get_config(self) -> Config:
        """è·å–ç³»ç»Ÿé…ç½®"""
        if self._config is None:
            try:
                config_path = Path(self.project_config['project_root']) / 'config' / 'config2.yaml'
                self._config = Config.from_yaml_file(config_path)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                self._config = None
        return self._config
    
    def _create_llm_and_embed_model(self):
        """åˆ›å»ºLLMå’ŒåµŒå…¥æ¨¡å‹"""
        config = self._get_config()
        if not config:
            return None, None
        
        try:
            llm_config = config.llm
            embed_config = config.embedding
            
            # åˆ›å»ºLLM
            llm = OpenAILike(
                model=llm_config.model,
                api_base=llm_config.base_url,
                api_key=llm_config.api_key,
                is_chat_model=True
            )
            
            # åˆ›å»ºEmbeddingæ¨¡å‹
            from metagpt.rag.factories.embedding import get_rag_embedding
            embed_model = get_rag_embedding(config=config)
            embed_model.embed_batch_size = 5  # ä¿®å¤ï¼šé™ä½batch sizeé¿å…400é”™è¯¯
            
            return llm, embed_model
            
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def collect_local_documents(self) -> List[str]:
        """æ”¶é›†æœ¬åœ°æ–‡æ¡£æ–‡ä»¶è·¯å¾„"""
        documents = []
        
        if not self.doc_dir.exists():
            print(f"âš ï¸ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {self.doc_dir}")
            return documents
        
        # æ”¶é›†æ‰€æœ‰.mdå’Œ.txtæ–‡ä»¶
        for pattern in ["*.md", "*.txt"]:
            for file_path in self.doc_dir.glob(pattern):
                try:
                    print(f"ğŸ“„ åŠ è½½æ–‡æ¡£: {file_path.name}")
                    documents.append(str(file_path))
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
        
        print(f"ğŸ“š å…±æ”¶é›†åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def is_index_exists(self) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨"""
        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•æ–‡ä»¶å
        index_files = ['docstore.json', 'index_store.json', 'default__vector_store.json']
        return all((self.index_dir / f).exists() for f in index_files)
    
    async def build_vector_index(self, force_rebuild: bool = False) -> bool:
        """æ„å»ºå‘é‡ç´¢å¼•"""
        if not VECTOR_SERVICES_AVAILABLE:
            print("âš ï¸ å‘é‡åŒ–æœåŠ¡ä¸å¯ç”¨")
            return False
        
        try:
            if self.is_index_exists() and not force_rebuild:
                print("âœ… æœ¬åœ°å‘é‡ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º")
                return True
            
            # æ”¶é›†æ–‡æ¡£
            documents = self.collect_local_documents()
            if not documents:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºç´¢å¼•")
                return False
            
            # åˆ›å»ºæ¨¡å‹
            llm, embed_model = self._create_llm_and_embed_model()
            if not embed_model:
                print("âŒ åµŒå…¥æ¨¡å‹åˆ›å»ºå¤±è´¥")
                return False
            
            print("ğŸ”§ å¼€å§‹æ„å»ºæœ¬åœ°å‘é‡ç´¢å¼•...")
            
            # ğŸ”§ å‚è€ƒå·²éªŒè¯çš„ä»£ç ï¼šä½¿ç”¨MetaGPT SimpleEngine
            engine = SimpleEngine.from_docs(
                input_files=documents,
                llm=llm,
                embed_model=embed_model,
                retriever_configs=[FAISSRetrieverConfig(dimensions=1024)]  # ä½¿ç”¨FAISSæ”¯æŒæŒä¹…åŒ–
            )
            
            # æŒä¹…åŒ–ç´¢å¼•
            engine.persist(str(self.index_dir))
            
            print(f"âœ… æœ¬åœ°å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {self.index_dir}")
            return True
            
        except Exception as e:
            print(f"âŒ æ„å»ºæœ¬åœ°å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    async def search_local_documents(self, query: str, top_k: int = 5) -> List[str]:
        """æœç´¢æœ¬åœ°æ–‡æ¡£"""
        if not VECTOR_SERVICES_AVAILABLE:
            print("âš ï¸ å‘é‡åŒ–æœåŠ¡ä¸å¯ç”¨")
            return []
        
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if not self.is_index_exists():
                print("âš ï¸ æœ¬åœ°å‘é‡ç´¢å¼•ä¸å­˜åœ¨ï¼Œå°è¯•æ„å»º...")
                if not await self.build_vector_index():
                    return []
            
            # åŠ è½½ç´¢å¼•
            if not self._engine:
                await self._load_index()
            
            if not self._engine:
                print("âŒ æ— æ³•åŠ è½½å‘é‡ç´¢å¼•")
                return []
            
            # æ‰§è¡Œæœç´¢
            results = await self._engine.aretrieve(query)
            
            # æå–æœç´¢ç»“æœ
            search_results = []
            for result in results[:top_k]:
                if hasattr(result, 'text'):
                    search_results.append(result.text.strip())
            
            print(f"ğŸ” æœ¬åœ°æ–‡æ¡£æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} æ¡ç»“æœ")
            return search_results
            
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _load_index(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„ç´¢å¼•"""
        try:
            if not self.index_dir.exists():
                return False
            
            # åˆ›å»ºæ¨¡å‹
            llm, embed_model = self._create_llm_and_embed_model()
            if not embed_model:
                return False
            
            print("ğŸ“– åŠ è½½å·²å­˜åœ¨çš„æœ¬åœ°å‘é‡ç´¢å¼•...")
            
            # ğŸ”§ å‚è€ƒå·²éªŒè¯çš„ä»£ç ï¼šä½¿ç”¨MetaGPT SimpleEngine.from_index
            from metagpt.rag.schema import FAISSIndexConfig
            
            config = FAISSIndexConfig(
                persist_path=str(self.index_dir),
                embed_model=embed_model
            )
            
            self._engine = SimpleEngine.from_index(
                index_config=config,
                embed_model=embed_model,
                llm=llm,
                retriever_configs=[FAISSRetrieverConfig(dimensions=1024)]
            )
            
            print(f"âœ… æœ¬åœ°å‘é‡ç´¢å¼•åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æœ¬åœ°å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "doc_dir": str(self.doc_dir),
            "index_dir": str(self.index_dir),
            "index_exists": self.is_index_exists(),
            "doc_count": 0,
            "index_loaded": self._engine is not None
        }
        
        # ç»Ÿè®¡æ–‡æ¡£æ•°é‡
        if self.doc_dir.exists():
            for pattern in ["*.md", "*.txt"]:
                stats["doc_count"] += len(list(self.doc_dir.glob(pattern)))
        
        return stats
