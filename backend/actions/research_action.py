#!/usr/bin/env python
"""
ç ”ç©¶Actioné›†åˆ - ProductManagerä¸“ç”¨
æ•´åˆMetaGPTåŸç”ŸProductManagerçš„ä¼˜ç§€å®è·µå’ŒCaseExpertçš„ç ”ç©¶èƒ½åŠ›
å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–æç¤ºè¯å’Œç ”ç©¶é€»è¾‘
"""
import asyncio
from datetime import datetime
from typing import List, Optional, Dict, Tuple
import hashlib
import json
import time
from pathlib import Path
from urllib.parse import urlparse

from pydantic import BaseModel, Field, TypeAdapter
from metagpt.actions import Action
from metagpt.logs import logger
from metagpt.tools.search_engine import SearchEngine
from metagpt.tools.web_browser_engine import WebBrowserEngine
from metagpt.utils.project_repo import ProjectRepo
from metagpt.utils.common import OutputParser
from backend.tools.search_utils import normalize_keywords
from backend.tools.json_utils import extract_json_from_llm_response
from backend.tools.project_info import get_project_info_text
from backend.config.research_prompts import (
    COMPREHENSIVE_RESEARCH_BASE_SYSTEM,  # ç»¼åˆç ”ç©¶åŸºç¡€ç³»ç»Ÿæç¤ºï¼ˆäº‹å®å¯†åº¦/è´¨é‡è¦æ±‚ï¼‰
    RESEARCH_TOPIC_SYSTEM,               # ä¸»é¢˜è®¾å®šæ¨¡æ¿
    SEARCH_KEYWORDS_PROMPT,              # å…³é”®è¯ç”Ÿæˆæç¤º
    DECOMPOSE_RESEARCH_PROMPT,           # ç ”ç©¶é—®é¢˜åˆ†è§£æç¤º
    RANK_URLS_PROMPT,                    # URL æ’åºæç¤ºï¼ˆç™½åå•çº¦æŸï¼‰
    WEB_CONTENT_ANALYSIS_PROMPT,         # ç½‘é¡µå†…å®¹åˆ†ææå–æç¤º
    GENERATE_RESEARCH_BRIEF_PROMPT,      # ç»Ÿä¸€ç”Ÿæˆç®€æŠ¥æ¨¡æ¿ï¼ˆ6é”®ï¼‰
    ENHANCEMENT_QUERIES,                 # ç ”ç©¶å¢å¼ºæŸ¥è¯¢æ¨¡æ¿
    RESEARCH_DECOMPOSITION_NUMS,         # é—®é¢˜åˆ†è§£æ•°é‡
    RESEARCH_URLS_PER_QUERY,             # æ¯é—®é¢˜URLæ•°é‡
    MAX_INPUT_TOKENS,                    # LLMè¾“å…¥é•¿åº¦é™åˆ¶
    DECOMPOSITION_DIMENSIONS,            # ç»Ÿä¸€ç ”ç©¶æ–¹å‘ç»´åº¦ç§å­
)

# MetaGPT åŸç”Ÿ RAG ç»„ä»¶ - å¼ºåˆ¶ä½¿ç”¨ï¼Œä¸å†æä¾›ç®€åŒ–ç‰ˆæœ¬


# --- æç¤ºè¯æ”¹ä¸ºé…ç½®é©±åŠ¨ï¼Œç§»é™¤ç¡¬ç¼–ç  ---


class Document(BaseModel):
    """å•ä¸ªæ–‡æ¡£çš„ç»“æ„åŒ–æ¨¡å‹"""
    filename: str
    content: str


class Documents(BaseModel):
    """æ–‡æ¡£é›†åˆçš„ç»“æ„åŒ–æ¨¡å‹"""
    docs: List[Document] = Field(default_factory=list)


class ResearchData(BaseModel):
    """ç ”ç©¶æˆæœçš„ç»“æ„åŒ–æ•°æ®æ¨¡å‹ï¼ˆå·²å¯¹é½æ–°çŸ¥è¯†åº“é€»è¾‘ï¼Œä»…ä¿ç•™å¿…è¦å­—æ®µï¼‰"""
    brief: str = Field(..., description="åŸºäºç ”ç©¶ç”Ÿæˆçš„ç»¼åˆæ€§ç®€æŠ¥")
    vector_store_path: str = Field(..., description="å­˜å‚¨ç ”ç©¶å†…å®¹å‘é‡ç´¢å¼•çš„è·¯å¾„")


class PrepareDocuments(Action):
    """æ‰«ææœ¬åœ°ç›®å½•ï¼ŒåŠ è½½ç”¨æˆ·æä¾›çš„æ–‡æ¡£ä½œä¸ºç ”ç©¶ææ–™"""
    
    async def run(self, uploads_path: Path) -> Documents:
        """æ‰«æuploadsç›®å½•ï¼Œè¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹"""
        docs = []
        if not uploads_path.exists():
            logger.warning(f"ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨: {uploads_path}")
            return Documents(docs=docs)

        logger.info(f"å¼€å§‹æ‰«ææ–‡æ¡£ç›®å½•: {uploads_path}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {'.txt', '.md', '.csv', '.json', '.yaml', '.yml'}
        
        for file_path in uploads_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                try:
                    content = file_path.read_text(encoding='utf-8')
                    docs.append(Document(filename=file_path.name, content=content))
                    logger.info(f"æˆåŠŸè¯»å–æ–‡æ¡£: {file_path.name}")
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"æ–‡æ¡£æ‰«æå®Œæˆï¼Œå…±è¯»å– {len(docs)} ä¸ªæ–‡æ¡£")
        return Documents(docs=docs)


class ConductComprehensiveResearch(Action):
    """
    ç»¼åˆç ”ç©¶Action - æ•´åˆæœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œç ”ç©¶
    å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–ç ”ç©¶é€»è¾‘å’Œæç¤ºè¯
    """
    
    def __init__(self, search_engine: SearchEngine = None, **kwargs):
        super().__init__(**kwargs)
        self.search_engine = search_engine

    async def run(
        self, 
        topic: str,
        decomposition_nums: int = RESEARCH_DECOMPOSITION_NUMS,
        url_per_query: int = RESEARCH_URLS_PER_QUERY,
        project_repo: ProjectRepo = None,
        local_docs: Documents = None
    ) -> ResearchData:
        """æ‰§è¡Œå…¨é¢çš„ç ”ç©¶ï¼Œæ•´åˆç½‘ç»œæœç´¢å’Œæœ¬åœ°æ–‡æ¡£ï¼Œæ„å»ºå‘é‡çŸ¥è¯†åº“"""
        logger.info(f"å¼€å§‹å¯¹ä¸»é¢˜ '{topic}' è¿›è¡Œå…¨é¢ç ”ç©¶ï¼ŒåŒ…æ‹¬å‘é‡åŒ–å¤„ç†...")

        # 1. ã€ä¼˜å…ˆã€‘ä½¿ç”¨ç»Ÿä¸€æ··åˆæ£€ç´¢æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“ï¼ˆå¦‚æœæä¾›æœ¬åœ°æ–‡æ¡£ï¼‰
        vector_store_path = ""
        if local_docs and local_docs.docs:
            logger.info("æ£€æµ‹åˆ°æœ¬åœ°æ–‡æ¡£ï¼Œä½¿ç”¨ç»Ÿä¸€æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“...")
            vector_store_path, _ = await self._build_project_knowledge_base_unified(
                topic, local_docs, project_repo
            )
            logger.info(f"âœ… é¡¹ç›®çŸ¥è¯†åº“æ„å»ºæˆåŠŸ: {vector_store_path}")
            logger.info("âœ… ç»Ÿä¸€æ£€ç´¢æœåŠ¡å·²å‡†å¤‡å°±ç»ªã€‚")
        
        # 2. ç½‘ç»œç ”ç©¶ï¼ˆç»Ÿä¸€ç ”ç©¶æ–¹å‘ä¸ç”Ÿæˆè§„åˆ™ï¼Œä¸å†æŒ‰ SOP åˆ†æ”¯ï¼‰

        online_research_content = await self._conduct_online_research(
            topic, 
            decomposition_nums, 
            url_per_query,
            project_vector_path=vector_store_path  # ä¼ é€’é¡¹ç›®çŸ¥è¯†åº“è·¯å¾„ç”¨äºRAGå¢å¼º
        )

        # 3. ä¿å­˜ç½‘ç»œç ”ç©¶å†…å®¹åˆ°èµ„æºç›®å½•ï¼ˆä¸è¿›å…¥æœ¬åœ°å‘é‡åº“ï¼‰å¹¶ç¡®ä¿å­˜åœ¨é¡¹ç›®çŸ¥è¯†åº“è·¯å¾„
        if online_research_content and project_repo:
            try:
                from datetime import datetime as _dt
                ts = _dt.now().strftime("%Y%m%d%H%M%S")
                fname = f"ç½‘ç»œæ¡ˆä¾‹æ·±åº¦ç ”ç©¶æŠ¥å‘Š{ts}.md"
                await project_repo.resources.save(filename=fname, content=online_research_content)
                logger.info(f"ğŸ“„ ç½‘ç»œç ”ç©¶å†…å®¹å·²ä¿å­˜åˆ°èµ„æºç›®å½•: {project_repo.resources.workdir / fname}")
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜ç½‘ç»œç ”ç©¶å†…å®¹åˆ°èµ„æºç›®å½•å¤±è´¥: {e}")
        if not vector_store_path:
            # è‹¥å°šæœªåˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªç©ºçš„ä»¥ä¿è¯åç»­æµç¨‹ä¾èµ–
            logger.info("ğŸ“ åˆ›å»ºç©ºé¡¹ç›®çŸ¥è¯†åº“ï¼ˆä¸åŒ…å«ç½‘ç»œç ”ç©¶å†…å®¹ï¼‰...")
            vector_store_path, _ = await self._build_project_knowledge_base_unified(
                topic, Documents(), project_repo, online_content=""
            )

        # 4. ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼ºå†…å®¹æ•´åˆï¼ˆåŠ å…¥å¯ç»´æŠ¤çš„ research_brief å†å²ï¼Œä¾›åç»­ä¸Šä¸‹æ–‡ç›´æ¥å¼•ç”¨ï¼‰
        combined_content = online_research_content
        if local_docs and local_docs.docs:
            local_docs_content = "\n\n--- æœ¬åœ°çŸ¥è¯†åº“ ---\n"
            for doc in local_docs.docs:
                local_docs_content += f"### æ–‡æ¡£: {doc.filename}\n{doc.content}\n\n"
            combined_content += local_docs_content
        
        # ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢å¢å¼ºç ”ç©¶ç®€æŠ¥ç”Ÿæˆ
        enhanced_content = await self._enhance_research_with_intelligent_search(
            topic, combined_content, vector_store_path
        )

        # æ”¶é›†å…è®¸å¼•ç”¨çš„æ¥æºç™½åå•
        allowed_web_urls: List[str] = []
        try:
            # ä»ç ”ç©¶å†…å®¹ä¸­æå–æµè§ˆè¿‡çš„URLï¼ˆ#### æ¥æº: <url>ï¼‰
            if online_research_content:
                for line in online_research_content.splitlines():
                    if line.startswith("#### æ¥æº:"):
                        url = line.split("#### æ¥æº:", 1)[1].strip()
                        if url.startswith("http"):
                            allowed_web_urls.append(url)
        except Exception:
            pass
        # å»é‡
        allowed_web_urls = list(dict.fromkeys(allowed_web_urls))

        allowed_project_docs: List[str] = []
        try:
            # æ¥è‡ªèµ„æºç›®å½•çš„ç½‘ç»œæ¡ˆä¾‹æ·±åº¦ç ”ç©¶æŠ¥å‘Š
            if project_repo:
                rp = project_repo.resources.workdir
                if rp.exists() and rp.is_dir():
                    for fp in rp.glob("ç½‘ç»œæ¡ˆä¾‹æ·±åº¦ç ”ç©¶æŠ¥å‘Š*.md"):
                        allowed_project_docs.append(fp.name)
            # æœ¬è½®æœ¬åœ°ä¸Šä¼ æ–‡æ¡£æ–‡ä»¶å
            if local_docs and local_docs.docs:
                for d in local_docs.docs:
                    allowed_project_docs.append(d.filename)
            allowed_project_docs = list(dict.fromkeys(allowed_project_docs))
        except Exception:
            pass

        # ä½¿ç”¨ç»Ÿä¸€çš„å¯ç»´æŠ¤ç®€æŠ¥æ¨¡æ¿ï¼ˆä»…6é”®ï¼‰
        prompt_template = GENERATE_RESEARCH_BRIEF_PROMPT
        time_stamp = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
        prompt = (
            prompt_template
            .replace("{content}", enhanced_content)
            .replace("{topic}", topic)
            .replace("{time_stamp}", time_stamp)
            .replace("{allowed_web_urls}", json.dumps(allowed_web_urls, ensure_ascii=False))
            .replace("{allowed_project_docs}", json.dumps(allowed_project_docs, ensure_ascii=False))
        )
        # é˜²æ­¢è¶…é•¿è¾“å…¥è§¦å‘åº•å±‚æä¾›å•†é•¿åº¦é™åˆ¶ï¼šæˆªæ–­åˆ°å®‰å…¨é•¿åº¦
        safe_prompt = prompt[:MAX_INPUT_TOKENS]
        # æ³¨å…¥é¡¹ç›®é…ç½®ä¿¡æ¯ä½œä¸ºç³»ç»Ÿçº§æç¤ºï¼Œç»Ÿä¸€å¯¹é½ä¸Šä¸‹æ–‡
        project_info_text = get_project_info_text()
        brief = await self._aask(safe_prompt, [COMPREHENSIVE_RESEARCH_BASE_SYSTEM, project_info_text])
        
        logger.info(f"ç ”ç©¶ç®€æŠ¥æ­£åœ¨ç”Ÿæˆ,æ•°åˆ†é’Ÿåè¯·æŸ¥é˜…ã€‚")

        # 5. ç¡®ä¿æœ€ç»ˆå‘é‡åº“è·¯å¾„å­˜åœ¨ï¼ˆå¿…é¡»ï¼‰
        if not vector_store_path:
            raise ValueError("é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç ”ç©¶æµç¨‹")

        # 6. åˆ›å»ºå¹¶è¿”å›ResearchData
        research_data = ResearchData(
            brief=brief,
            vector_store_path=vector_store_path,
        )

        if project_repo:
            # ç”Ÿæˆâ€œå¯ç»´æŠ¤ç ”ç©¶ç®€æŠ¥â€ï¼šå­—æ®µçº§LLMåˆå¹¶ï¼Œä»…ä¿ç•™6é”®+å…ƒä¿¡æ¯
            docs_filename = "research_brief.md"
            from pathlib import Path as _Path
            brief_file = _Path(project_repo.docs.workdir) / docs_filename
            old_brief = {}
            try:
                if brief_file.exists():
                    old_text = brief_file.read_text(encoding="utf-8").strip()
                    old_brief = extract_json_from_llm_response(old_text)
                    if not isinstance(old_brief, dict):
                        old_brief = {}
            except Exception:
                old_brief = {}

            # new_notes ä½¿ç”¨å¢å¼ºåçš„ç ”ç©¶å†…å®¹
            new_notes = enhanced_content or online_research_content or ""
            merged = await self._merge_research_brief(old_brief, new_notes)
            merged_json = json.dumps(merged, ensure_ascii=False, indent=2)
            await project_repo.docs.save(filename=docs_filename, content=merged_json)
            brief_path = project_repo.docs.workdir / docs_filename
            logger.info(f"ç ”ç©¶ç®€æŠ¥å·²ä¿å­˜åˆ°: {brief_path}")

        return research_data

    # ========== ğŸš€ æ–°çš„ç»Ÿä¸€çŸ¥è¯†åº“ç®¡ç†æ–¹æ³• ==========
    
    async def _build_project_knowledge_base_unified(
        self, 
        topic: str, 
        local_docs: Documents, 
        project_repo=None, 
        online_content: str = ""
    ) -> tuple[str, List[str]]:
        """
        ğŸš€ ä½¿ç”¨ç»Ÿä¸€çš„æ··åˆæ£€ç´¢æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“
        """
        try:
            from backend.services.hybrid_search import hybrid_search
            
            # ç¡®å®šé¡¹ç›®ID
            project_id = project_repo.workdir.name if project_repo else f"research_{hash(topic) % 10000}"
            
            # åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“
            project_vector_path = hybrid_search.create_project_knowledge_base(project_id)
            
            # å‡†å¤‡è¦æ·»åŠ çš„å†…å®¹
            contents_to_add = []
            
            # æ·»åŠ æœ¬åœ°æ–‡æ¡£
            if local_docs and local_docs.docs:
                for doc in local_docs.docs:
                    contents_to_add.append({
                        "content": doc.content,
                        "filename": doc.filename
                    })
                logger.info(f"ğŸ“„ å‡†å¤‡æ·»åŠ  {len(local_docs.docs)} ä¸ªæœ¬åœ°æ–‡æ¡£")
            
            # æ·»åŠ ç½‘ç»œç ”ç©¶å†…å®¹
            if online_content:
                contents_to_add.append({
                    "content": online_content,
                    "filename": f"ç½‘ç»œç ”ç©¶_{topic}.md"
                })
                logger.info("ğŸŒ å‡†å¤‡æ·»åŠ ç½‘ç»œç ”ç©¶å†…å®¹")
            
            # æ‰¹é‡æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“
            if contents_to_add:
                success = hybrid_search.add_multiple_contents_to_project(contents_to_add, project_vector_path)
                if not success:
                    logger.warning("âš ï¸ éƒ¨åˆ†å†…å®¹æ·»åŠ å¤±è´¥")
            
            logger.info(f"âœ… ç»Ÿä¸€é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå®Œæˆ: {project_vector_path}")
            return project_vector_path, []
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")
            # ä¸é™çº§ï¼Œè®©é”™è¯¯æš´éœ²å‡ºæ¥ï¼Œå¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€æ¶æ„
            raise e
    
    async def _add_online_content_to_project(self, online_content: str, project_vector_path: str, topic: str, project_repo=None):
        """å°†ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ åˆ°ç°æœ‰é¡¹ç›®çŸ¥è¯†åº“"""
        try:
            from backend.services.hybrid_search import hybrid_search
            from datetime import datetime
            
            # å›ºå®šæ›´å‹å¥½çš„æ–‡ä»¶åå¹¶è¿½åŠ æ—¶é—´æˆ³ï¼Œé¿å…è¿‡é•¿/å«ç‰¹æ®Šå­—ç¬¦ä¸è¦†ç›–
            ts = datetime.now().strftime("%Y%m%d%H%M%S")
            success = hybrid_search.add_content_to_project(
                content=online_content,
                filename=f"ç½‘ç»œæ¡ˆä¾‹æ·±åº¦ç ”ç©¶æŠ¥å‘Š{ts}.md",
                project_vector_storage_path=project_vector_path
            )
            
            if success:
                logger.info("âœ… ç½‘ç»œç ”ç©¶å†…å®¹å·²æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“")
            else:
                logger.warning("âš ï¸ ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç½‘ç»œå†…å®¹å¤±è´¥: {e}")
    
    # å·²å¯¹é½ç»Ÿä¸€æ£€ç´¢é€»è¾‘ï¼šä¸å†æ‰‹å·¥è¯»å–å†…å®¹å—
    
    
    async def _enhance_research_with_intelligent_search(
        self, 
        topic: str, 
        combined_content: str, 
        vector_store_path: str
    ) -> str:
        """
        ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢å¢å¼ºç ”ç©¶å†…å®¹
        """
        try:
            from backend.services.intelligent_search import intelligent_search
            
            # ğŸ§  é’ˆå¯¹ç ”ç©¶ç®€æŠ¥ç”Ÿæˆçš„ä¸“é—¨æŸ¥è¯¢ï¼ˆé…ç½®é©±åŠ¨ï¼‰
            enhancement_queries = [q.replace('{topic}', topic) for q in ENHANCEMENT_QUERIES] if ENHANCEMENT_QUERIES else []
            
            enhanced_sections = []
            
            for query in enhancement_queries:
                logger.info(f"ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼º: {query}")
                search_result = await intelligent_search.intelligent_search(
                    query=query,
                    project_vector_storage_path=vector_store_path,
                    mode="hybrid",  # ç”±æƒé‡ä¸æ„å›¾è·¯ç”±å†³å®šæ˜¯å¦èµ°KG
                    enable_global=True,
                    max_results=5
                )
                
                if search_result.get("results"):
                    enhanced_sections.append(f"### ğŸ§  æ™ºèƒ½æ£€ç´¢: {query}\n")
                    enhanced_sections.extend(search_result["results"])
                    enhanced_sections.append("\n")
            
            if enhanced_sections:
                enhanced_content = combined_content + "\n\n--- ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼ºå†…å®¹ ---\n" + "\n".join(enhanced_sections)
                logger.info(f"âœ… æ™ºèƒ½æ£€ç´¢å¢å¼ºå®Œæˆï¼Œæ–°å¢ {len(enhanced_sections)} ä¸ªå†…å®¹æ®µ")
                return enhanced_content
            else:
                logger.info("â„¹ï¸ æ™ºèƒ½æ£€ç´¢æœªå‘ç°é¢å¤–å†…å®¹")
                return combined_content
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ™ºèƒ½æ£€ç´¢å¢å¼ºå¤±è´¥: {e}")
            return combined_content

    async def _llm_merge_text(self, field_name: str, old_text: str, new_notes: str) -> str:
        from backend.config.research_prompts import MERGE_FIELD_PROMPT, MAX_INPUT_TOKENS
        prompt = MERGE_FIELD_PROMPT.format(field_name=field_name, old_text=old_text or "ï¼ˆæ— ï¼‰", new_notes=new_notes or "ï¼ˆæ— ï¼‰")
        project_info_text = get_project_info_text()
        merged = await self._aask(prompt[:MAX_INPUT_TOKENS], [project_info_text])
        return (merged or "").strip()

    async def _llm_merge_events(self, old_events: str, new_notes: str) -> str:
        from backend.config.research_prompts import MERGE_EVENTS_PROMPT, MAX_INPUT_TOKENS
        prompt = MERGE_EVENTS_PROMPT.format(old_events=old_events or "ï¼ˆæ— ï¼‰", new_notes=new_notes or "ï¼ˆæ— ï¼‰")
        project_info_text = get_project_info_text()
        merged = await self._aask(prompt[:MAX_INPUT_TOKENS], [project_info_text])
        return (merged or "").strip()

    async def _merge_research_brief(self, old_brief: dict, new_notes: str) -> dict:
        fields = ["é¡¹ç›®æƒ…å†µ", "èµ„é‡‘æƒ…å†µ", "é‡è¦äº‹ä»¶", "æ”¿ç­–å¼•ç”¨", "æ¨èæ–¹æ³•", "å¯å€Ÿé‰´ç½‘ç»œæ¡ˆä¾‹"]
        merged = {}
        for f in fields:
            old_val = old_brief.get(f, "") if isinstance(old_brief, dict) else ""
            if f == "é‡è¦äº‹ä»¶":
                merged[f] = await self._llm_merge_events(old_val, new_notes)
            else:
                merged[f] = await self._llm_merge_text(f, old_val, new_notes)
        return merged
    
    async def _conduct_online_research(self, topic: str, decomposition_nums: int, url_per_query: int, project_vector_path: str = "") -> str:
        """æ‰§è¡Œåœ¨çº¿ç ”ç©¶"""
        if not self.search_engine:
            logger.error("âŒ æœç´¢å¼•æ“æœªåˆå§‹åŒ–ï¼æ— æ³•è¿›è¡Œåœ¨çº¿ç ”ç©¶")
            raise ValueError("æœç´¢å¼•æ“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œåœ¨çº¿ç ”ç©¶ã€‚è¯·æ£€æŸ¥config/config2.yamlä¸­çš„searché…ç½®")
        
        logger.info("æ­¥éª¤ 1: ç”Ÿæˆæœç´¢å…³é”®è¯")
        keywords_prompt = RESEARCH_TOPIC_SYSTEM.format(topic=topic)
        try:
            project_info_text = get_project_info_text()
            keywords_str = await self._aask(
                SEARCH_KEYWORDS_PROMPT[:MAX_INPUT_TOKENS],
                [keywords_prompt[:MAX_INPUT_TOKENS], project_info_text]
            )
        except Exception as e:
            # å…³é”®è¯é˜¶æ®µå¤±è´¥ç›´æ¥ä¸­æ–­ï¼Œé¿å…åç»­æµç¨‹è´¨é‡ä¸å¯æ§
            raise ValueError(f"å…³é”®è¯ç”Ÿæˆè°ƒç”¨å¤±è´¥: {e}")
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(1)

        # ä½¿ç”¨ç»Ÿä¸€JSONæå–å·¥å…·ï¼Œå…¼å®¹ä»£ç å—/åŒ…è£¹æ–‡æœ¬/éä¸¥æ ¼JSON
        try:
            parsed = extract_json_from_llm_response(keywords_str)
            if isinstance(parsed, list):
                raw_keywords = parsed
            elif isinstance(parsed, dict):
                raw_keywords = parsed.get('keywords', []) or list(parsed.values())
            else:
                raise ValueError('parsed keywords not list/dict')
        except Exception as e:
            # æ˜ç¡®æš´éœ²è§£æé”™è¯¯ï¼Œä¾¿äºå¿«é€Ÿå®šä½
            raise ValueError(f"å…³é”®è¯è§£æå¤±è´¥: {e}; åŸå§‹è¿”å›: {keywords_str}")

        # ç»Ÿä¸€å°†å…³é”®è¯è§„èŒƒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé€šç”¨å·¥å…·ï¼‰
        keywords = normalize_keywords(raw_keywords, topic)

        logger.info(f"å…³é”®è¯: {keywords}")

        # ä¸²è¡Œæœç´¢å…³é”®è¯ï¼Œé¿å…å¹¶å‘è¯·æ±‚è§¦å‘é¢‘ç‡é™åˆ¶
        search_results = []
        for i, kw in enumerate(keywords):
            try:
                if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(2)  # æ¯ä¸ªè¯·æ±‚é—´éš”2ç§’
                result = await self.search_engine.run(kw, as_string=False)
                search_results.append(result)
                logger.info(f"æˆåŠŸæœç´¢å…³é”®è¯: {kw}")
            except Exception as e:
                logger.error(f"æœç´¢å…³é”®è¯å¤±è´¥ {kw}: {e}")
                search_results.append([])  # æ·»åŠ ç©ºç»“æœä¿æŒç´¢å¼•ä¸€è‡´
        
        # ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼ºï¼šä½¿ç”¨æ™ºèƒ½æ£€ç´¢æœåŠ¡æŸ¥è¯¢é¡¹ç›®çŸ¥è¯†åº“
        rag_results_str = ""
        if project_vector_path:
            try:
                from backend.services.intelligent_search import intelligent_search
                logger.info("...ğŸ§  å¯åŠ¨æ™ºèƒ½æ£€ç´¢æŸ¥è¯¢é¡¹ç›®çŸ¥è¯†åº“...")
                
                # ä½¿ç”¨æ··åˆæ™ºèƒ½æ£€ç´¢
                search_result = await intelligent_search.intelligent_search(
                    query=" ".join(keywords),
                    project_vector_storage_path=project_vector_path,
                    mode="hybrid",  # ä½¿ç”¨æ··åˆæ™ºèƒ½æ£€ç´¢
                    enable_global=True,
                    max_results=3
                )
                
                if search_result.get("results"):
                    rag_results_str = "\n\n### ğŸ§  æ™ºèƒ½æ£€ç´¢ç›¸å…³ä¿¡æ¯\n" + "\n".join(search_result["results"])
                    
                    # æ·»åŠ æ™ºèƒ½æ´å¯Ÿ
                    if search_result.get("insights"):
                        rag_results_str += "\n\n### ğŸ’¡ æ™ºèƒ½åˆ†ææ´å¯Ÿ\n" + "\n".join(search_result["insights"])
                        
            except Exception as e:
                logger.warning(f"æ™ºèƒ½æ£€ç´¢æŸ¥è¯¢å¤±è´¥: {e}")
        
        search_results_str = "\n".join([f"#### å…³é”®è¯: {kw}\n{res}\n" for kw, res in zip(keywords, search_results)])
        
        # å°†RAGç»“æœå’Œç½‘ç»œæœç´¢ç»“æœåˆå¹¶
        combined_search_results = search_results_str + rag_results_str

        logger.info("æ­¥éª¤ 2: åˆ†è§£ç ”ç©¶é—®é¢˜ï¼ˆç»Ÿä¸€è·¯å¾„ï¼‰")
        # ç»Ÿä¸€ï¼šä¼˜å…ˆç”¨ç»´åº¦ç›´å‡ºï¼ˆè‹¥é…ç½®å­˜åœ¨ï¼‰ï¼Œå¦åˆ™ç”¨ LLM åˆ†è§£
        queries = []
        # è¿‡æ»¤ç©ºç™½/æ— æ•ˆç»´åº¦ï¼Œé¿å…ç”Ÿæˆç©ºé—®é¢˜æ¨¡æ¿
        dims_seed = [
            d for d in (DECOMPOSITION_DIMENSIONS or [])
            if isinstance(d, str) and d.strip()
        ]
        if dims_seed:
            dims = dims_seed[:max(1, decomposition_nums)]
            queries = [f"å›´ç»•â€˜{d}â€™æå‡ºä¸€ä¸ªä¸ä¸»é¢˜â€˜{topic}â€™å¼ºç›¸å…³ä¸”å¯æ£€ç´¢çš„å…·ä½“é—®é¢˜" for d in dims]
        if not queries:
            decompose_prompt = DECOMPOSE_RESEARCH_PROMPT.format(
                decomposition_nums=decomposition_nums,
                url_per_query=url_per_query,
                search_results=combined_search_results
            )
            project_info_text = get_project_info_text()
            queries_str = await self._aask(
                decompose_prompt[:MAX_INPUT_TOKENS],
                [keywords_prompt[:MAX_INPUT_TOKENS], project_info_text]
            )
            await asyncio.sleep(1)
            try:
                parsed = extract_json_from_llm_response(queries_str)
                if isinstance(parsed, list):
                    queries = parsed
                elif isinstance(parsed, dict):
                    for key in ("queries", "questions", "items"):
                        if key in parsed and isinstance(parsed[key], list):
                            queries = parsed[key]
                            break
                    else:
                        queries = [v for v in parsed.values() if isinstance(v, str) and v.strip()]
                else:
                    raise ValueError("parsed queries not list/dict")
            except Exception as e:
                logger.warning(f"é—®é¢˜åˆ†è§£å¤±è´¥: {e}, ä½¿ç”¨å…³é”®è¯ä½œä¸ºé—®é¢˜")
                queries = keywords
        logger.info(f"ç ”ç©¶é—®é¢˜: {queries}")

        # ä¸²è¡Œå¤„ç†æ¯ä¸ªé—®é¢˜ï¼Œé¿å…å¹¶å‘æœç´¢
        summaries = []
        for i, q in enumerate(queries):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(2)  # æ¯ä¸ªé—®é¢˜å¤„ç†é—´éš”2ç§’ï¼Œå¢åŠ å»¶è¿Ÿ
            summary = await self._search_and_summarize_query(topic, q, url_per_query)
            summaries.append(summary)

        return "\n\n".join(summaries)

    async def _search_and_summarize_query(self, topic: str, query: str, url_per_query: int) -> str:
        """æœç´¢ã€æ’åºå¹¶æ€»ç»“å•ä¸ªé—®é¢˜çš„URL"""
        logger.info(f"å¤„ç†é—®é¢˜: {query}")
        urls = await self._search_and_rank_urls(topic, query, url_per_query)
        
        if not urls:
            return f"### é—®é¢˜: {query}\n\næœªèƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚\n"

        # ä¸²è¡Œæµè§ˆå’Œåˆ†æURLï¼Œé¿å…å¹¶å‘è¯·æ±‚
        contents = []
        for i, url in enumerate(urls):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(1)  # æ¯ä¸ªURLå¤„ç†é—´éš”1ç§’
            content = await self._web_browse_and_summarize(url, query)
            contents.append(content)

        # è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å†…å®¹
        relevant_contents = [c for c in contents if "ä¸ç›¸å…³" not in c]
        
        # summary = f"### é—®é¢˜: {query}\n\n" + "\n\n".join(relevant_contents)
        return relevant_contents

    async def _search_and_rank_urls(self, topic: str, query: str, num_results: int) -> List[str]:
        """æœç´¢å¹¶æ’åºURL"""
        max_results = max(num_results * 2, 6)
        try:
            results = await self.search_engine.run(query, max_results=max_results, as_string=False)
            if not results:
                logger.error(f"âŒ æœç´¢å¼•æ“æœªè¿”å›ä»»ä½•ç»“æœ: {query}")
                raise ValueError(f"æœç´¢å¼•æ“å¯¹æŸ¥è¯¢'{query}'æœªè¿”å›ä»»ä½•ç»“æœï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé…ç½®é”™è¯¯")
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥ {query}: {e}")
            raise e  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸éšè—
    
        # ä»¥æœºå™¨å¯è¯»JSONå½¢å¼æä¾›å€™é€‰ï¼Œä¾¿äºLLMè¿›è¡ŒåŸŸåç™½åå•ç­›é€‰
        candidates = []
        for i, res in enumerate(results):
            link = res.get("link", "")
            parsed = urlparse(link) if link else None
            candidates.append({
                "index": i,
                "link": link,
                "domain": (parsed.netloc if parsed else ""),
                "title": res.get("title", ""),
            })
        _results_str = json.dumps(candidates, ensure_ascii=False)
        time_stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        prompt = RANK_URLS_PROMPT.format(topic=topic, query=query, results=_results_str, time_stamp=time_stamp)
        
        logger.debug(f"URLæ’åºæç¤ºè¯: {prompt}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        # æ³¨å…¥é¡¹ç›®é…ç½®ä¿¡æ¯ä½œä¸ºç³»ç»Ÿçº§æç¤º
        project_info_text = get_project_info_text()
        indices_str = await self._aask(prompt[:MAX_INPUT_TOKENS], [project_info_text])
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(0.5)
        
        logger.debug(f"LLMè¿”å›çš„æ’åºç»“æœ: {indices_str}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        try:
            indices = OutputParser.extract_struct(indices_str, list)
        except Exception:
            indices = []

        ranked_results = []
        if indices:
            ranked_results = [results[i] for i in indices if isinstance(i, int) and i < len(results)]
        
        # å½“LLMå› ç™½åå•è¿‡ä¸¥æˆ–è§£æå¤±è´¥è¿”å›ç©ºç´¢å¼•æ—¶ï¼Œé‡‡ç”¨ä»£ç çº§ç™½åå•å›é€€ï¼Œé¿å…æµç¨‹ä¸­æ–­
        if not ranked_results:
            logger.warning("âš ï¸ LLM URLæ’åºè¿”å›ç©ºï¼Œå¯ç”¨ç¨‹åºåŒ–ç™½åå•å›é€€")
            def is_whitelisted(url: str) -> bool:
                try:
                    host = urlparse(url).netloc.lower()
                    if not host:
                        return False
                    from backend.config.global_prompts import URL_WHITELIST
                    suffixes = URL_WHITELIST.get("suffix", []) or []
                    hosts = URL_WHITELIST.get("hosts", []) or []
                    for suf in suffixes:
                        if host.endswith(suf.lower()):
                            return True
                    if host in {h.lower() for h in hosts}:
                        return True
                    return False
                except Exception:
                    return False

            filtered = [res for res in results if is_whitelisted(res.get("link", ""))]
            ranked_results = filtered[:num_results] if filtered else results[:num_results]
    
        final_urls = [res['link'] for res in ranked_results[:num_results]]
        logger.info(f"æœ€ç»ˆè·å¾— {len(final_urls)} ä¸ªURLç”¨äºæŸ¥è¯¢: {query}")
        
        return final_urls

    async def _web_browse_and_summarize(self, url: str, query: str) -> str:
        """æµè§ˆç½‘é¡µå¹¶æ€»ç»“å†…å®¹"""
        try:
            content = await WebBrowserEngine().run(url)
            prompt = WEB_CONTENT_ANALYSIS_PROMPT.format(content=content, query=query)
            # æ³¨å…¥é¡¹ç›®é…ç½®ä¿¡æ¯ä½œä¸ºç³»ç»Ÿçº§æç¤º
            project_info_text = get_project_info_text()
            summary = await self._aask(prompt[:MAX_INPUT_TOKENS], [project_info_text])
            
            # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
            await asyncio.sleep(1)
            return f"## å‚è€ƒæ¥æº: {url}\n{summary}"
        except Exception as e:
            logger.error(f"æµè§ˆURLå¤±è´¥ {url}: {e}")
            return f"## å‚è€ƒæ¥æº: {url}\n\næ— æ³•è®¿é—®æˆ–å¤„ç†æ­¤é¡µé¢ã€‚"