#!/usr/bin/env python
"""
æŒ‡æ ‡è¯„åˆ†Action - ä»writer_actionä¸­å½»åº•ç‹¬ç«‹
"""
from metagpt.actions import Action
from metagpt.logs import logger
from backend.tools.json_utils import extract_json_from_llm_response

from backend.config.performance_constants import (
    ENV_WRITER_BASE_SYSTEM,
    ENV_EVALUATION_TYPES,
    ENV_WRITER_EVALUATION_PROMPT_TEMPLATE,
    ENV_METRIC_PROMPT_SPEC,
)
from pathlib import Path
import re


class EvaluateMetrics(Action):
    """
    æŒ‡æ ‡è¯„åˆ†Action - æŒ‰ç…§æ ‡å‡†åŒ–è¯„ä»·ç±»å‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†
    ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆè¯„ä»·æ„è§å’Œå…·ä½“å¾—åˆ†
    """

    async def run(self, metric_table_json: str, vector_store_path: str, metric_table_md_path: str | None = None) -> dict:
        """
        å¯¹æ‰€æœ‰æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼Œè¿”å›è¯„åˆ†ç»“æœ

        Returns:
            dict: {
                "metrics_scores": [{"metric": {}, "score": 85.5, "opinion": "è¯„ä»·æ„è§"}],
                "level1_summary": {"å†³ç­–": 12.5, "è¿‡ç¨‹": 22.3, "äº§å‡º": 30.1, "æ•ˆç›Š": 20.6},
                "total_score": 85.5,
                "grade": "è‰¯å¥½"
            }
        """
        logger.info("ğŸ“Š å¼€å§‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†...")

        try:
            # è§£ææŒ‡æ ‡æ•°æ®ï¼ˆä½¿ç”¨é€šç”¨æå–å·¥å…·ï¼Œå…¼å®¹ä»£ç å—/æ¾æ•£JSONï¼‰
            metrics_data = extract_json_from_llm_response(metric_table_json)

            if isinstance(metrics_data, dict) and "error" in metrics_data:
                return {"error": "æŒ‡æ ‡ä½“ç³»æ„å»ºå¤±è´¥", "details": metrics_data}

            # extract_json_from_llm_response å·²åšå½’ä¸€åŒ–ï¼›æ­¤å¤„åªåšæœ€ç»ˆå…œåº•
            if isinstance(metrics_data, dict):
                metrics_data = [metrics_data]
            elif not isinstance(metrics_data, list):
                metrics_data = []

            # ä¸ºæ¯ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„åˆ†
            metrics_scores = []
            level1_scores = {"å†³ç­–": 0, "è¿‡ç¨‹": 0, "äº§å‡º": 0, "æ•ˆç›Š": 0}

            for metric in metrics_data:
                try:
                    # æ‰§è¡Œå•ä¸ªæŒ‡æ ‡è¯„åˆ†
                    score, opinion = await self._evaluate_single_metric(metric, vector_store_path)

                    metrics_scores.append({
                        "metric": metric,
                        "score": score,
                        "opinion": opinion,
                        "weight_score": score * metric.get("åˆ†å€¼", 0) / 100
                    })

                    # ç´¯è®¡ä¸€çº§æŒ‡æ ‡å¾—åˆ†
                    level1 = metric.get("ä¸€çº§æŒ‡æ ‡", "")
                    if level1 in level1_scores:
                        level1_scores[level1] += score * metric.get("åˆ†å€¼", 0) / 100

                    logger.info(f"âœ… å®ŒæˆæŒ‡æ ‡è¯„åˆ†: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {score}åˆ†")

                except Exception as e:
                    logger.error(f"æŒ‡æ ‡è¯„åˆ†å¤±è´¥: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {e}")
                    # ç»™é»˜è®¤åˆ†æ•°
                    metrics_scores.append({
                        "metric": metric,
                        "score": 0,
                        "opinion": f"è¯„åˆ†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}",
                        "weight_score": 0
                    })

            # å°†æ„è§ä¸å¾—åˆ†å›å†™è‡³æŒ‡æ ‡è¡¨mdï¼ˆè‹¥æä¾›è·¯å¾„ï¼‰
            if metric_table_md_path:
                try:
                    self._update_metric_table_md(metric_table_md_path, metrics_scores)
                    logger.info(f"ğŸ“ å·²å›å†™è¯„åˆ†ä¸æ„è§è‡³: {metric_table_md_path}")
                except Exception as e:
                    logger.error(f"å›å†™metric_analysis_table.mdå¤±è´¥: {e}")

            total_score = round(sum(level1_scores.values()), 2)
            result = {
                "metrics_scores": metrics_scores,
                "level1_summary": level1_scores,
                "total_score": total_score,
            }

            logger.info(f"ğŸ“Š æŒ‡æ ‡è¯„åˆ†å®Œæˆï¼Œæ€»åˆ†: {total_score:.2f}åˆ†")
            return result

        except Exception as e:
            logger.error(f"æŒ‡æ ‡è¯„åˆ†è¿‡ç¨‹å¤±è´¥: {e}")
            return {"error": "æŒ‡æ ‡è¯„åˆ†å¤±è´¥", "details": str(e)}

    def _update_metric_table_md(self, md_path: str, metrics_scores: list[dict]) -> None:
        """åœ¨ metric_analysis_table.md çš„ JSON é‡Œä¸ºåŒ¹é…çš„æŒ‡æ ‡è¡¥å…… opinion ä¸ scoreã€‚"""
        path = Path(md_path)
        if not path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶: {md_path}")

        text = path.read_text(encoding="utf-8")

        # æå– ```json ... ``` ä»£ç å—
        code_block_match = re.search(r"```json\s*(.*?)\s*```", text, flags=re.DOTALL)
        if not code_block_match:
            raise ValueError("metric_analysis_table.md å†…æœªæ‰¾åˆ°JSONä»£ç å—")

        json_str = code_block_match.group(1)
        data = extract_json_from_llm_response(json_str)

        # å½’ä¸€ä¸ºåˆ—è¡¨
        if isinstance(data, dict):
            metrics_list = [data]
        elif isinstance(data, list):
            metrics_list = data
        else:
            raise ValueError("metric_analysis_table.md ä¸­JSONä¸æ˜¯å¯¹è±¡æˆ–æ•°ç»„")

        # å»ºç«‹è¯„åˆ†æ˜ å°„ï¼ˆä¼˜å…ˆä½¿ç”¨ metric_idï¼Œå›é€€ nameï¼‰
        score_map: dict[str, dict] = {}
        for item in metrics_scores:
            metric = item.get("metric", {})
            key = metric.get("metric_id") or metric.get("name")
            if key:
                score_map[key] = {"score": item.get("score", 0), "opinion": item.get("opinion", "")}

        # å›å†™è‡³åŸåˆ—è¡¨ï¼ˆä¼˜å…ˆ metric_idï¼Œå…¶æ¬¡ nameï¼‰
        matched_keys = set()
        for m in metrics_list:
            if not isinstance(m, dict):
                continue
            key = m.get("metric_id") or m.get("name")
            if key and key in score_map:
                m["score"] = score_map[key]["score"]
                m["opinion"] = score_map[key]["opinion"]
                matched_keys.add(key)

        # è‹¥ä»æœ‰æœªåŒ¹é…çš„è¯„åˆ†é¡¹ï¼Œåˆ™è¿½åŠ ä¸ºæ–°è®°å½•ï¼Œç¡®ä¿ä¸ä¸¢åˆ†
        for key, so in score_map.items():
            if key in matched_keys:
                continue
            # æŸ¥æ‰¾åŸå§‹æŒ‡æ ‡ä»¥é™„å¸¦åŸºæœ¬å­—æ®µ
            origin = None
            for item in metrics_scores:
                metric = item.get("metric", {})
                if (metric.get("metric_id") or metric.get("name")) == key:
                    origin = metric
                    break
            base = {"metric_id": origin.get("metric_id") if origin else key,
                    "name": origin.get("name") if origin else key,
                    "category": origin.get("category") if origin else ""}
            base.update({"score": so.get("score", 0), "opinion": so.get("opinion", "")})
            metrics_list.append(base)

        # å†™å› mdï¼ˆä¿æŒ```json å—ï¼‰
        from json import dumps
        new_json = dumps(metrics_list, ensure_ascii=False, indent=2)
        new_text = text[:code_block_match.start(1)] + new_json + text[code_block_match.end(1):]
        path.write_text(new_text, encoding="utf-8")

    async def _evaluate_single_metric(self, metric: dict, vector_store_path: str) -> tuple:
        """ç»Ÿä¸€çš„é…ç½®é©±åŠ¨è¯„ä»·ï¼šæ ¹æ®è¯„ä»·ç±»å‹ä»é…ç½®è¯»å–æ¨¡æ¿å¹¶æ‰§è¡Œ"""
        metric_name = metric.get("name", "æœªçŸ¥æŒ‡æ ‡")
        evaluation_type = metric.get("evaluation_type", "")
        evaluation_points = metric.get("evaluation_points", [])
        scoring_method = metric.get("scoring_method", "")

        # RAGæ£€ç´¢äº‹å®
        facts = await self._retrieve_metric_facts(metric_name, vector_store_path)

        # ä»é…ç½®è·å–è¯¥è¯„ä»·ç±»å‹çš„è¯¦ç»†è¯´æ˜
        eval_cfg = ENV_EVALUATION_TYPES.get(evaluation_type, {})
        type_description = eval_cfg.get("description", "")
        scoring_guidance = eval_cfg.get("scoring_guidance", "")
        opinion_requirements = eval_cfg.get("opinion_requirements", "")

        # æŒ‡æ ‡çº§æç¤ºè¯ç»„åˆè§„èŒƒ
        spec_default = ENV_METRIC_PROMPT_SPEC.get('default', {})
        spec_type = ENV_METRIC_PROMPT_SPEC.get('by_evaluation_type', {}).get(evaluation_type, {})
        points_intro = spec_type.get('points_intro', spec_default.get('points_intro', 'è¯„ä»·è¦ç‚¹ï¼š'))
        point_bullet = spec_type.get('point_bullet', spec_default.get('point_bullet', '- '))
        scoring_method_intro = spec_type.get('scoring_method_intro', spec_default.get('scoring_method_intro', 'è¯„åˆ†æ–¹æ³•ï¼š'))
        max_points = int(spec_default.get('max_points', 10))

        # è§„èŒƒåŒ–è¯„ä»·è¦ç‚¹
        if isinstance(evaluation_points, list):
            evaluation_points = evaluation_points[:max_points]
            points_str = points_intro + "\n" + "\n".join([f"{point_bullet}{pt}" for pt in evaluation_points]) if evaluation_points else points_intro + " æ— "
        else:
            points_str = points_intro + "\n" + str(evaluation_points)

        # è§„èŒƒåŒ–è¯„åˆ†æ–¹æ³•
        scoring_method = f"{scoring_method_intro}{scoring_method}" if scoring_method else f"{scoring_method_intro}æ— "

        if ENV_WRITER_EVALUATION_PROMPT_TEMPLATE:
            prompt = ENV_WRITER_EVALUATION_PROMPT_TEMPLATE.format(
                evaluation_type=evaluation_type or "æ ‡å‡†è¯„ä»·",
                evaluation_points=points_str,
                facts=facts,
                scoring_method=scoring_method,
                type_description=type_description,
                scoring_guidance=scoring_guidance,
                opinion_requirements=opinion_requirements,
            )
        else:
            # å…œåº•æ¨¡æ¿
            prompt = f"è¯·åŸºäºä»¥ä¸‹äº‹å®ï¼ŒæŒ‰{evaluation_type or 'æ ‡å‡†è¯„ä»·'}è¿›è¡Œè¯„åˆ†ã€‚\n\nè¦ç‚¹:\n{points_str}\n\näº‹å®:\n{facts}\n\nè¯„åˆ†æ–¹æ³•:{scoring_method}"

        try:
            result = await self._aask(prompt, [ENV_WRITER_BASE_SYSTEM])
            parsed = extract_json_from_llm_response(result)
            return parsed.get("score", 0), parsed.get("opinion", "è¯„ä»·æ„è§ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            logger.error(f"ç»Ÿä¸€è¯„ä»·å¤±è´¥: {e}")
            return 0, f"ç»Ÿä¸€è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"

    async def _retrieve_metric_facts(self, metric_name: str, vector_store_path: str) -> str:
        """ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢æœåŠ¡ä¸ºæŒ‡æ ‡æ£€ç´¢ç›¸å…³äº‹å®ä¾æ®"""
        try:
            from backend.services.intelligent_search import intelligent_search

            primary_query = f"{metric_name} çš„å…·ä½“æ•°æ®ã€å®Œæˆæƒ…å†µå’Œå®æ–½æ•ˆæœ"

            search_result = await intelligent_search.intelligent_search(
                query=primary_query,
                project_vector_storage_path=vector_store_path,
                mode="hybrid",
                enable_global=True,
                max_results=3,
            )

            if search_result.get("results"):
                facts = "\n\n".join(search_result["results"])
                if search_result.get("insights"):
                    facts += "\n\nğŸ’¡ æ™ºèƒ½åˆ†æ:\n" + "\n".join(search_result["insights"])
                return facts
            return f"æœªèƒ½æ£€ç´¢åˆ°å…³äº'{metric_name}'çš„ç›¸å…³äº‹å®ä¾æ®ã€‚"
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ£€ç´¢æŒ‡æ ‡äº‹å®å¤±è´¥: {e}")
            return f"æ£€ç´¢å¤±è´¥ï¼Œæ— æ³•è·å–å…³äº'{metric_name}'çš„äº‹å®ä¾æ®ã€‚"




