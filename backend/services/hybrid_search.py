"""
æ··åˆæ£€ç´¢æœåŠ¡
åŒæ—¶æ£€ç´¢å…¨å±€çŸ¥è¯†åº“å’Œé¡¹ç›®çŸ¥è¯†åº“ï¼Œåˆå¹¶ç»“æœ
"""

import asyncio
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple
from metagpt.logs import logger
from metagpt.rag.engines.simple import SimpleEngine
from metagpt.rag.schema import FAISSRetrieverConfig
from metagpt.config2 import Config

from .global_knowledge import global_knowledge


class HybridSearchService:
    """æ··åˆæ£€ç´¢æœåŠ¡ï¼šå…¨å±€çŸ¥è¯†åº“ + é¡¹ç›®çŸ¥è¯†åº“"""
    
    def __init__(self):
        self._project_engines = {}  # ç¼“å­˜é¡¹ç›®å¼•æ“
        self._config = None
    
    def _get_config(self) -> Config:
        """è·å–é…ç½®"""
        if self._config is None:
            self._config = Config.from_yaml_file(Path('config/config2.yaml'))
        return self._config
    
    def _create_llm_and_embed_model(self):
        """åˆ›å»ºLLMå’ŒåµŒå…¥æ¨¡å‹"""
        config = self._get_config()
        llm_config = config.llm
        embed_config = config.embedding
        
        # ğŸ”§ æŒ‰ç…§é˜¿é‡Œäº‘å®˜æ–¹æ–‡æ¡£ä½¿ç”¨OpenAI-Likeæ–¹å¼ 
        from llama_index.embeddings.dashscope import DashScopeEmbedding
        from llama_index.llms.openai_like import OpenAILike
        
        # åˆ›å»ºLLM - ä½¿ç”¨å®˜æ–¹æ¨èçš„OpenAI-Likeæ–¹å¼
        llm = OpenAILike(
            model=llm_config.model,  # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼šqwen-max-latest
            api_base=llm_config.base_url,  # "https://dashscope.aliyuncs.com/compatible-mode/v1"
            api_key=llm_config.api_key,
            is_chat_model=True
        )
        
        # åˆ›å»ºEmbeddingæ¨¡å‹ - ä½¿ç”¨å®˜æ–¹DashScopeEmbedding
        embed_model = DashScopeEmbedding(
            model_name=embed_config.model,  # text-embedding-v3
            api_key=embed_config.api_key,
            dashscope_api_key=embed_config.api_key  # DashScopeä¸“ç”¨å‚æ•°
        )
        embed_model.embed_batch_size = 8
        
        return llm, embed_model
    
    def _get_project_vector_index_path(self, project_vector_storage_path: str) -> str:
        """è·å–é¡¹ç›®å‘é‡ç´¢å¼•è·¯å¾„"""
        project_dir = Path(project_vector_storage_path).parent
        return str(project_dir / "vector_index")
    
    def _is_project_index_exists(self, index_path: str) -> bool:
        """æ£€æŸ¥é¡¹ç›®ç´¢å¼•æ˜¯å¦å­˜åœ¨"""
        index_path = Path(index_path)
        # ä½¿ç”¨æ­£ç¡®çš„FAISSç´¢å¼•æ–‡ä»¶å
        index_files = ['default__vector_store.json', 'docstore.json', 'index_store.json']
        return all((index_path / f).exists() for f in index_files)
    
    async def _build_project_index(self, project_vector_storage_path: str) -> bool:
        """æ„å»ºé¡¹ç›®å‘é‡ç´¢å¼•"""
        try:
            # æ”¶é›†é¡¹ç›®æ–‡æ¡£
            project_files = []
            if os.path.isdir(project_vector_storage_path):
                project_files = [
                    os.path.join(project_vector_storage_path, f) 
                    for f in os.listdir(project_vector_storage_path) 
                    if f.endswith(('.md', '.txt'))
                ]
            
            if not project_files:
                logger.warning(f"âš ï¸ é¡¹ç›®çŸ¥è¯†åº“ä¸ºç©º: {project_vector_storage_path}")
                return False
            
            logger.info(f"ğŸ”§ æ„å»ºé¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•: {len(project_files)} ä¸ªæ–‡ä»¶")
            llm, embed_model = self._create_llm_and_embed_model()
            
            # ğŸš€ ä½¿ç”¨FAISS Retrieveræ”¯æŒæŒä¹…åŒ–
            from metagpt.rag.schema import FAISSRetrieverConfig
            
            # æ„å»ºå¼•æ“æ—¶å°±æŒ‡å®šæ”¯æŒæŒä¹…åŒ–çš„é…ç½®
            engine = SimpleEngine.from_docs(
                input_files=project_files,
                llm=llm,
                embed_model=embed_model,
                retriever_configs=[FAISSRetrieverConfig(dimensions=1024)]  # ä½¿ç”¨FAISSæ”¯æŒæŒä¹…åŒ–
            )
            
            index_path = self._get_project_vector_index_path(project_vector_storage_path)
            Path(index_path).mkdir(parents=True, exist_ok=True)
            
            # æŒä¹…åŒ–ç´¢å¼•
            engine.persist(index_path)
            
            logger.info(f"âœ… é¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºé¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    async def _get_project_engine(self, project_vector_storage_path: str) -> SimpleEngine:
        """è·å–æˆ–åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“å¼•æ“"""
        cache_key = project_vector_storage_path
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._project_engines:
            return self._project_engines[cache_key]
        
        try:
            index_path = self._get_project_vector_index_path(project_vector_storage_path)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æ„å»º
            if not self._is_project_index_exists(index_path):
                logger.info("ğŸ“ é¡¹ç›®ç´¢å¼•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ„å»º...")
                if not await self._build_project_index(project_vector_storage_path):
                    raise Exception("æ„å»ºé¡¹ç›®ç´¢å¼•å¤±è´¥")
            
            # ä»ç´¢å¼•åŠ è½½å¼•æ“
            logger.info(f"ğŸ“– åŠ è½½é¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•: {index_path}")
            llm, embed_model = self._create_llm_and_embed_model()
            
            from metagpt.rag.schema import FAISSIndexConfig
            config = FAISSIndexConfig(
                persist_path=index_path,
                embed_model=embed_model
            )
            engine = SimpleEngine.from_index(
                index_config=config,
                embed_model=embed_model,
                llm=llm,
                retriever_configs=[FAISSRetrieverConfig(dimensions=1024)]
            )
            
            # ç¼“å­˜å¼•æ“
            self._project_engines[cache_key] = engine
            logger.info("âœ… é¡¹ç›®çŸ¥è¯†åº“å¼•æ“åŠ è½½æˆåŠŸ")
            return engine
            
        except Exception as e:
            logger.error(f"âŒ è·å–é¡¹ç›®çŸ¥è¯†åº“å¼•æ“å¤±è´¥: {e}")
            raise
    
    async def _search_project_knowledge(self, query: str, project_vector_storage_path: str, top_k: int = 3) -> List[str]:
        """æœç´¢é¡¹ç›®çŸ¥è¯†åº“"""
        try:
            engine = await self._get_project_engine(project_vector_storage_path)
            results = await engine.aretrieve(query)
            return [result.text.strip() for result in results[:top_k]]
        except Exception as e:
            logger.error(f"âŒ é¡¹ç›®çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}")
            return []
    
    def _merge_search_results(
        self, 
        global_results: List[str], 
        project_results: List[str],
        global_weight: float = 0.3,
        project_weight: float = 0.7,
        limit: int = 6,
    ) -> List[str]:
        """åˆå¹¶æ£€ç´¢ç»“æœï¼Œé¡¹ç›®çŸ¥è¯†åº“æƒé‡æ›´é«˜"""
        
        merged_results = []
        
        # å…ˆæ·»åŠ é¡¹ç›®ç»“æœï¼ˆæƒé‡æ›´é«˜ï¼‰
        for result in project_results:
            if result and result.strip():
                merged_results.append(f"ğŸ“ [é¡¹ç›®çŸ¥è¯†] {result.strip()}")
        
        # å†æ·»åŠ å…¨å±€ç»“æœ
        for result in global_results:
            if result and result.strip():
                merged_results.append(f"ğŸŒ [å…¨å±€çŸ¥è¯†] {result.strip()}")
        
        # å»é‡å¹¶é™åˆ¶æ€»æ•°
        seen = set()
        unique_results = []
        for result in merged_results:
            if result not in seen:
                seen.add(result)
                unique_results.append(result)
        
        return unique_results[:limit]  # é™åˆ¶æ€»ç»“æœæ•°ï¼ˆå¯é…ç½®ï¼‰
    
    async def hybrid_search(
        self, 
        query: str, 
        project_vector_storage_path: str,
        enable_global: bool = True,
        global_top_k: int = 2,
        project_top_k: int = 4
    ) -> List[str]:
        """
        æ··åˆæ£€ç´¢ï¼šåŒæ—¶æœç´¢å…¨å±€çŸ¥è¯†åº“å’Œé¡¹ç›®çŸ¥è¯†åº“
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            project_vector_storage_path: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
            enable_global: æ˜¯å¦å¯ç”¨å…¨å±€çŸ¥è¯†åº“æœç´¢
            global_top_k: å…¨å±€çŸ¥è¯†åº“è¿”å›ç»“æœæ•°
            project_top_k: é¡¹ç›®çŸ¥è¯†åº“è¿”å›ç»“æœæ•°
        """
        try:
            tasks = []
            
            # é¡¹ç›®çŸ¥è¯†åº“æœç´¢ï¼ˆæ€»æ˜¯æ‰§è¡Œï¼‰
            project_task = self._search_project_knowledge(
                query, project_vector_storage_path, project_top_k
            )
            tasks.append(project_task)
            
            # å…¨å±€çŸ¥è¯†åº“æœç´¢ï¼ˆå¯é€‰ï¼‰
            if enable_global:
                global_task = global_knowledge.search_global(query, global_top_k)
                tasks.append(global_task)
            else:
                tasks.append(asyncio.coroutine(lambda: [])())
            
            # å¹¶è¡Œæ‰§è¡Œ
            project_results, global_results = await asyncio.gather(*tasks)
            
            logger.info(f"ğŸ” æ··åˆæ£€ç´¢å®Œæˆ - é¡¹ç›®ç»“æœ: {len(project_results)}, å…¨å±€ç»“æœ: {len(global_results)}")
            
            # åˆå¹¶ç»“æœï¼ˆæŒ‰ top_k ä¹‹å’Œé™åˆ¶æ€»ç»“æœæ•°ï¼‰
            return self._merge_search_results(global_results, project_results, limit=max(1, int(global_top_k) + int(project_top_k)))
            
        except Exception as e:
            logger.error(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def invalidate_project_cache(self, project_vector_storage_path: str):
        """æ¸…é™¤é¡¹ç›®å¼•æ“ç¼“å­˜ï¼ˆå½“é¡¹ç›®æ–‡æ¡£æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        cache_key = project_vector_storage_path
        if cache_key in self._project_engines:
            del self._project_engines[cache_key]
            logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤é¡¹ç›®å¼•æ“ç¼“å­˜: {cache_key}")
    
    # ========== ğŸ“ é¡¹ç›®çŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½ ==========
    
    def create_project_knowledge_base(self, project_id: str, workspace_root: str = "workspace") -> str:
        """
        ä¸ºé¡¹ç›®åˆ›å»ºä¸“ç”¨çŸ¥è¯†åº“ç›®å½•
        
        Returns:
            str: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
        """
        project_vector_path = Path(workspace_root) / project_id / "vector_storage" / "project_docs"
        project_vector_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“ é¡¹ç›®çŸ¥è¯†åº“å·²åˆ›å»º: {project_vector_path}")
        return str(project_vector_path)
    
    def add_content_to_project(self, content: str, filename: str, project_vector_storage_path: str, invalidate_cache: bool = True) -> bool:
        """
        æ·»åŠ å†…å®¹åˆ°é¡¹ç›®çŸ¥è¯†åº“ - ğŸš€ ç»Ÿä¸€ä½¿ç”¨MetaGPTåŸç”Ÿåˆ†å—ç­–ç•¥
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            filename: æ–‡ä»¶å
            project_vector_storage_path: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(project_vector_storage_path).mkdir(parents=True, exist_ok=True)
            
            # ğŸš€ æ”¹ä¸ºä¿å­˜å®Œæ•´æ–‡ä»¶ï¼Œè®©MetaGPTå†…éƒ¨å¤„ç†åˆ†å—
            file_path = Path(project_vector_storage_path) / filename
            file_path.write_text(content, encoding='utf-8')
            
            # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆå¯é…ç½®ï¼‰
            if invalidate_cache:
                self.invalidate_project_cache(project_vector_storage_path)
            
            logger.info(f"ğŸ“„ å·²æ·»åŠ å®Œæ•´æ–‡æ¡£åˆ°é¡¹ç›®çŸ¥è¯†åº“: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å†…å®¹åˆ°é¡¹ç›®çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def add_multiple_contents_to_project(self, contents: List[dict], project_vector_storage_path: str) -> bool:
        """
        æ‰¹é‡æ·»åŠ å†…å®¹åˆ°é¡¹ç›®çŸ¥è¯†åº“
        
        Args:
            contents: å†…å®¹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"content": "å†…å®¹", "filename": "æ–‡ä»¶å"}, ...]
            project_vector_storage_path: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
        """
        try:
            # ä»…å¯¹â€œæ–°å¢æˆ–å†…å®¹å˜æ›´â€çš„æ–‡ä»¶è¿›è¡Œå†™å…¥ï¼Œå¹¶åœ¨æœ€åç»Ÿä¸€å¤±æ•ˆç¼“å­˜
            Path(project_vector_storage_path).mkdir(parents=True, exist_ok=True)

            added_or_updated = 0
            skipped_unchanged = 0

            for idx, item in enumerate(contents):
                new_content = item.get("content", "")
                filename = item.get("filename", f"content_{idx}.txt")
                file_path = Path(project_vector_storage_path) / filename

                is_unchanged = False
                if file_path.exists():
                    try:
                        old_content = file_path.read_text(encoding='utf-8')
                        is_unchanged = (old_content == new_content)
                    except Exception:
                        # è¯»å–å¤±è´¥åˆ™è§†ä¸ºéœ€è¦æ›´æ–°
                        is_unchanged = False

                if is_unchanged:
                    skipped_unchanged += 1
                    continue

                # æ–°å¢æˆ–å˜æ›´ï¼šå†™å…¥ä½†ä¸ç«‹å³å¤±æ•ˆç¼“å­˜
                file_path.write_text(new_content, encoding='utf-8')
                added_or_updated += 1

            # ä»…å½“æœ‰å˜æ›´æ—¶ï¼Œç»Ÿä¸€å¤±æ•ˆç¼“å­˜
            if added_or_updated > 0:
                self.invalidate_project_cache(project_vector_storage_path)

            logger.info(
                f"ğŸ“¦ æ‰¹é‡åŒæ­¥å®Œæˆ: æ–°å¢/æ›´æ–° {added_or_updated} ä¸ªï¼Œè·³è¿‡æœªå˜åŒ– {skipped_unchanged} ä¸ªï¼Œæ€»è®¡ {len(contents)} ä¸ª"
            )
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ·»åŠ å†…å®¹å¤±è´¥: {e}")
            return False
    
    # ğŸ—‘ï¸ ç§»é™¤å¤æ‚çš„æ‰‹åŠ¨åˆ†å—é€»è¾‘ï¼Œç»Ÿä¸€ä½¿ç”¨MetaGPTåŸç”ŸSentenceSplitter
    # è¿™æ ·ä¸å…¨å±€çŸ¥è¯†åº“ä¿æŒä¸€è‡´ï¼Œç®€åŒ–ç»´æŠ¤å¤æ‚åº¦
    
    def get_project_knowledge_stats(self, project_vector_storage_path: str) -> dict:
        """è·å–é¡¹ç›®çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not Path(project_vector_storage_path).exists():
                return {"exists": False, "file_count": 0, "index_exists": False}
            
            # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
            files = list(Path(project_vector_storage_path).glob("*.txt"))
            file_count = len(files)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            index_path = self._get_project_vector_index_path(project_vector_storage_path)
            index_exists = self._is_project_index_exists(index_path)
            
            return {
                "exists": True,
                "file_count": file_count,
                "index_exists": index_exists,
                "storage_path": project_vector_storage_path,
                "index_path": index_path
            }
        except Exception as e:
            logger.error(f"âŒ è·å–é¡¹ç›®çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥: {e}")
            return {"exists": False, "error": str(e)}


# å…¨å±€å•ä¾‹å®ä¾‹
hybrid_search = HybridSearchService()