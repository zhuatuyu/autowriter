#!/usr/bin/env python
"""
å†™ä½œä¸“å®¶Actioné›†åˆ - å†…å®¹ç”Ÿæˆå’Œæ•´åˆ
"""
import pandas as pd
from pathlib import Path
from metagpt.actions import Action
from metagpt.logs import logger
from metagpt.const import DEFAULT_WORKSPACE_ROOT

from .pm_action import Task


# --- å†™ä½œä¸“å®¶ä¸“ç”¨æç¤ºè¯æ¨¡æ¿ ---
WRITER_BASE_SYSTEM = """ä½ æ˜¯ç»©æ•ˆè¯„ä»·æŠ¥å‘Šçš„å†™ä½œä¸“å®¶ã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1. åŸºäºäº‹å®ä¾æ®å’Œæ•°æ®æ”¯æ’‘ï¼Œæ’°å†™é«˜è´¨é‡çš„æŠ¥å‘Šç« èŠ‚
2. ä¸¥æ ¼éµå¾ªæ¶æ„å¸ˆåˆ¶å®šçš„å†™ä½œæŒ‡å¯¼å’Œæ£€ç´¢è¦æ±‚
3. ç¡®ä¿å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€æ·±å…¥ï¼Œç¬¦åˆç»©æ•ˆè¯„ä»·æŠ¥å‘Šçš„æ ‡å‡†
"""

SECTION_WRITING_PROMPT = """è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ’°å†™æŠ¥å‘Šç« èŠ‚ï¼š

## ç« èŠ‚æ ‡é¢˜
{section_title}

## å†™ä½œè¦æ±‚ä¸æŒ‡å¯¼
{instruction}

## ç›¸å…³äº‹å®ä¾æ®ï¼ˆæ¥è‡ªå‘é‡æ£€ç´¢ï¼‰
{factual_basis}

## å…³è”çš„ç»©æ•ˆæŒ‡æ ‡
{metrics_text}

## ğŸ“‹ å†™ä½œæ ‡å‡†ä¸è´¨é‡è¦æ±‚

### ğŸ¯ å†…å®¹è¦æ±‚
1. **æ•°æ®é©±åŠ¨**: æ‰€æœ‰è®ºç‚¹å¿…é¡»åŸºäºä¸Šè¿°äº‹å®ä¾æ®ï¼Œé¿å…ç©ºæ³›è®ºè¿°
2. **ç»“æ„æ¸…æ™°**: æŒ‰ç…§å†™ä½œè¦æ±‚ä¸­çš„æŒ‡å¯¼é¡ºåºç»„ç»‡å†…å®¹
3. **æ·±åº¦åˆ†æ**: ä¸ä»…è¦åˆ—å‡ºäº‹å®ï¼Œè¿˜è¦åˆ†æåŸå› ã€å½±å“å’Œè¶‹åŠ¿
4. **ä¸“ä¸šè¡¨è¾¾**: ä½¿ç”¨ç»©æ•ˆè¯„ä»·ä¸“ä¸šæœ¯è¯­å’Œå•†ä¸šåˆ†æè¯­è¨€

### ğŸ“Š æ ¼å¼è¦æ±‚
1. **è¡¨æ ¼å±•ç¤º**: æ¶‰åŠæ•°æ®å¯¹æ¯”æ—¶ï¼ŒåŠ¡å¿…ä½¿ç”¨Markdownè¡¨æ ¼æ ¼å¼
2. **åˆ†å±‚è®ºè¿°**: ä½¿ç”¨é€‚å½“çš„æ ‡é¢˜å±‚çº§ç»„ç»‡å†…å®¹ç»“æ„
3. **æ•°æ®å¼•ç”¨**: æ˜ç¡®æ ‡æ³¨æ•°æ®æ¥æºå’Œå¼•ç”¨ä¾æ®
4. **å­—æ•°æ§åˆ¶**: ç« èŠ‚å†…å®¹æ§åˆ¶åœ¨{word_limit}å­—ä»¥å†…

### ğŸ” æ£€ç´¢éªŒè¯è¦æ±‚
- å¦‚æœäº‹å®ä¾æ®ä¸­ç¼ºå°‘æŸé¡¹å…³é”®ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æ ‡æ³¨"**ä¿¡æ¯å¾…è¡¥å……**"
- ç¡®ä¿æ¯ä¸ªå…³é”®è®ºç‚¹éƒ½æœ‰å…·ä½“çš„æ•°æ®æˆ–äº‹å®æ”¯æ’‘
- é¿å…ä½¿ç”¨æ¨¡ç³Šçš„è¡¨è¿°ï¼Œå¦‚"å¤§çº¦"ã€"å¯èƒ½"ç­‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚å¼€å§‹æ’°å†™ï¼š
"""


class EvaluateMetrics(Action):
    """
    æŒ‡æ ‡è¯„åˆ†Action - æŒ‰ç…§æ ‡å‡†åŒ–è¯„ä»·ç±»å‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†
    ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆè¯„ä»·æ„è§å’Œå…·ä½“å¾—åˆ†
    """
    
    async def run(self, metric_table_json: str, vector_store_path: str) -> dict:
        """
        å¯¹æ‰€æœ‰æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼Œè¿”å›è¯„åˆ†ç»“æœ
        
        Returns:
            dict: {
                "metrics_scores": [{"metric": {}, "score": 85.5, "opinion": "è¯„ä»·æ„è§"}],
                "level1_summary": {"å†³ç­–": 12.5, "è¿‡ç¨‹": 22.3, "äº§å‡º": 30.1, "æ•ˆç›Š": 20.6},
                "total_score": 85.5,
                "grade": "è‰¯å¥½"
            }
        """
        logger.info("ğŸ“Š å¼€å§‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†...")
        
        try:
            # è§£ææŒ‡æ ‡æ•°æ®
            import json
            metrics_data = json.loads(metric_table_json)
            
            if isinstance(metrics_data, dict) and "error" in metrics_data:
                return {"error": "æŒ‡æ ‡ä½“ç³»æ„å»ºå¤±è´¥", "details": metrics_data}
            
            # ä¸ºæ¯ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„åˆ†
            metrics_scores = []
            level1_scores = {"å†³ç­–": 0, "è¿‡ç¨‹": 0, "äº§å‡º": 0, "æ•ˆç›Š": 0}
            
            for metric in metrics_data:
                try:
                    # æ‰§è¡Œå•ä¸ªæŒ‡æ ‡è¯„åˆ†
                    score, opinion = await self._evaluate_single_metric(metric, vector_store_path)
                    
                    metrics_scores.append({
                        "metric": metric,
                        "score": score,
                        "opinion": opinion,
                        "weight_score": score * metric.get("åˆ†å€¼", 0) / 100
                    })
                    
                    # ç´¯è®¡ä¸€çº§æŒ‡æ ‡å¾—åˆ†
                    level1 = metric.get("ä¸€çº§æŒ‡æ ‡", "")
                    if level1 in level1_scores:
                        level1_scores[level1] += score * metric.get("åˆ†å€¼", 0) / 100
                    
                    logger.info(f"âœ… å®ŒæˆæŒ‡æ ‡è¯„åˆ†: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {score}åˆ†")
                    
                except Exception as e:
                    logger.error(f"æŒ‡æ ‡è¯„åˆ†å¤±è´¥: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {e}")
                    # ç»™é»˜è®¤åˆ†æ•°
                    metrics_scores.append({
                        "metric": metric,
                        "score": 0,
                        "opinion": f"è¯„åˆ†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}",
                        "weight_score": 0
                    })
            
            # è®¡ç®—æ€»åˆ†
            total_score = sum(level1_scores.values())
            
            # ç¡®å®šè¯„ä»·ç­‰çº§
            grade = self._determine_grade(total_score)
            
            result = {
                "metrics_scores": metrics_scores,
                "level1_summary": level1_scores,
                "total_score": round(total_score, 2),
                "grade": grade
            }
            
            logger.info(f"ğŸ“Š æŒ‡æ ‡è¯„åˆ†å®Œæˆï¼Œæ€»åˆ†: {total_score:.2f}åˆ†ï¼Œç­‰çº§: {grade}")
            return result
            
        except Exception as e:
            logger.error(f"æŒ‡æ ‡è¯„åˆ†è¿‡ç¨‹å¤±è´¥: {e}")
            return {"error": "æŒ‡æ ‡è¯„åˆ†å¤±è´¥", "details": str(e)}
    
    async def _evaluate_single_metric(self, metric: dict, vector_store_path: str) -> tuple:
        """
        è¯„ä»·å•ä¸ªæŒ‡æ ‡ï¼Œè¿”å›(å¾—åˆ†, è¯„ä»·æ„è§)
        """
        metric_name = metric.get("name", "æœªçŸ¥æŒ‡æ ‡")
        evaluation_type = metric.get("evaluation_type", "")
        evaluation_points = metric.get("evaluation_points", [])
        scoring_method = metric.get("scoring_method", "")
        
        # RAGæ£€ç´¢ç›¸å…³äº‹å®
        facts = await self._retrieve_metric_facts(metric_name, vector_store_path)
        
        # æ ¹æ®è¯„ä»·ç±»å‹æ‰§è¡Œç›¸åº”çš„è¯„åˆ†é€»è¾‘
        if evaluation_type == "è¦ç´ ç¬¦åˆåº¦è®¡åˆ†":
            return await self._evaluate_element_compliance(facts, evaluation_points, scoring_method)
        elif evaluation_type == "å…¬å¼è®¡ç®—å¾—åˆ†":
            return await self._evaluate_formula_calculation(facts, evaluation_points, scoring_method)
        elif evaluation_type == "æ¡ä»¶åˆ¤æ–­å¾—åˆ†":
            return await self._evaluate_condition_judgment(facts, evaluation_points, scoring_method)
        elif evaluation_type == "å®šæ€§ä¸å®šé‡ç»“åˆ":
            return await self._evaluate_qualitative_quantitative(facts, evaluation_points, scoring_method)
        elif evaluation_type == "é€’å‡æ‰£åˆ†æœºåˆ¶":
            return await self._evaluate_deduction_scoring(facts, evaluation_points, scoring_method)
        elif evaluation_type == "æå…‹ç‰¹é‡è¡¨æ³•":
            return await self._evaluate_likert_scale(facts, evaluation_points, scoring_method)
        else:
            # é»˜è®¤è¯„åˆ†æ–¹å¼
            return await self._evaluate_default(facts, metric_name)
    
    async def _retrieve_metric_facts(self, metric_name: str, vector_store_path: str) -> str:
        """
        ä¸ºæŒ‡æ ‡æ£€ç´¢ç›¸å…³äº‹å®ä¾æ®
        """
        try:
            from metagpt.rag.engines.simple import SimpleEngine
            from llama_index.llms.openai import OpenAI as LlamaOpenAI
            from pathlib import Path
            from metagpt.config2 import Config
            from metagpt.rag.factories.embedding import get_rag_embedding
            import os
            
            if not os.path.exists(vector_store_path):
                return f"å‘é‡åº“ä¸å¯ç”¨ï¼Œæ— æ³•æ£€ç´¢å…³äº'{metric_name}'çš„äº‹å®ä¾æ®ã€‚"
            
            vector_files = []
            if os.path.isdir(vector_store_path):
                vector_files = [os.path.join(vector_store_path, f) for f in os.listdir(vector_store_path) if f.endswith('.txt')]
            
            if not vector_files:
                return f"å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•æ£€ç´¢å…³äº'{metric_name}'çš„äº‹å®ä¾æ®ã€‚"
            
            # ä½¿ç”¨MetaGPTåŸç”Ÿçš„RAGå¼•æ“
            full_config = Config.from_yaml_file(Path('config/config2.yaml'))
            llm_config = full_config.llm
            llm = LlamaOpenAI(
                api_key=llm_config.api_key,
                base_url=llm_config.base_url,
                model="gpt-3.5-turbo"
            )
            
            embed_model = get_rag_embedding(config=full_config)
            embed_model.embed_batch_size = 10
            
            engine = SimpleEngine.from_docs(
                input_files=vector_files,
                llm=llm,
                embed_model=embed_model
            )
            
            # æ‰§è¡Œæ£€ç´¢
            results = await engine.aretrieve(metric_name)
            
            if results:
                facts = "\n\n".join([result.text.strip() for result in results[:3]])
                logger.debug(f"æˆåŠŸæ£€ç´¢åˆ°å…³äº'{metric_name}'çš„äº‹å®ä¾æ®")
                return facts
            else:
                return f"æœªèƒ½æ£€ç´¢åˆ°å…³äº'{metric_name}'çš„ç›¸å…³äº‹å®ä¾æ®ã€‚"
                
        except Exception as e:
            logger.error(f"æ£€ç´¢æŒ‡æ ‡äº‹å®å¤±è´¥: {e}")
            return f"æ£€ç´¢è¿‡ç¨‹å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    
    async def _evaluate_element_compliance(self, facts: str, evaluation_points: list, scoring_method: str) -> tuple:
        """
        è¦ç´ ç¬¦åˆåº¦è®¡åˆ†è¯„ä»·
        """
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹å®ä¾æ®ï¼Œå¯¹ç…§è¯„ä»·è¦ç‚¹è¿›è¡Œè¦ç´ ç¬¦åˆåº¦è¯„ä»·ï¼š

è¯„ä»·è¦ç‚¹ï¼š
{chr(10).join(evaluation_points)}

äº‹å®ä¾æ®ï¼š
{facts}

è¯„åˆ†æ–¹æ³•ï¼š{scoring_method}

è¯·æŒ‰ç…§è¦ç´ ç¬¦åˆåº¦è®¡åˆ†çš„æ ‡å‡†ï¼Œç”Ÿæˆè¯„ä»·æ„è§å¹¶è®¡ç®—å¾—åˆ†ï¼š
1. é€ä¸€å¯¹ç…§è¯„ä»·è¦ç‚¹ï¼Œåˆ¤æ–­æ¯ä¸ªè¦ç‚¹çš„ç¬¦åˆæƒ…å†µ
2. æ ¹æ®ç¬¦åˆçš„è¦ç‚¹æ•°é‡å’Œè¯„åˆ†æ–¹æ³•è®¡ç®—å¾—åˆ†
3. ç”Ÿæˆç¬¦åˆæ ¼å¼è¦æ±‚çš„è¯„ä»·æ„è§ï¼ˆä¸åŒ…å«æœ€ç»ˆå¾—åˆ†ï¼‰

è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 85.5,
    "opinion": "é¡¹ç›®ç«‹é¡¹ä¾æ®ç›¸å…³æ³•å¾‹æ³•è§„ï¼Œç¬¦åˆè¯„ä»·è¦ç‚¹â‘ ï¼›é¡¹ç›®å†…å®¹ç¬¦åˆè¡Œä¸šè§„åˆ’ï¼Œç¬¦åˆè¯„ä»·è¦ç‚¹â‘¡ï¼›..."
}}
"""
        
        try:
            result = await self._aask(evaluation_prompt, [WRITER_BASE_SYSTEM])
            # è§£æç»“æœ
            import json
            parsed = self._extract_json_from_evaluation_response(result)
            return parsed.get("score", 0), parsed.get("opinion", "è¯„ä»·æ„è§ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            logger.error(f"è¦ç´ ç¬¦åˆåº¦è¯„ä»·å¤±è´¥: {e}")
            return 0, f"è¦ç´ ç¬¦åˆåº¦è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    async def _evaluate_formula_calculation(self, facts: str, evaluation_points: list, scoring_method: str) -> tuple:
        """
        å…¬å¼è®¡ç®—å¾—åˆ†è¯„ä»·
        """
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹å®ä¾æ®ï¼Œè¿›è¡Œå…¬å¼è®¡ç®—å¾—åˆ†è¯„ä»·ï¼š

è®¡ç®—è¦æ±‚ï¼š
{chr(10).join(evaluation_points)}

äº‹å®ä¾æ®ï¼š
{facts}

è¯„åˆ†æ–¹æ³•ï¼š{scoring_method}

è¯·æŒ‰ç…§å…¬å¼è®¡ç®—å¾—åˆ†çš„æ ‡å‡†ï¼š
1. ä»äº‹å®ä¸­æå–å…·ä½“æ•°æ®
2. æŒ‰ç…§å…¬å¼è¿›è¡Œè®¡ç®—
3. å°†è®¡ç®—ç»“æœè½¬æ¢ä¸ºå¾—åˆ†
4. ç”ŸæˆåŒ…å«å®Œæ•´è®¡ç®—è¿‡ç¨‹çš„è¯„ä»·æ„è§

è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 87.93,
    "opinion": "ä¾æ®åŸºç¡€æ•°æ®è¡¨ï¼Œ2021-2024å¹´é¢„ç®—æ‰§è¡Œç‡åˆ†åˆ«ä¸º100.00%ã€100.00%ã€100.00%ã€68.91%ï¼Œå››å¹´åŠ æƒå¹³å‡å€¼ä¸º87.93%ï¼›..."
}}
"""
        
        try:
            result = await self._aask(evaluation_prompt, [WRITER_BASE_SYSTEM])
            parsed = self._extract_json_from_evaluation_response(result)
            return parsed.get("score", 0), parsed.get("opinion", "è¯„ä»·æ„è§ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            logger.error(f"å…¬å¼è®¡ç®—è¯„ä»·å¤±è´¥: {e}")
            return 0, f"å…¬å¼è®¡ç®—è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    async def _evaluate_condition_judgment(self, facts: str, evaluation_points: list, scoring_method: str) -> tuple:
        """
        æ¡ä»¶åˆ¤æ–­å¾—åˆ†è¯„ä»·
        """
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹å®ä¾æ®ï¼Œè¿›è¡Œæ¡ä»¶åˆ¤æ–­å¾—åˆ†è¯„ä»·ï¼š

åˆ¤æ–­æ¡ä»¶ï¼š
{chr(10).join(evaluation_points)}

äº‹å®ä¾æ®ï¼š
{facts}

è¯„åˆ†æ–¹æ³•ï¼š{scoring_method}

è¯·æŒ‰ç…§æ¡ä»¶åˆ¤æ–­å¾—åˆ†çš„æ ‡å‡†ï¼š
1. åˆ¤æ–­æ¯ä¸ªæ¡ä»¶çš„æ»¡è¶³æƒ…å†µ
2. æ ¹æ®æ¡ä»¶æ»¡è¶³ç¨‹åº¦ç»™å‡ºå¾—åˆ†
3. æä¾›å…·ä½“çš„è¯æ®æ”¯æ’‘

è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 75.0,
    "opinion": "é¡¹ç›®åˆ¶å®šäº†å®Œæ•´çš„ç®¡ç†åŠæ³•ï¼Œç¬¦åˆæ¡ä»¶â‘ ï¼›ç”³æŠ¥ææ–™å­˜åœ¨æ ¼å¼é—®é¢˜ï¼Œä¸ç¬¦åˆæ¡ä»¶â‘¡ï¼›..."
}}
"""
        
        try:
            result = await self._aask(evaluation_prompt, [WRITER_BASE_SYSTEM])
            parsed = self._extract_json_from_evaluation_response(result)
            return parsed.get("score", 0), parsed.get("opinion", "è¯„ä»·æ„è§ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            logger.error(f"æ¡ä»¶åˆ¤æ–­è¯„ä»·å¤±è´¥: {e}")
            return 0, f"æ¡ä»¶åˆ¤æ–­è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    async def _evaluate_qualitative_quantitative(self, facts: str, evaluation_points: list, scoring_method: str) -> tuple:
        """
        å®šæ€§ä¸å®šé‡ç»“åˆè¯„ä»·
        """
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹å®ä¾æ®ï¼Œè¿›è¡Œå®šæ€§ä¸å®šé‡ç»“åˆè¯„ä»·ï¼š

è¯„ä»·è¦æ±‚ï¼š
{chr(10).join(evaluation_points)}

äº‹å®ä¾æ®ï¼š
{facts}

è¯„åˆ†æ–¹æ³•ï¼š{scoring_method}

è¯·æŒ‰ç…§å®šæ€§ä¸å®šé‡ç»“åˆçš„æ ‡å‡†ï¼š
1. åˆ†åˆ«è¿›è¡Œå®šé‡å’Œå®šæ€§è¯„ä»·
2. æŒ‰æƒé‡ç»¼åˆè®¡ç®—å¾—åˆ†
3. è¯„ä»·æ„è§è¦ä½“ç°ä¸¤ä¸ªæ–¹é¢çš„ç»“åˆ

è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 92.0,
    "opinion": "ä¾æ®åŸºç¡€æ•°æ®è¡¨ï¼Œè®¾å¤‡åŒ¹é…åº¦è¾¾90.00%ï¼Œç¬¦åˆè¯„ä»·è¦ç‚¹â‘ ï¼›é€šè¿‡å®åœ°è°ƒç ”å‘ç°è®¾å¤‡åˆ©ç”¨å……åˆ†ï¼Œç¬¦åˆè¯„ä»·è¦ç‚¹â‘¡ï¼›..."
}}
"""
        
        try:
            result = await self._aask(evaluation_prompt, [WRITER_BASE_SYSTEM])
            parsed = self._extract_json_from_evaluation_response(result)
            return parsed.get("score", 0), parsed.get("opinion", "è¯„ä»·æ„è§ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            logger.error(f"å®šæ€§å®šé‡è¯„ä»·å¤±è´¥: {e}")
            return 0, f"å®šæ€§å®šé‡è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    async def _evaluate_deduction_scoring(self, facts: str, evaluation_points: list, scoring_method: str) -> tuple:
        """
        é€’å‡æ‰£åˆ†æœºåˆ¶è¯„ä»·
        """
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹å®ä¾æ®ï¼Œè¿›è¡Œé€’å‡æ‰£åˆ†æœºåˆ¶è¯„ä»·ï¼š

æ‰£åˆ†æ ‡å‡†ï¼š
{chr(10).join(evaluation_points)}

äº‹å®ä¾æ®ï¼š
{facts}

è¯„åˆ†æ–¹æ³•ï¼š{scoring_method}

è¯·æŒ‰ç…§é€’å‡æ‰£åˆ†æœºåˆ¶çš„æ ‡å‡†ï¼š
1. è¯†åˆ«å­˜åœ¨çš„é—®é¢˜
2. æŒ‰ç…§æ‰£åˆ†æ ‡å‡†è®¡ç®—æ‰£åˆ†
3. ä»æ»¡åˆ†ä¸­æ‰£é™¤ç›¸åº”åˆ†æ•°

è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 70.0,
    "opinion": "æŒ‰æ—¶æŠ¥é€å·¥ä½œæ€»ç»“ï¼Œç¬¦åˆè¦æ±‚ï¼›å‘ç°2ä¸ªèµ›äº‹æœªæŒ‰è§„å®šæ—¶é™æŠ¥é€æ€»ç»“ï¼Œæ‰£é™¤20åˆ†ï¼›æœªæä¾›ä¸“å®¶è®ºè¯èµ„æ–™ï¼Œæ‰£é™¤10åˆ†ï¼›..."
}}
"""
        
        try:
            result = await self._aask(evaluation_prompt, [WRITER_BASE_SYSTEM])
            parsed = self._extract_json_from_evaluation_response(result)
            return parsed.get("score", 0), parsed.get("opinion", "è¯„ä»·æ„è§ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            logger.error(f"é€’å‡æ‰£åˆ†è¯„ä»·å¤±è´¥: {e}")
            return 0, f"é€’å‡æ‰£åˆ†è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    async def _evaluate_likert_scale(self, facts: str, evaluation_points: list, scoring_method: str) -> tuple:
        """
        æå…‹ç‰¹é‡è¡¨æ³•è¯„ä»·
        """
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹å®ä¾æ®ï¼Œè¿›è¡Œæå…‹ç‰¹é‡è¡¨æ³•è¯„ä»·ï¼š

è°ƒæŸ¥è¦æ±‚ï¼š
{chr(10).join(evaluation_points)}

äº‹å®ä¾æ®ï¼š
{facts}

è¯„åˆ†æ–¹æ³•ï¼š{scoring_method}

è¯·æŒ‰ç…§æå…‹ç‰¹é‡è¡¨æ³•çš„æ ‡å‡†ï¼š
1. åˆ†æè°ƒæŸ¥æ•°æ®å’Œæ ·æœ¬é‡
2. è®¡ç®—æ»¡æ„åº¦ç™¾åˆ†æ¯”
3. æ ¹æ®æ»¡æ„åº¦ç¡®å®šå¾—åˆ†

è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 91.58,
    "opinion": "æ ¹æ®ç”µå­é—®å·è°ƒæŸ¥åŠç»“æœæ±‡æ€»ï¼Œå…±å›æ”¶æœ‰æ•ˆé—®å·1330ä»½ï¼Œæ»¡æ„åº¦é—®é¢˜å›ç­”æ€»æ•°ä¸º9310é¢˜ï¼Œå…¶ä¸­éå¸¸æ»¡æ„7055é¢˜ï¼Œæ¯”è¾ƒæ»¡æ„1589é¢˜ï¼Œæ— æ„Ÿ511é¢˜ï¼Œæ¯”è¾ƒä¸æ»¡æ„96é¢˜ï¼Œéå¸¸ä¸æ»¡æ„59é¢˜ã€‚ç»è®¡ç®—ï¼Œæ»¡æ„åº¦ä¸º91.58%ã€‚"
}}
"""
        
        try:
            result = await self._aask(evaluation_prompt, [WRITER_BASE_SYSTEM])
            parsed = self._extract_json_from_evaluation_response(result)
            return parsed.get("score", 0), parsed.get("opinion", "è¯„ä»·æ„è§ç”Ÿæˆå¤±è´¥")
        except Exception as e:
            logger.error(f"æå…‹ç‰¹é‡è¡¨è¯„ä»·å¤±è´¥: {e}")
            return 0, f"æå…‹ç‰¹é‡è¡¨è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    async def _evaluate_default(self, facts: str, metric_name: str) -> tuple:
        """
        é»˜è®¤è¯„ä»·æ–¹å¼ï¼ˆå½“è¯„ä»·ç±»å‹æœªè¯†åˆ«æ—¶ä½¿ç”¨ï¼‰
        """
        evaluation_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹å®ä¾æ®ï¼Œå¯¹æŒ‡æ ‡"{metric_name}"è¿›è¡Œè¯„ä»·ï¼š

äº‹å®ä¾æ®ï¼š
{facts}

è¯·ç”Ÿæˆè¯„ä»·æ„è§å¹¶ç»™å‡ºåˆç†çš„å¾—åˆ†ï¼ˆ0-100åˆ†ï¼‰ï¼š

è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 80.0,
    "opinion": "åŸºäºç°æœ‰äº‹å®ä¾æ®çš„ç»¼åˆè¯„ä»·æ„è§..."
}}
"""
        
        try:
            result = await self._aask(evaluation_prompt, [WRITER_BASE_SYSTEM])
            parsed = self._extract_json_from_evaluation_response(result)
            return parsed.get("score", 60), parsed.get("opinion", "é»˜è®¤è¯„ä»·æ„è§")
        except Exception as e:
            logger.error(f"é»˜è®¤è¯„ä»·å¤±è´¥: {e}")
            return 60, f"é»˜è®¤è¯„ä»·ï¼šåŸºäºæœ‰é™ä¿¡æ¯ç»™å‡ºä¸­ç­‰è¯„ä»·"
    
    def _extract_json_from_evaluation_response(self, response: str) -> dict:
        """
        ä»è¯„ä»·å“åº”ä¸­æå–JSONå†…å®¹
        """
        try:
            import json
            import re
            
            # å°è¯•å¤šç§JSONæå–æ–¹æ³•
            try:
                return json.loads(response)
            except:
                pass
            
            # æå–JSONä»£ç å—
            json_pattern = r'```json\s*(.*?)\s*```'
            match = re.search(json_pattern, response, re.DOTALL)
            if match:
                return json.loads(match.group(1).strip())
            
            # æŸ¥æ‰¾å¤§æ‹¬å·å†…å®¹
            start_idx = response.find('{')
            if start_idx != -1:
                brace_count = 0
                end_idx = start_idx
                for i, char in enumerate(response[start_idx:], start_idx):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end_idx = i
                            break
                
                if brace_count == 0:
                    json_str = response[start_idx:end_idx+1]
                    return json.loads(json_str)
            
            raise ValueError("æ— æ³•æå–JSONå†…å®¹")
            
        except Exception as e:
            logger.error(f"JSONæå–å¤±è´¥: {e}")
            return {"score": 0, "opinion": "è¯„ä»·ç»“æœè§£æå¤±è´¥"}
    
    def _determine_grade(self, total_score: float) -> str:
        """
        æ ¹æ®æ€»åˆ†ç¡®å®šè¯„ä»·ç­‰çº§
        """
        if total_score >= 90:
            return "ä¼˜ç§€"
        elif total_score >= 80:
            return "è‰¯å¥½"
        elif total_score >= 70:
            return "ä¸€èˆ¬"
        elif total_score >= 60:
            return "åŠæ ¼"
        else:
            return "è¾ƒå·®"


class WriteSection(Action):
    """
    å†™ä½œç« èŠ‚Action - WriterExpertçš„æ ¸å¿ƒèƒ½åŠ›
    é›†æˆRAGæ£€ç´¢ï¼Œç»“åˆäº‹å®ä¾æ®å’Œæ•°æ®ç”Ÿæˆç« èŠ‚å†…å®¹
    """
    
    async def run(
        self, 
        task: Task, 
        vector_store_path: str, 
        metric_table_json: str
    ) -> str:
        """
        åŸºäºä»»åŠ¡è¦æ±‚ã€å‘é‡ç´¢å¼•å’ŒæŒ‡æ ‡æ•°æ®ç”Ÿæˆç« èŠ‚å†…å®¹
        """
        logger.info(f"å¼€å§‹å†™ä½œç« èŠ‚: {task.section_title}")
        
        # 1. åŠ è½½æŒ‡æ ‡æ•°æ®
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            import json
            metric_data = json.loads(metric_table_json)
            
            # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆæ–°æ ¼å¼ï¼‰ï¼Œè½¬æ¢ä¸ºDataFrame
            if isinstance(metric_data, list):
                metric_df = pd.DataFrame(metric_data)
            else:
                # å¦‚æœæ˜¯DataFrameæ ¼å¼ï¼ˆæ—§æ ¼å¼ï¼‰ï¼Œç›´æ¥ç”¨pandasè¯»å–
                metric_df = pd.read_json(metric_table_json)
        except Exception as e:
            logger.error(f"è§£ææŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameä»¥é˜²æ­¢ç¨‹åºå´©æºƒ
            metric_df = pd.DataFrame()
        
        # 2. è·å–ç›¸å…³æŒ‡æ ‡æ•°æ®
        relevant_metrics = self._get_relevant_metrics(task, metric_df)
        
        # 3. RAGæ£€ç´¢äº‹å®ä¾æ® (ç®€åŒ–å®ç°)
        factual_basis = await self._retrieve_factual_basis(task, vector_store_path)
        
        # 4. æ„å»ºå†™ä½œprompt
        prompt = self._build_writing_prompt(task, factual_basis, relevant_metrics)
        
        # 5. ç”Ÿæˆç« èŠ‚å†…å®¹
        section_content = await self._generate_content(prompt)
        
        logger.info(f"ç« èŠ‚å†™ä½œå®Œæˆ: {task.section_title}")
        return section_content
    
    def _get_relevant_metrics(self, task: Task, metric_df: pd.DataFrame) -> pd.DataFrame:
        """è·å–ä¸ä»»åŠ¡ç›¸å…³çš„æŒ‡æ ‡æ•°æ®"""
        if not task.metric_ids or metric_df.empty:
            return pd.DataFrame()
        
        # æ£€æŸ¥DataFrameæ˜¯å¦æœ‰å¿…è¦çš„åˆ—
        if 'metric_id' not in metric_df.columns:
            logger.warning("æŒ‡æ ‡æ•°æ®ä¸­ç¼ºå°‘metric_idåˆ—ï¼Œè¿”å›ç©ºDataFrame")
            return pd.DataFrame()
        
        relevant_metrics = metric_df[metric_df['metric_id'].isin(task.metric_ids)]
        return relevant_metrics
    
    async def _retrieve_factual_basis(self, task: Task, vector_store_path: str) -> str:
        """ä»å‘é‡ç´¢å¼•ä¸­æ£€ç´¢ç›¸å…³çš„äº‹å®ä¾æ®"""
        try:
            from metagpt.rag.engines.simple import SimpleEngine
            from llama_index.llms.openai import OpenAI as LlamaOpenAI
            from pathlib import Path
            from metagpt.config2 import Config
            from metagpt.rag.factories.embedding import get_rag_embedding
            import os
            
            if not os.path.exists(vector_store_path):
                logger.warning(f"å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨: {vector_store_path}")
                return f"å‘é‡åº“ä¸å¯ç”¨ï¼Œæ— æ³•æ£€ç´¢å…³äº'{task.section_title}'çš„äº‹å®ä¾æ®ã€‚"
            
            # æ£€æŸ¥å‘é‡åº“æ–‡ä»¶
            vector_files = []
            if os.path.isdir(vector_store_path):
                vector_files = [os.path.join(vector_store_path, f) for f in os.listdir(vector_store_path) if f.endswith('.txt')]
            
            if not vector_files:
                logger.warning(f"å‘é‡åº“ç›®å½•ä¸ºç©º: {vector_store_path}")
                return f"å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•æ£€ç´¢å…³äº'{task.section_title}'çš„äº‹å®ä¾æ®ã€‚"
            
            # ä½¿ç”¨MetaGPTåŸç”Ÿçš„RAGå¼•æ“
            full_config = Config.from_yaml_file(Path('config/config2.yaml'))
            
            # è·å–LLMé…ç½®
            llm_config = full_config.llm
            llm = LlamaOpenAI(
                api_key=llm_config.api_key,
                base_url=llm_config.base_url,
                model="gpt-3.5-turbo"
            )
            
            # ä½¿ç”¨MetaGPTåŸç”Ÿembeddingå·¥å‚
            embed_model = get_rag_embedding(config=full_config)
            embed_model.embed_batch_size = 10
            
            engine = SimpleEngine.from_docs(
                input_files=vector_files,
                llm=llm,
                embed_model=embed_model
            )
            
            # æ„å»ºæ£€ç´¢æŸ¥è¯¢ - ç»“åˆç« èŠ‚æ ‡é¢˜å’Œå†™ä½œè¦æ±‚
            search_query = f"{task.section_title} {task.instruction[:200]}"
            
            # æ‰§è¡Œæ£€ç´¢
            results = await engine.aretrieve(search_query)
            
            # æå–æ£€ç´¢åˆ°çš„å†…å®¹
            factual_basis = ""
            if results:
                factual_basis = "\n\n".join([
                    f"**ç›¸å…³èµ„æ–™{i+1}**: {result.text.strip()}" 
                    for i, result in enumerate(results[:3])  # å–å‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                ])
                logger.info(f"æˆåŠŸä»å‘é‡åº“æ£€ç´¢åˆ° {len(results)} æ¡ç›¸å…³ä¿¡æ¯ç”¨äºç« èŠ‚: {task.section_title}")
            else:
                factual_basis = f"æœªèƒ½ä»å‘é‡åº“ä¸­æ£€ç´¢åˆ°å…³äº'{task.section_title}'çš„ç›¸å…³ä¿¡æ¯ã€‚"
                logger.warning(f"å‘é‡æ£€ç´¢æœªè¿”å›ç»“æœ: {task.section_title}")
            
            return factual_basis
            
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return f"å‘é‡æ£€ç´¢å‘ç”Ÿé”™è¯¯ï¼Œæ— æ³•è·å–å…³äº'{task.section_title}'çš„äº‹å®ä¾æ®ã€‚é”™è¯¯: {str(e)}"
    
    def _build_writing_prompt(self, task: Task, factual_basis: str, relevant_metrics: pd.DataFrame) -> str:
        """æ„å»ºå†™ä½œprompt - æ•´åˆArchitectçš„å†™ä½œæŒ‡å¯¼"""
        
        # æ ¼å¼åŒ–æŒ‡æ ‡æ•°æ®
        metrics_text = ""
        if not relevant_metrics.empty:
            # æ£€æŸ¥DataFrameçš„åˆ—ç»“æ„
            if 'name' in relevant_metrics.columns:
                # æ–°æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨æŒ‡æ ‡ä¿¡æ¯
                metrics_text = "\n".join([
                    f"- **{row['name']}** ({row.get('category', 'æœªåˆ†ç±»')}): {row.get('è¯„åˆ†è§„åˆ™', 'è¯„åˆ†è§„åˆ™å¾…è¡¥å……')}"
                    for _, row in relevant_metrics.iterrows()
                ])
            else:
                # æ—§æ ¼å¼å…¼å®¹
                metrics_text = "\n".join([
                    f"- {row.get('name', 'æœªçŸ¥æŒ‡æ ‡')}: {row.get('value', 'æ•°å€¼å¾…è¡¥å……')} ({row.get('analysis', 'åˆ†æå¾…è¡¥å……')})"
                    for _, row in relevant_metrics.iterrows()
                ])
        
        prompt = SECTION_WRITING_PROMPT.format(
            section_title=task.section_title,
            instruction=task.instruction,
            factual_basis=factual_basis,
            metrics_text=metrics_text,
            word_limit="4000"
        )
        return prompt
    
    async def _generate_content(self, prompt: str) -> str:
        """ç”Ÿæˆç« èŠ‚å†…å®¹"""
        # ä½¿ç”¨LLMç”Ÿæˆå†…å®¹
        section_content = await self._aask(prompt, [WRITER_BASE_SYSTEM])
        return section_content


class IntegrateReport(Action):
    """
    æ•´åˆæŠ¥å‘ŠAction - å°†æ‰€æœ‰ç« èŠ‚æ•´åˆä¸ºæœ€ç»ˆæŠ¥å‘Šï¼ŒåŒ…æ‹¬æŒ‡æ ‡è¯„åˆ†é™„ä»¶
    """
    
    async def run(self, sections: list, report_title: str, metrics_evaluation: dict = None) -> str:
        """
        æ•´åˆæ‰€æœ‰ç« èŠ‚ä¸ºæœ€ç»ˆæŠ¥å‘Šï¼ŒåŒ…æ‹¬æŒ‡æ ‡è¯„åˆ†é™„ä»¶å’Œç»¼åˆè¯„ä»·è¡¨
        """
        logger.info("å¼€å§‹æ•´åˆæœ€ç»ˆæŠ¥å‘Šï¼ˆåŒ…å«æŒ‡æ ‡è¯„åˆ†é™„ä»¶ï¼‰")
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report_parts = [
            f"# {report_title}",
            "",
            "## ğŸ“‹ ç›®å½•",
            ""
        ]
        
        # ç”Ÿæˆç›®å½•
        toc_items = []
        for i, section in enumerate(sections, 1):
            # ä»ç« èŠ‚å†…å®¹ä¸­æå–æ ‡é¢˜
            lines = section.split('\n')
            title = lines[0].replace('#', '').strip() if lines else f"ç« èŠ‚{i}"
            toc_items.append(f"{i}. {title}")
        
        # æ·»åŠ é™„ä»¶ç›®å½•
        if metrics_evaluation and metrics_evaluation.get('metrics_scores'):
            toc_items.extend([
                "",
                "**é™„ä»¶**",
                "A. æŒ‡æ ‡è¯„åˆ†è¯¦è¡¨",
                "B. ç»¼åˆè¯„ä»·æ±‡æ€»è¡¨"
            ])
        
        report_parts.extend(toc_items)
        report_parts.extend(["", "---", ""])
        
        # æ·»åŠ ç»¼åˆè¯„ä»·ç»“è®ºï¼ˆå¦‚æœæœ‰æŒ‡æ ‡è¯„åˆ†ï¼‰
        if metrics_evaluation and metrics_evaluation.get('total_score') > 0:
            total_score = metrics_evaluation.get('total_score', 0)
            grade = metrics_evaluation.get('grade', 'å¾…è¯„ä»·')
            level1_summary = metrics_evaluation.get('level1_summary', {})
            
            conclusion_section = f"""## ğŸ’¯ ç»¼åˆç»©æ•ˆè¯„ä»·ç»“è®º

**æ€»ä½“è¯„ä»·ç­‰çº§**: {grade}  
**ç»¼åˆå¾—åˆ†**: {total_score:.2f}åˆ†

### ä¸€çº§æŒ‡æ ‡å¾—åˆ†æƒ…å†µ

| ä¸€çº§æŒ‡æ ‡ | æ ‡å‡†åˆ†å€¼ | å®é™…å¾—åˆ† | å¾—åˆ†ç‡ |
|---------|---------|---------|--------|
| å†³ç­–æŒ‡æ ‡ | 15.00åˆ† | {level1_summary.get('å†³ç­–', 0):.2f}åˆ† | {(level1_summary.get('å†³ç­–', 0)/15*100):.1f}% |
| è¿‡ç¨‹æŒ‡æ ‡ | 25.00åˆ† | {level1_summary.get('è¿‡ç¨‹', 0):.2f}åˆ† | {(level1_summary.get('è¿‡ç¨‹', 0)/25*100):.1f}% |
| äº§å‡ºæŒ‡æ ‡ | 35.00åˆ† | {level1_summary.get('äº§å‡º', 0):.2f}åˆ† | {(level1_summary.get('äº§å‡º', 0)/35*100):.1f}% |
| æ•ˆç›ŠæŒ‡æ ‡ | 25.00åˆ† | {level1_summary.get('æ•ˆç›Š', 0):.2f}åˆ† | {(level1_summary.get('æ•ˆç›Š', 0)/25*100):.1f}% |
| **åˆè®¡** | **100.00åˆ†** | **{total_score:.2f}åˆ†** | **{total_score:.1f}%** |

æ ¹æ®ç»©æ•ˆè¯„ä»·ç»“æœï¼Œæœ¬é¡¹ç›®åœ¨å„é¡¹æŒ‡æ ‡ä¸­çš„è¡¨ç°{"ä¼˜ç§€" if total_score >= 90 else "è‰¯å¥½" if total_score >= 80 else "ä¸­ç­‰" if total_score >= 70 else "æœ‰å¾…æ”¹è¿›"}ï¼Œè¯¦ç»†çš„æŒ‡æ ‡è¯„åˆ†æƒ…å†µè¯·å‚è§é™„ä»¶Aã€‚

---
"""
            report_parts.append(conclusion_section)
        
        # æ·»åŠ æ‰€æœ‰ä¸»æŠ¥å‘Šç« èŠ‚
        for section in sections:
            report_parts.append(section)
            report_parts.append("")
        
        # æ·»åŠ æŒ‡æ ‡è¯„åˆ†é™„ä»¶
        if metrics_evaluation and metrics_evaluation.get('metrics_scores'):
            logger.info("æ·»åŠ æŒ‡æ ‡è¯„åˆ†é™„ä»¶...")
            
            report_parts.extend([
                "---",
                "",
                "# ğŸ“Š é™„ä»¶ï¼šæŒ‡æ ‡è¯„åˆ†è¯¦è¡¨",
                "",
                "## é™„ä»¶Aï¼šå„é¡¹æŒ‡æ ‡è¯„åˆ†æ˜ç»†",
                ""
            ])
            
            # æŒ‰ä¸€çº§æŒ‡æ ‡åˆ†ç»„
            level1_groups = {"å†³ç­–": [], "è¿‡ç¨‹": [], "äº§å‡º": [], "æ•ˆç›Š": []}
            for score_item in metrics_evaluation['metrics_scores']:
                metric = score_item['metric']
                level1 = metric.get('ä¸€çº§æŒ‡æ ‡', 'å…¶ä»–')
                if level1 in level1_groups:
                    level1_groups[level1].append(score_item)
            
            # ç”Ÿæˆå„ä¸€çº§æŒ‡æ ‡çš„è¯¦ç»†è¯„åˆ†
            for level1_name, score_items in level1_groups.items():
                if not score_items:
                    continue
                    
                report_parts.extend([
                    f"### A.{list(level1_groups.keys()).index(level1_name)+1} {level1_name}æŒ‡æ ‡è¯„åˆ†",
                    ""
                ])
                
                for score_item in score_items:
                    metric = score_item['metric']
                    score = score_item['score']
                    opinion = score_item['opinion']
                    weight_score = score_item['weight_score']
                    
                    metric_detail = f"""#### {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')}

**æŒ‡æ ‡æƒé‡**: {metric.get('åˆ†å€¼', 0)}åˆ†  
**è¯„ä»·ç±»å‹**: {metric.get('evaluation_type', 'æ ‡å‡†è¯„ä»·')}  
**å®é™…å¾—åˆ†**: {score:.2f}åˆ†  
**æƒé‡å¾—åˆ†**: {weight_score:.2f}åˆ†  

**è¯„ä»·æ„è§**:  
{opinion}

**è¯„åˆ†ä¾æ®**:  
{metric.get('scoring_method', 'æ— å…·ä½“è¯„åˆ†æ–¹æ³•')}

---
"""
                    report_parts.append(metric_detail)
            
            # æ·»åŠ ç»¼åˆè¯„ä»·æ±‡æ€»è¡¨
            report_parts.extend([
                "",
                "## é™„ä»¶Bï¼šç»¼åˆè¯„ä»·æ±‡æ€»è¡¨",
                "",
                "### B.1 æŒ‡æ ‡æƒé‡ä¸å¾—åˆ†æ±‡æ€»",
                "",
                "| æŒ‡æ ‡åç§° | ä¸€çº§æŒ‡æ ‡ | æƒé‡åˆ†å€¼ | å®é™…å¾—åˆ† | æƒé‡å¾—åˆ† | è¯„ä»·ç±»å‹ |",
                "|---------|---------|---------|---------|---------|---------|"
            ])
            
            for score_item in metrics_evaluation['metrics_scores']:
                metric = score_item['metric']
                score = score_item['score']
                weight_score = score_item['weight_score']
                
                row = f"| {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} | {metric.get('ä¸€çº§æŒ‡æ ‡', '')} | {metric.get('åˆ†å€¼', 0)}åˆ† | {score:.2f}åˆ† | {weight_score:.2f}åˆ† | {metric.get('evaluation_type', '')} |"
                report_parts.append(row)
            
            # æ€»åˆ†è¡Œ
            total_weight = sum([m['metric'].get('åˆ†å€¼', 0) for m in metrics_evaluation['metrics_scores']])
            total_score = metrics_evaluation.get('total_score', 0)
            report_parts.extend([
                f"| **æ€»è®¡** | **å…¨éƒ¨** | **{total_weight}åˆ†** | **-** | **{total_score:.2f}åˆ†** | **ç»¼åˆè¯„ä»·** |",
                "",
                f"**æœ€ç»ˆè¯„ä»·ç­‰çº§**: {metrics_evaluation.get('grade', 'å¾…è¯„ä»·')}",
                ""
            ])
        
        # æ·»åŠ æŠ¥å‘Šå°¾éƒ¨
        report_parts.extend([
            "---",
            "",
            "## ğŸ“„ æŠ¥å‘Šè¯´æ˜",
            "",
            "- æœ¬æŠ¥å‘ŠåŸºäºå‘é‡æ£€ç´¢æŠ€æœ¯å’Œæ ‡å‡†åŒ–è¯„ä»·æ–¹æ³•ç”Ÿæˆ",
            "- æŒ‡æ ‡è¯„åˆ†é‡‡ç”¨6ç§æ ‡å‡†åŒ–è¯„ä»·ç±»å‹è¿›è¡Œä¸“ä¸šè¯„ä¼°", 
            "- æ‰€æœ‰è¯„ä»·æ„è§å‡åŸºäºå®é™…é¡¹ç›®èµ„æ–™å’Œæ•°æ®åˆ†æ",
            "",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*",
            f"*è¯„ä»·èŒƒå›´: å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šå››ä¸ªç»´åº¦*",
            f"*è¯„ä»·æ–¹æ³•: æ ‡å‡†åŒ–ç»©æ•ˆè¯„ä»·ä½“ç³»*"
        ])
        
        final_report = "\n".join(report_parts)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ - æ·»åŠ æ—¶é—´æˆ³é¿å…è¦†ç›–
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        report_filename = f"final_report_{timestamp}.md"
        
        if hasattr(self, '_project_repo') and self._project_repo:
            report_path = self._project_repo.docs.workdir / report_filename
        else:
            report_path = DEFAULT_WORKSPACE_ROOT / report_filename
        
        report_path.write_text(final_report, encoding='utf-8')
        
        logger.info(f"å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        if metrics_evaluation and metrics_evaluation.get('metrics_scores'):
            logger.info(f"ğŸ“Š æŠ¥å‘ŠåŒ…å« {len(metrics_evaluation['metrics_scores'])} ä¸ªæŒ‡æ ‡çš„è¯¦ç»†è¯„åˆ†")
            logger.info(f"ğŸ“Š ç»¼åˆå¾—åˆ†: {metrics_evaluation.get('total_score', 0):.2f}åˆ†")
        
        return final_report

