#!/usr/bin/env python
"""
ç ”ç©¶Actioné›†åˆ - ProductManagerä¸“ç”¨
æ•´åˆMetaGPTåŸç”ŸProductManagerçš„ä¼˜ç§€å®è·µå’ŒCaseExpertçš„ç ”ç©¶èƒ½åŠ›
å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–æç¤ºè¯å’Œç ”ç©¶é€»è¾‘
"""
import asyncio
from datetime import datetime
from typing import List, Optional, Dict, Tuple
import hashlib
import time
from pathlib import Path

from pydantic import BaseModel, Field, TypeAdapter
from metagpt.actions import Action
from metagpt.logs import logger
from metagpt.tools.search_engine import SearchEngine
from metagpt.tools.web_browser_engine import WebBrowserEngine
from metagpt.utils.project_repo import ProjectRepo
from metagpt.utils.common import OutputParser
from backend.tools.search_utils import normalize_keywords
from backend.config.performance_constants import (
    ENV_COMPREHENSIVE_RESEARCH_BASE_SYSTEM,
    ENV_RESEARCH_TOPIC_SYSTEM,
    ENV_SEARCH_KEYWORDS_PROMPT,
    ENV_DECOMPOSE_RESEARCH_PROMPT,
    ENV_RANK_URLS_PROMPT,
    ENV_WEB_CONTENT_ANALYSIS_PROMPT,
    ENV_GENERATE_RESEARCH_BRIEF_PROMPT,
    ENV_ENHANCEMENT_QUERIES,
    ENV_RESEARCH_DECOMPOSITION_NUMS,
    ENV_RESEARCH_URLS_PER_QUERY,
    ENV_FALLBACK_KEYWORDS,
    ENV_MAX_INPUT_TOKENS,
)

# MetaGPT åŸç”Ÿ RAG ç»„ä»¶ - å¼ºåˆ¶ä½¿ç”¨ï¼Œä¸å†æä¾›ç®€åŒ–ç‰ˆæœ¬


# --- æç¤ºè¯æ”¹ä¸ºé…ç½®é©±åŠ¨ï¼Œç§»é™¤ç¡¬ç¼–ç  ---


class Document(BaseModel):
    """å•ä¸ªæ–‡æ¡£çš„ç»“æ„åŒ–æ¨¡å‹"""
    filename: str
    content: str


class Documents(BaseModel):
    """æ–‡æ¡£é›†åˆçš„ç»“æ„åŒ–æ¨¡å‹"""
    docs: List[Document] = Field(default_factory=list)


class ResearchData(BaseModel):
    """ç ”ç©¶æˆæœçš„ç»“æ„åŒ–æ•°æ®æ¨¡å‹ï¼ˆå·²å¯¹é½æ–°çŸ¥è¯†åº“é€»è¾‘ï¼Œä»…ä¿ç•™å¿…è¦å­—æ®µï¼‰"""
    brief: str = Field(..., description="åŸºäºç ”ç©¶ç”Ÿæˆçš„ç»¼åˆæ€§ç®€æŠ¥")
    vector_store_path: str = Field(..., description="å­˜å‚¨ç ”ç©¶å†…å®¹å‘é‡ç´¢å¼•çš„è·¯å¾„")


class PrepareDocuments(Action):
    """æ‰«ææœ¬åœ°ç›®å½•ï¼ŒåŠ è½½ç”¨æˆ·æä¾›çš„æ–‡æ¡£ä½œä¸ºç ”ç©¶ææ–™"""
    
    async def run(self, uploads_path: Path) -> Documents:
        """æ‰«æuploadsç›®å½•ï¼Œè¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹"""
        docs = []
        if not uploads_path.exists():
            logger.warning(f"ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨: {uploads_path}")
            return Documents(docs=docs)

        logger.info(f"å¼€å§‹æ‰«ææ–‡æ¡£ç›®å½•: {uploads_path}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {'.txt', '.md', '.csv', '.json', '.yaml', '.yml'}
        
        for file_path in uploads_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                try:
                    content = file_path.read_text(encoding='utf-8')
                    docs.append(Document(filename=file_path.name, content=content))
                    logger.info(f"æˆåŠŸè¯»å–æ–‡æ¡£: {file_path.name}")
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"æ–‡æ¡£æ‰«æå®Œæˆï¼Œå…±è¯»å– {len(docs)} ä¸ªæ–‡æ¡£")
        return Documents(docs=docs)


class ConductComprehensiveResearch(Action):
    """
    ç»¼åˆç ”ç©¶Action - æ•´åˆæœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œç ”ç©¶
    å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–ç ”ç©¶é€»è¾‘å’Œæç¤ºè¯
    """
    
    def __init__(self, search_engine: SearchEngine = None, **kwargs):
        super().__init__(**kwargs)
        self.search_engine = search_engine

    async def run(
        self, 
        topic: str,
        decomposition_nums: int = ENV_RESEARCH_DECOMPOSITION_NUMS,
        url_per_query: int = ENV_RESEARCH_URLS_PER_QUERY,
        project_repo: ProjectRepo = None,
        local_docs: Documents = None
    ) -> ResearchData:
        """æ‰§è¡Œå…¨é¢çš„ç ”ç©¶ï¼Œæ•´åˆç½‘ç»œæœç´¢å’Œæœ¬åœ°æ–‡æ¡£ï¼Œæ„å»ºå‘é‡çŸ¥è¯†åº“"""
        logger.info(f"å¼€å§‹å¯¹ä¸»é¢˜ '{topic}' è¿›è¡Œå…¨é¢ç ”ç©¶ï¼ŒåŒ…æ‹¬å‘é‡åŒ–å¤„ç†...")

        # 1. ã€ä¼˜å…ˆã€‘ä½¿ç”¨ç»Ÿä¸€æ··åˆæ£€ç´¢æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“ï¼ˆå¦‚æœæä¾›æœ¬åœ°æ–‡æ¡£ï¼‰
        vector_store_path = ""
        if local_docs and local_docs.docs:
            logger.info("æ£€æµ‹åˆ°æœ¬åœ°æ–‡æ¡£ï¼Œä½¿ç”¨ç»Ÿä¸€æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“...")
            vector_store_path, _ = await self._build_project_knowledge_base_unified(
                topic, local_docs, project_repo
            )
            logger.info(f"âœ… é¡¹ç›®çŸ¥è¯†åº“æ„å»ºæˆåŠŸ: {vector_store_path}")
            logger.info("âœ… ç»Ÿä¸€æ£€ç´¢æœåŠ¡å·²å‡†å¤‡å°±ç»ªã€‚")
        
        # 2. ç½‘ç»œç ”ç©¶ (å¦‚æœæœ‰é¡¹ç›®çŸ¥è¯†åº“ï¼Œå°†ç”¨äºRAGå¢å¼º)
        online_research_content = await self._conduct_online_research(
            topic, 
            decomposition_nums, 
            url_per_query,
            project_vector_path=vector_store_path  # ä¼ é€’é¡¹ç›®çŸ¥è¯†åº“è·¯å¾„ç”¨äºRAGå¢å¼º
        )

        # 3. å°†ç½‘ç»œç ”ç©¶å†…å®¹ä¹Ÿæ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“ï¼ˆå®ç°å…±å»ºå…±äº«ï¼‰
        if online_research_content and vector_store_path:
            logger.info("ğŸ”„ å°†ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“...")
            await self._add_online_content_to_project(online_research_content, vector_store_path, topic, project_repo)
        elif online_research_content and not vector_store_path:
            # å¦‚æœæ²¡æœ‰æœ¬åœ°æ–‡æ¡£ï¼Œä¸ºç½‘ç»œå†…å®¹åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“
            logger.info("ğŸ“ ä¸ºç½‘ç»œç ”ç©¶å†…å®¹åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“...")
            vector_store_path, _ = await self._build_project_knowledge_base_unified(
                topic, Documents(), project_repo, online_content=online_research_content
            )

        # 4. ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼ºå†…å®¹æ•´åˆ
        combined_content = online_research_content
        if local_docs and local_docs.docs:
            local_docs_content = "\n\n--- æœ¬åœ°çŸ¥è¯†åº“ ---\n"
            for doc in local_docs.docs:
                local_docs_content += f"### æ–‡æ¡£: {doc.filename}\n{doc.content}\n\n"
            combined_content += local_docs_content
        
        # ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢å¢å¼ºç ”ç©¶ç®€æŠ¥ç”Ÿæˆ
        enhanced_content = await self._enhance_research_with_intelligent_search(
            topic, combined_content, vector_store_path
        )

        prompt = ENV_GENERATE_RESEARCH_BRIEF_PROMPT.format(content=enhanced_content, topic=topic)
        # é˜²æ­¢è¶…é•¿è¾“å…¥è§¦å‘åº•å±‚æä¾›å•†é•¿åº¦é™åˆ¶ï¼šæˆªæ–­åˆ°å®‰å…¨é•¿åº¦
        safe_prompt = prompt[:ENV_MAX_INPUT_TOKENS]
        brief = await self._aask(safe_prompt, [ENV_COMPREHENSIVE_RESEARCH_BASE_SYSTEM])
        
        logger.info(f"ç ”ç©¶ç®€æŠ¥ç”Ÿæˆå®Œæ¯•ã€‚")

        # 5. ç¡®ä¿æœ€ç»ˆå‘é‡åº“è·¯å¾„å­˜åœ¨ï¼ˆå¿…é¡»ï¼‰
        if not vector_store_path:
            raise ValueError("é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç ”ç©¶æµç¨‹")

        # 6. åˆ›å»ºå¹¶è¿”å›ResearchData
        research_data = ResearchData(
            brief=brief,
            vector_store_path=vector_store_path,
        )

        if project_repo:
            docs_filename = "research_brief.md"  # ä½¿ç”¨å›ºå®šçš„æ–‡ä»¶å
            await project_repo.docs.save(filename=docs_filename, content=brief)
            brief_path = project_repo.docs.workdir / docs_filename
            logger.info(f"ç ”ç©¶ç®€æŠ¥å·²ä¿å­˜åˆ°: {brief_path}")

        return research_data

    # ========== ğŸš€ æ–°çš„ç»Ÿä¸€çŸ¥è¯†åº“ç®¡ç†æ–¹æ³• ==========
    
    async def _build_project_knowledge_base_unified(
        self, 
        topic: str, 
        local_docs: Documents, 
        project_repo=None, 
        online_content: str = ""
    ) -> tuple[str, List[str]]:
        """
        ğŸš€ ä½¿ç”¨ç»Ÿä¸€çš„æ··åˆæ£€ç´¢æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“
        """
        try:
            from backend.services.hybrid_search import hybrid_search
            
            # ç¡®å®šé¡¹ç›®ID
            project_id = project_repo.workdir.name if project_repo else f"research_{hash(topic) % 10000}"
            
            # åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“
            project_vector_path = hybrid_search.create_project_knowledge_base(project_id)
            
            # å‡†å¤‡è¦æ·»åŠ çš„å†…å®¹
            contents_to_add = []
            
            # æ·»åŠ æœ¬åœ°æ–‡æ¡£
            if local_docs and local_docs.docs:
                for doc in local_docs.docs:
                    contents_to_add.append({
                        "content": doc.content,
                        "filename": doc.filename
                    })
                logger.info(f"ğŸ“„ å‡†å¤‡æ·»åŠ  {len(local_docs.docs)} ä¸ªæœ¬åœ°æ–‡æ¡£")
            
            # æ·»åŠ ç½‘ç»œç ”ç©¶å†…å®¹
            if online_content:
                contents_to_add.append({
                    "content": online_content,
                    "filename": f"ç½‘ç»œç ”ç©¶_{topic}.md"
                })
                logger.info("ğŸŒ å‡†å¤‡æ·»åŠ ç½‘ç»œç ”ç©¶å†…å®¹")
            
            # æ‰¹é‡æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“
            if contents_to_add:
                success = hybrid_search.add_multiple_contents_to_project(contents_to_add, project_vector_path)
                if not success:
                    logger.warning("âš ï¸ éƒ¨åˆ†å†…å®¹æ·»åŠ å¤±è´¥")
            
            logger.info(f"âœ… ç»Ÿä¸€é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå®Œæˆ: {project_vector_path}")
            return project_vector_path, []
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")
            # ä¸é™çº§ï¼Œè®©é”™è¯¯æš´éœ²å‡ºæ¥ï¼Œå¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€æ¶æ„
            raise e
    
    async def _add_online_content_to_project(self, online_content: str, project_vector_path: str, topic: str, project_repo=None):
        """å°†ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ åˆ°ç°æœ‰é¡¹ç›®çŸ¥è¯†åº“"""
        try:
            from backend.services.hybrid_search import hybrid_search
            
            success = hybrid_search.add_content_to_project(
                content=online_content,
                filename=f"ç½‘ç»œç ”ç©¶_{topic}.md",
                project_vector_storage_path=project_vector_path
            )
            
            if success:
                logger.info("âœ… ç½‘ç»œç ”ç©¶å†…å®¹å·²æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“")
            else:
                logger.warning("âš ï¸ ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç½‘ç»œå†…å®¹å¤±è´¥: {e}")
    
    # å·²å¯¹é½ç»Ÿä¸€æ£€ç´¢é€»è¾‘ï¼šä¸å†æ‰‹å·¥è¯»å–å†…å®¹å—
    
    
    async def _enhance_research_with_intelligent_search(
        self, 
        topic: str, 
        combined_content: str, 
        vector_store_path: str
    ) -> str:
        """
        ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢å¢å¼ºç ”ç©¶å†…å®¹
        """
        try:
            from backend.services.intelligent_search import intelligent_search
            
            # ğŸ§  é’ˆå¯¹ç ”ç©¶ç®€æŠ¥ç”Ÿæˆçš„ä¸“é—¨æŸ¥è¯¢ï¼ˆé…ç½®é©±åŠ¨ï¼‰
            enhancement_queries = [q.replace('{topic}', topic) for q in ENV_ENHANCEMENT_QUERIES] if ENV_ENHANCEMENT_QUERIES else []
            
            enhanced_sections = []
            
            for query in enhancement_queries:
                logger.info(f"ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼º: {query}")
                search_result = await intelligent_search.intelligent_search(
                    query=query,
                    project_vector_storage_path=vector_store_path,
                    mode="knowledge_graph",  # ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†å›¾è°±æ¨ç†
                    enable_global=True,
                    max_results=2
                )
                
                if search_result.get("results"):
                    enhanced_sections.append(f"### ğŸ§  æ™ºèƒ½æ£€ç´¢: {query}\n")
                    enhanced_sections.extend(search_result["results"])
                    enhanced_sections.append("\n")
            
            if enhanced_sections:
                enhanced_content = combined_content + "\n\n--- ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼ºå†…å®¹ ---\n" + "\n".join(enhanced_sections)
                logger.info(f"âœ… æ™ºèƒ½æ£€ç´¢å¢å¼ºå®Œæˆï¼Œæ–°å¢ {len(enhanced_sections)} ä¸ªå†…å®¹æ®µ")
                return enhanced_content
            else:
                logger.info("â„¹ï¸ æ™ºèƒ½æ£€ç´¢æœªå‘ç°é¢å¤–å†…å®¹")
                return combined_content
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ™ºèƒ½æ£€ç´¢å¢å¼ºå¤±è´¥: {e}")
            return combined_content
    
    async def _conduct_online_research(self, topic: str, decomposition_nums: int, url_per_query: int, project_vector_path: str = "") -> str:
        """æ‰§è¡Œåœ¨çº¿ç ”ç©¶"""
        if not self.search_engine:
            logger.error("âŒ æœç´¢å¼•æ“æœªåˆå§‹åŒ–ï¼æ— æ³•è¿›è¡Œåœ¨çº¿ç ”ç©¶")
            raise ValueError("æœç´¢å¼•æ“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œåœ¨çº¿ç ”ç©¶ã€‚è¯·æ£€æŸ¥config/config2.yamlä¸­çš„searché…ç½®")
        
        logger.info("æ­¥éª¤ 1: ç”Ÿæˆæœç´¢å…³é”®è¯")
        keywords_prompt = ENV_RESEARCH_TOPIC_SYSTEM.format(topic=topic)
        try:
            keywords_str = await self._aask(
                ENV_SEARCH_KEYWORDS_PROMPT[:ENV_MAX_INPUT_TOKENS],
                [keywords_prompt[:ENV_MAX_INPUT_TOKENS]]
            )
        except Exception as e:
            logger.warning(f"âš ï¸ å…³é”®è¯ç”Ÿæˆè°ƒç”¨å¤±è´¥ï¼Œå°†ä½¿ç”¨å›é€€å…³é”®è¯: {e}")
            fallback = ENV_FALLBACK_KEYWORDS or []
            if not fallback:
                fallback = [topic[:50]]
            import json as _json
            keywords_str = _json.dumps(fallback, ensure_ascii=False)
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(1)
        
        try:
            raw_keywords = OutputParser.extract_struct(keywords_str, list)
        except Exception as e:
            logger.warning(f"âš ï¸ å…³é”®è¯è§£æå¤±è´¥ï¼Œä½¿ç”¨å›é€€å…³é”®è¯: {e}")
            raw_keywords = ENV_FALLBACK_KEYWORDS or [topic[:50]]

        # ç»Ÿä¸€å°†å…³é”®è¯è§„èŒƒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé€šç”¨å·¥å…·ï¼‰
        keywords = normalize_keywords(raw_keywords, topic)

        logger.info(f"å…³é”®è¯: {keywords}")

        # ä¸²è¡Œæœç´¢å…³é”®è¯ï¼Œé¿å…å¹¶å‘è¯·æ±‚è§¦å‘é¢‘ç‡é™åˆ¶
        search_results = []
        for i, kw in enumerate(keywords):
            try:
                if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(2)  # æ¯ä¸ªè¯·æ±‚é—´éš”2ç§’
                result = await self.search_engine.run(kw, as_string=False)
                search_results.append(result)
                logger.info(f"æˆåŠŸæœç´¢å…³é”®è¯: {kw}")
            except Exception as e:
                logger.error(f"æœç´¢å…³é”®è¯å¤±è´¥ {kw}: {e}")
                search_results.append([])  # æ·»åŠ ç©ºç»“æœä¿æŒç´¢å¼•ä¸€è‡´
        
        # ğŸ§  æ™ºèƒ½æ£€ç´¢å¢å¼ºï¼šä½¿ç”¨æ™ºèƒ½æ£€ç´¢æœåŠ¡æŸ¥è¯¢é¡¹ç›®çŸ¥è¯†åº“
        rag_results_str = ""
        if project_vector_path:
            try:
                from backend.services.intelligent_search import intelligent_search
                logger.info("...ğŸ§  å¯åŠ¨æ™ºèƒ½æ£€ç´¢æŸ¥è¯¢é¡¹ç›®çŸ¥è¯†åº“...")
                
                # ä½¿ç”¨æ··åˆæ™ºèƒ½æ£€ç´¢
                search_result = await intelligent_search.intelligent_search(
                    query=" ".join(keywords),
                    project_vector_storage_path=project_vector_path,
                    mode="hybrid",  # ä½¿ç”¨æ··åˆæ™ºèƒ½æ£€ç´¢
                    enable_global=True,
                    max_results=3
                )
                
                if search_result.get("results"):
                    rag_results_str = "\n\n### ğŸ§  æ™ºèƒ½æ£€ç´¢ç›¸å…³ä¿¡æ¯\n" + "\n".join(search_result["results"])
                    
                    # æ·»åŠ æ™ºèƒ½æ´å¯Ÿ
                    if search_result.get("insights"):
                        rag_results_str += "\n\n### ğŸ’¡ æ™ºèƒ½åˆ†ææ´å¯Ÿ\n" + "\n".join(search_result["insights"])
                        
            except Exception as e:
                logger.warning(f"æ™ºèƒ½æ£€ç´¢æŸ¥è¯¢å¤±è´¥: {e}")
        
        search_results_str = "\n".join([f"#### å…³é”®è¯: {kw}\n{res}\n" for kw, res in zip(keywords, search_results)])
        
        # å°†RAGç»“æœå’Œç½‘ç»œæœç´¢ç»“æœåˆå¹¶
        combined_search_results = search_results_str + rag_results_str

        logger.info("æ­¥éª¤ 2: åˆ†è§£ç ”ç©¶é—®é¢˜")
        decompose_prompt = ENV_DECOMPOSE_RESEARCH_PROMPT.format(
            decomposition_nums=decomposition_nums,
            url_per_query=url_per_query,
            search_results=combined_search_results
        )
        queries_str = await self._aask(
            decompose_prompt[:ENV_MAX_INPUT_TOKENS],
            [keywords_prompt[:ENV_MAX_INPUT_TOKENS]]
        )
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(1)
        try:
            queries = OutputParser.extract_struct(queries_str, list)
        except Exception as e:
            logger.warning(f"é—®é¢˜åˆ†è§£å¤±è´¥: {e}, ä½¿ç”¨å…³é”®è¯ä½œä¸ºé—®é¢˜")
            queries = keywords
        
        logger.info(f"ç ”ç©¶é—®é¢˜: {queries}")

        # ä¸²è¡Œå¤„ç†æ¯ä¸ªé—®é¢˜ï¼Œé¿å…å¹¶å‘æœç´¢
        summaries = []
        for i, q in enumerate(queries):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(2)  # æ¯ä¸ªé—®é¢˜å¤„ç†é—´éš”2ç§’ï¼Œå¢åŠ å»¶è¿Ÿ
            summary = await self._search_and_summarize_query(topic, q, url_per_query)
            summaries.append(summary)

        return "\n\n".join(summaries)

    async def _search_and_summarize_query(self, topic: str, query: str, url_per_query: int) -> str:
        """æœç´¢ã€æ’åºå¹¶æ€»ç»“å•ä¸ªé—®é¢˜çš„URL"""
        logger.info(f"å¤„ç†é—®é¢˜: {query}")
        urls = await self._search_and_rank_urls(topic, query, url_per_query)
        
        if not urls:
            return f"### é—®é¢˜: {query}\n\næœªèƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚\n"

        # ä¸²è¡Œæµè§ˆå’Œåˆ†æURLï¼Œé¿å…å¹¶å‘è¯·æ±‚
        contents = []
        for i, url in enumerate(urls):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(1)  # æ¯ä¸ªURLå¤„ç†é—´éš”1ç§’
            content = await self._web_browse_and_summarize(url, query)
            contents.append(content)

        # è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å†…å®¹
        relevant_contents = [c for c in contents if "ä¸ç›¸å…³" not in c]
        
        summary = f"### é—®é¢˜: {query}\n\n" + "\n\n".join(relevant_contents)
        return summary

    async def _search_and_rank_urls(self, topic: str, query: str, num_results: int) -> List[str]:
        """æœç´¢å¹¶æ’åºURL"""
        max_results = max(num_results * 2, 6)
        try:
            results = await self.search_engine.run(query, max_results=max_results, as_string=False)
            if not results:
                logger.error(f"âŒ æœç´¢å¼•æ“æœªè¿”å›ä»»ä½•ç»“æœ: {query}")
                raise ValueError(f"æœç´¢å¼•æ“å¯¹æŸ¥è¯¢'{query}'æœªè¿”å›ä»»ä½•ç»“æœï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé…ç½®é”™è¯¯")
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥ {query}: {e}")
            raise e  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸éšè—
    
        _results_str = "\n".join(f"{i}: {res}" for i, res in enumerate(results))
        time_stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        prompt = ENV_RANK_URLS_PROMPT.format(topic=topic, query=query, results=_results_str, time_stamp=time_stamp)
        
        logger.debug(f"URLæ’åºæç¤ºè¯: {prompt}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        indices_str = await self._aask(prompt[:ENV_MAX_INPUT_TOKENS])
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(0.5)
        
        logger.debug(f"LLMè¿”å›çš„æ’åºç»“æœ: {indices_str}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        try:
            indices = OutputParser.extract_struct(indices_str, list)
            if not indices:
                logger.error(f"âŒ LLMè¿”å›ç©ºçš„æ’åºç´¢å¼•: {indices_str}")
                raise ValueError(f"LLM URLæ’åºå¤±è´¥ï¼Œè¿”å›ç©ºç´¢å¼•åˆ—è¡¨")
            ranked_results = [results[i] for i in indices if i < len(results)]
        except Exception as e:
            logger.error(f"âŒ URLæ’åºå¤±è´¥: {e}")
            raise e  # ä¸é™çº§ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
    
        final_urls = [res['link'] for res in ranked_results[:num_results]]
        logger.info(f"æœ€ç»ˆè·å¾— {len(final_urls)} ä¸ªURLç”¨äºæŸ¥è¯¢: {query}")
        
        return final_urls

    async def _web_browse_and_summarize(self, url: str, query: str) -> str:
        """æµè§ˆç½‘é¡µå¹¶æ€»ç»“å†…å®¹"""
        try:
            content = await WebBrowserEngine().run(url)
            prompt = ENV_WEB_CONTENT_ANALYSIS_PROMPT.format(content=content, query=query)
            summary = await self._aask(prompt[:ENV_MAX_INPUT_TOKENS])
            
            # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
            await asyncio.sleep(1)
            return f"#### æ¥æº: {url}\n{summary}"
        except Exception as e:
            logger.error(f"æµè§ˆURLå¤±è´¥ {url}: {e}")
            return f"#### æ¥æº: {url}\n\næ— æ³•è®¿é—®æˆ–å¤„ç†æ­¤é¡µé¢ã€‚"