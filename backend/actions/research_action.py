#!/usr/bin/env python
"""
ç ”ç©¶Actioné›†åˆ - ProductManagerä¸“ç”¨
æ•´åˆMetaGPTåŸç”ŸProductManagerçš„ä¼˜ç§€å®è·µå’ŒCaseExpertçš„ç ”ç©¶èƒ½åŠ›
å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–æç¤ºè¯å’Œç ”ç©¶é€»è¾‘
"""
import asyncio
from datetime import datetime
from typing import List, Optional, Dict, Tuple
import hashlib
import time
from pathlib import Path

from pydantic import BaseModel, Field, TypeAdapter
from metagpt.actions import Action
from metagpt.logs import logger
from metagpt.tools.search_engine import SearchEngine
from metagpt.tools.web_browser_engine import WebBrowserEngine
from metagpt.utils.project_repo import ProjectRepo
from metagpt.utils.common import OutputParser

# MetaGPT åŸç”Ÿ RAG ç»„ä»¶ - å¼ºåˆ¶ä½¿ç”¨ï¼Œä¸å†æä¾›ç®€åŒ–ç‰ˆæœ¬


# --- æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–æç¤ºè¯ ---
COMPREHENSIVE_RESEARCH_BASE_SYSTEM = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„AIç ”ç©¶åˆ†æå¸ˆå’Œäº§å“ç»ç†ã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1. æ·±å…¥ç†è§£ç”¨æˆ·éœ€æ±‚å’Œä¸šåŠ¡èƒŒæ™¯
2. è¿›è¡Œå…¨é¢çš„å¸‚åœºå’Œæ¡ˆä¾‹ç ”ç©¶
3. ç”Ÿæˆé«˜è´¨é‡çš„ç ”ç©¶ç®€æŠ¥ï¼Œä¸ºåç»­çš„æ¶æ„è®¾è®¡å’Œå†…å®¹ç”Ÿæˆæä¾›åšå®åŸºç¡€
"""

RESEARCH_TOPIC_SYSTEM = "ä½ æ˜¯ä¸€åAIç ”ç©¶åˆ†æå¸ˆï¼Œä½ çš„ç ”ç©¶ä¸»é¢˜æ˜¯:\n#ä¸»é¢˜#\n{topic}"

SEARCH_KEYWORDS_PROMPT = """è¯·æ ¹æ®ä½ çš„ç ”ç©¶ä¸»é¢˜ï¼Œæä¾›æœ€å¤š3ä¸ªå¿…è¦çš„å…³é”®è¯ç”¨äºç½‘ç»œæœç´¢ã€‚
è¿™äº›å…³é”®è¯åº”è¯¥èƒ½å¤Ÿå¸®åŠ©æ”¶é›†åˆ°æœ€ç›¸å…³å’Œæœ€æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚
ä½ çš„å›åº”å¿…é¡»æ˜¯JSONæ ¼å¼ï¼Œä¾‹å¦‚ï¼š["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]ã€‚"""

DECOMPOSE_RESEARCH_PROMPT = """### è¦æ±‚
1. ä½ çš„ç ”ç©¶ä¸»é¢˜å’Œåˆæ­¥æœç´¢ç»“æœå±•ç¤ºåœ¨"å‚è€ƒä¿¡æ¯"éƒ¨åˆ†ã€‚
2. è¯·åŸºäºè¿™äº›ä¿¡æ¯ï¼Œç”Ÿæˆæœ€å¤š {decomposition_nums} ä¸ªä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³çš„ã€æ›´å…·ä½“çš„è°ƒæŸ¥é—®é¢˜ã€‚
3. è¿™äº›é—®é¢˜åº”è¯¥èƒ½å¤Ÿå¸®åŠ©æ·±å…¥äº†è§£ä¸»é¢˜çš„ä¸åŒæ–¹é¢ï¼ŒåŒ…æ‹¬å¸‚åœºè¶‹åŠ¿ã€æœ€ä½³å®è·µã€æŠ€æœ¯æ–¹æ¡ˆç­‰ã€‚
4. ä½ çš„å›åº”å¿…é¡»æ˜¯JSONæ ¼å¼ï¼š["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3", ...]ã€‚

### å‚è€ƒä¿¡æ¯
{search_results}
"""

RANK_URLS_PROMPT = """### ä¸»é¢˜
{topic}
### å…·ä½“é—®é¢˜
{query}
### åŸå§‹æœç´¢ç»“æœ
{results}
### è¦æ±‚
è¯·ç§»é™¤ä¸å…·ä½“é—®é¢˜æˆ–ä¸»é¢˜æ— å…³çš„æœç´¢ç»“æœã€‚
å¦‚æœé—®é¢˜å…·æœ‰æ—¶æ•ˆæ€§ï¼Œè¯·ç§»é™¤è¿‡æ—¶çš„ä¿¡æ¯ã€‚å½“å‰æ—¶é—´æ˜¯ {time_stamp}ã€‚
ç„¶åï¼Œæ ¹æ®é“¾æ¥çš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§å¯¹å‰©ä¸‹çš„ç»“æœè¿›è¡Œæ’åºã€‚
ä¼˜å…ˆè€ƒè™‘ï¼šå®˜æ–¹æ–‡æ¡£ã€æƒå¨æœºæ„ã€çŸ¥åä¼ä¸šæ¡ˆä¾‹ã€å­¦æœ¯ç ”ç©¶ç­‰ã€‚
ä»¥JSONæ ¼å¼æä¾›æ’åºåç»“æœçš„ç´¢å¼•ï¼Œä¾‹å¦‚ [0, 1, 3, 4, ...]ï¼Œä¸è¦åŒ…å«å…¶ä»–ä»»ä½•æ–‡å­—ã€‚
"""

WEB_CONTENT_ANALYSIS_PROMPT = """### è¦æ±‚
1. åˆ©ç”¨"å‚è€ƒä¿¡æ¯"ä¸­çš„æ–‡æœ¬æ¥å›ç­”é—®é¢˜"{query}"ã€‚
2. å¦‚æœæ— æ³•ç›´æ¥å›ç­”ï¼Œä½†å†…å®¹ä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³ï¼Œè¯·å¯¹æ–‡æœ¬è¿›è¡Œå…¨é¢æ€»ç»“ã€‚
3. å¦‚æœå†…å®¹å®Œå…¨ä¸ç›¸å…³ï¼Œè¯·å›å¤"ä¸ç›¸å…³"ã€‚
4. é‡ç‚¹æå–ï¼šå…³é”®æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯ã€æœ€ä½³å®è·µã€æŠ€æœ¯æ–¹æ¡ˆã€å¸‚åœºè¶‹åŠ¿ç­‰ã€‚
5. ä¿æŒå®¢è§‚æ€§ï¼Œæ³¨æ˜ä¿¡æ¯æ¥æºçš„å¯ä¿¡åº¦ã€‚

### å‚è€ƒä¿¡æ¯
{content}
"""

GENERATE_RESEARCH_BRIEF_PROMPT = """### å‚è€ƒä¿¡æ¯
{content}

### æŒ‡ä»¤
ä½ æ˜¯ä¸€åé¡¶çº§çš„AIç ”ç©¶åˆ†æå¸ˆï¼Œä½ çš„ç›®æ ‡æ˜¯ä¸ºä¸‹æ¸¸çš„æŠ¥å‘Šæ¶æ„å¸ˆæä¾›ä¸€ä»½å……æ»¡æ´å¯ŸåŠ›çš„å‰æœŸè°ƒç ”ç®€æŠ¥ã€‚
è¯·æ ¹æ®ä»¥ä¸Šå‚è€ƒä¿¡æ¯ï¼ˆåŒ…å«ç½‘ç»œæ¡ˆä¾‹å’Œæœ¬åœ°çŸ¥è¯†åº“ï¼‰ï¼Œå›´ç»•ä¸»é¢˜â€œ{topic}â€ç”Ÿæˆä¸€ä»½é«˜è´¨é‡çš„ç ”ç©¶ç®€æŠ¥ã€‚

ç®€æŠ¥æ ‡é¢˜æ ¼å¼åº”ä¸ºï¼š**ã€Šå½“å‰é¡¹ç›®ç»©æ•ˆè¯„ä»·æŠ¥å‘Šå‰æœŸè°ƒç ”ç®€æŠ¥ã€‹**

### ç®€æŠ¥ç»“æ„è¦æ±‚ (è¯·ä¸¥æ ¼éµå¾ª)

**1. é¡¹ç›®ç«‹é¡¹èƒŒæ™¯åŠç›®çš„åˆ†æ**
   - **åˆ†æè¦æ±‚**: ç»¼åˆç½‘ç»œæ¡ˆä¾‹å’Œæœ¬åœ°èµ„æ–™ï¼Œåˆ†ææ­¤ç±»é¡¹ç›®é€šå¸¸åœ¨ä½•ç§èƒŒæ™¯ä¸‹ç«‹é¡¹ï¼Œå…¶æ ¸å¿ƒç›®çš„æ˜¯ä»€ä¹ˆã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: æŒ‡å‡ºåœ¨æ’°å†™æœ€ç»ˆæŠ¥å‘Šçš„â€œé¡¹ç›®æ¦‚è¿°â€ç« èŠ‚æ—¶ï¼Œåº”é‡ç‚¹çªå‡ºå“ªäº›èƒŒæ™¯å› ç´ å’Œé¡¹ç›®ç›®çš„ï¼Œä»¥å½°æ˜¾å…¶å¿…è¦æ€§å’Œé‡è¦æ€§ã€‚

**2. é¡¹ç›®ä¸»è¦å†…å®¹æ´å¯Ÿ**
   - **åˆ†æè¦æ±‚**: å‰–æä¸åŒè¡Œä¸šçš„ç±»ä¼¼ç»©æ•ˆæŠ¥å‘Šé€šå¸¸åŒ…å«å“ªäº›æ ¸å¿ƒå†…å®¹ã€‚ç»“åˆæœ¬åœ°èµ„æ–™ï¼Œæç‚¼å‡ºæœ¬æ¬¡æŠ¥å‘Šåº”é‡ç‚¹å…³æ³¨çš„å‡ ä¸ªæ–¹é¢ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºæŠ¥å‘Šçš„â€œé¡¹ç›®æ¦‚è¿°â€éƒ¨åˆ†æä¾›å†…å®¹å»ºè®®ï¼ŒæŒ‡å‡ºå“ªäº›æ˜¯å¿…é¡»åŒ…å«çš„å…³é”®æ¨¡å—ã€‚

**3. èµ„é‡‘æŠ•å…¥å’Œä½¿ç”¨æƒ…å†µåˆ†æ**
   - **åˆ†æè¦æ±‚**: æ€»ç»“ç½‘ç»œæ¡ˆä¾‹ä¸­å…³äºèµ„é‡‘ç®¡ç†å’Œä½¿ç”¨çš„ç»éªŒä¸æ•™è®­ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: æé†’æ¶æ„å¸ˆåœ¨è®¾è®¡â€œèµ„é‡‘ä½¿ç”¨â€ç›¸å…³ç« èŠ‚æ—¶ï¼Œåº”å¼•å¯¼å†™ä½œè€…é‡ç‚¹å…³æ³¨å“ªäº›æ•°æ®ï¼ˆå¦‚é¢„ç®—æ‰§è¡Œç‡ã€èµ„é‡‘åˆ°ä½åŠæ—¶æ€§ç­‰ï¼‰ï¼Œå¹¶å¯ä»¥ä»å“ªäº›è§’åº¦è¿›è¡Œåˆ†æã€‚

**4. é¡¹ç›®å®æ–½ä¸ç»„ç»‡ç®¡ç†ç»éªŒå€Ÿé‰´**
   - **åˆ†æè¦æ±‚**: ä»å‚è€ƒä¿¡æ¯ä¸­æç‚¼å‡ºé¡¹ç›®å®æ–½å’Œç»„ç»‡ç®¡ç†æ–¹é¢çš„æœ€ä½³å®è·µæˆ–å¸¸è§é—®é¢˜ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºæŠ¥å‘Šçš„â€œç»„ç»‡ç®¡ç†â€éƒ¨åˆ†æä¾›å†™ä½œæ–¹å‘ï¼Œä¾‹å¦‚å¯ä»¥å€Ÿé‰´å“ªäº›ç®¡ç†æ¨¡å¼ï¼Œæˆ–è€…éœ€è¦è§„é¿å“ªäº›å¸¸è§é£é™©ã€‚

**5. ç»©æ•ˆç›®æ ‡è®¾å®šè¦ç‚¹**
   - **åˆ†æè¦æ±‚**: åŸºäºå¯¹å››ä¸ªç»´åº¦ï¼ˆå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šï¼‰çš„ç†è§£ï¼Œåˆ†æç½‘ç»œæ¡ˆä¾‹ä¸­æ˜¯å¦‚ä½•è®¾å®šç»©æ•ˆç›®æ ‡çš„ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: æŒ‡å‡ºåœ¨è®¾è®¡æŠ¥å‘Šçš„â€œç»©æ•ˆç›®æ ‡â€éƒ¨åˆ†æ—¶ï¼Œå››ä¸ªç»´åº¦åˆ†åˆ«å¯ä»¥ä»å“ªäº›è§’åº¦è¿›è¡Œç›®æ ‡è®¾å®šï¼Œå¹¶æä¾›å¯ä»¥å€Ÿé‰´çš„ç›®æ ‡æè¿°æ–¹å¼ã€‚**è¿™ä¸æ˜¯æœ€ç»ˆçš„æŒ‡æ ‡ï¼Œè€Œæ˜¯ç›®æ ‡è®¾å®šçš„æ€è·¯å’Œæ–¹å‘**ã€‚

**6. å­˜åœ¨çš„é—®é¢˜å’ŒåŸå› åˆ†æ**
   - **åˆ†æè¦æ±‚**: åŸºäºç½‘ç»œæ¡ˆä¾‹å’Œæœ¬åœ°èµ„æ–™ï¼Œå½’çº³æ€»ç»“ç±»ä¼¼é¡¹ç›®åœ¨å®æ–½è¿‡ç¨‹ä¸­æ™®éå­˜åœ¨çš„é—®é¢˜ï¼ˆå¦‚èµ„é‡‘ä½¿ç”¨æ•ˆç‡ä½ã€ç›®æ ‡è¾¾æˆç‡ä¸è¶³ã€ç›‘ç®¡ç¼ºä½ç­‰ï¼‰ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: é¢„æµ‹å½“å‰é¡¹ç›®å¯èƒ½é¢ä¸´çš„æ½œåœ¨é£é™©å’ŒæŒ‘æˆ˜ã€‚è¿™éƒ¨åˆ†å†…å®¹å°†ä½œä¸ºæŠ¥å‘Šä¸­â€œå­˜åœ¨çš„é—®é¢˜â€ç« èŠ‚çš„é‡è¦å‚è€ƒï¼Œä½¿å¾—æœ€ç»ˆæŠ¥å‘Šä¸ä»…èƒ½æ€»ç»“æˆç»©ï¼Œæ›´èƒ½ä½“ç°å‰ç»æ€§çš„é£é™©æ€è€ƒã€‚

**7. æ”¹è¿›å»ºè®®ä¸ç»éªŒå€Ÿé‰´**
   - **åˆ†æè¦æ±‚**: æ·±å…¥åˆ†ææˆåŠŸæ¡ˆä¾‹æ˜¯å¦‚ä½•è§£å†³ä¸Šè¿°æ™®éé—®é¢˜çš„ï¼Œæç‚¼å‡ºå¯æ“ä½œã€å¯å€Ÿé‰´çš„æ”¹è¿›æªæ–½å’Œç®¡ç†ç»éªŒã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºæŠ¥å‘Šçš„â€œæ”¹è¿›å»ºè®®â€ç« èŠ‚æä¾›ç´ æã€‚è¿™äº›åŸºäºçœŸå®æ¡ˆä¾‹çš„å»ºè®®ï¼Œå°†æ¯”æ³›æ³›è€Œè°ˆçš„é€šç”¨å»ºè®®æ›´å…·è¯´æœåŠ›å’Œå¯æ“ä½œæ€§ã€‚

**8. ç»©æ•ˆè¯„ä»·ä½“ç³»æ¨è (æ ¸å¿ƒå†…å®¹)**
   - **åˆ†æè¦æ±‚**: åŸºäºæ‰€æœ‰ç ”ç©¶ï¼Œä¸ºå½“å‰é¡¹ç›®æ¨èä¸€å¥—ç»©æ•ˆè¯„ä»·æŒ‡æ ‡ä½“ç³»ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºâ€œå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šâ€å››ä¸ªç»´åº¦ï¼Œ**æ¯ä¸ªç»´åº¦æ¨è5ä¸ªå…·ä½“ã€å¯è¡¡é‡çš„è¯„ä»·æŒ‡æ ‡**ã€‚æŒ‡æ ‡åº”æ¸…æ™°ï¼Œå¹¶ç®€è¦è¯´æ˜æ¨èç†ç”±ã€‚è¿™å°†æ˜¯æ¶æ„å¸ˆè®¾è®¡æŒ‡æ ‡ä½“ç³»çš„é‡è¦è¾“å…¥ã€‚

### è´¨é‡è¦æ±‚
- **æ·±åº¦æ´å¯Ÿ**: ä¸è¦ç®€å•ç½—åˆ—ä¿¡æ¯ï¼Œè¦æç‚¼è§‚ç‚¹ã€æ€»ç»“ç»éªŒã€ç»™å‡ºå»ºè®®ã€‚
- **å¼ºåŠ›æ”¯æŒ**: ç®€æŠ¥çš„æ¯ä¸€éƒ¨åˆ†éƒ½åº”ä¸ºä¸‹æ¸¸çš„æ¶æ„å¸ˆæä¾›æ¸…æ™°çš„ã€å¯æ“ä½œçš„æŒ‡å¯¼ã€‚
- **æ ¼å¼æ¸…æ™°**: ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æ„åˆ†æ˜ã€‚


**å¼•ç”¨è¦æ±‚ï¼š**
- åœ¨ç®€æŠ¥æœ«å°¾åˆ—å‡ºæ‰€æœ‰ä¿¡æ¯æ¥æºçš„URL
- å¯¹å…³é”®æ•°æ®å’Œè§‚ç‚¹è¿›è¡Œé€‚å½“çš„å¼•ç”¨æ ‡æ³¨
"""


class Document(BaseModel):
    """å•ä¸ªæ–‡æ¡£çš„ç»“æ„åŒ–æ¨¡å‹"""
    filename: str
    content: str


class Documents(BaseModel):
    """æ–‡æ¡£é›†åˆçš„ç»“æ„åŒ–æ¨¡å‹"""
    docs: List[Document] = Field(default_factory=list)


class ResearchData(BaseModel):
    """ç ”ç©¶æˆæœçš„ç»“æ„åŒ–æ•°æ®æ¨¡å‹"""
    brief: str = Field(..., description="åŸºäºç ”ç©¶ç”Ÿæˆçš„ç»¼åˆæ€§ç®€æŠ¥")
    vector_store_path: str = Field(..., description="å­˜å‚¨ç ”ç©¶å†…å®¹å‘é‡ç´¢å¼•çš„è·¯å¾„")
    content_chunks: List[str] = Field(default_factory=list, description="åˆ†å—çš„å†…å®¹åˆ—è¡¨ï¼Œä¾›RAGæ£€ç´¢ä½¿ç”¨")


class PrepareDocuments(Action):
    """æ‰«ææœ¬åœ°ç›®å½•ï¼ŒåŠ è½½ç”¨æˆ·æä¾›çš„æ–‡æ¡£ä½œä¸ºç ”ç©¶ææ–™"""
    
    async def run(self, uploads_path: Path) -> Documents:
        """æ‰«æuploadsç›®å½•ï¼Œè¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹"""
        docs = []
        if not uploads_path.exists():
            logger.warning(f"ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨: {uploads_path}")
            return Documents(docs=docs)

        logger.info(f"å¼€å§‹æ‰«ææ–‡æ¡£ç›®å½•: {uploads_path}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {'.txt', '.md', '.csv', '.json', '.yaml', '.yml'}
        
        for file_path in uploads_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                try:
                    content = file_path.read_text(encoding='utf-8')
                    docs.append(Document(filename=file_path.name, content=content))
                    logger.info(f"æˆåŠŸè¯»å–æ–‡æ¡£: {file_path.name}")
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"æ–‡æ¡£æ‰«æå®Œæˆï¼Œå…±è¯»å– {len(docs)} ä¸ªæ–‡æ¡£")
        return Documents(docs=docs)


class ConductComprehensiveResearch(Action):
    """
    ç»¼åˆç ”ç©¶Action - æ•´åˆæœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œç ”ç©¶
    å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–ç ”ç©¶é€»è¾‘å’Œæç¤ºè¯
    """
    
    def __init__(self, search_engine: SearchEngine = None, **kwargs):
        super().__init__(**kwargs)
        self.search_engine = search_engine

    async def run(
        self, 
        topic: str,
        decomposition_nums: int = 3,  # æµ‹è¯•æ¨¡å¼: 3->2
        url_per_query: int = 3,       # æµ‹è¯•æ¨¡å¼: 3->2
        project_repo: ProjectRepo = None,
        local_docs: Documents = None
    ) -> ResearchData:
        """æ‰§è¡Œå…¨é¢çš„ç ”ç©¶ï¼Œæ•´åˆç½‘ç»œæœç´¢å’Œæœ¬åœ°æ–‡æ¡£ï¼Œæ„å»ºå‘é‡çŸ¥è¯†åº“"""
        logger.info(f"å¼€å§‹å¯¹ä¸»é¢˜ '{topic}' è¿›è¡Œå…¨é¢ç ”ç©¶ï¼ŒåŒ…æ‹¬å‘é‡åŒ–å¤„ç†...")

        # 1. ã€ä¼˜å…ˆã€‘ä½¿ç”¨ç»Ÿä¸€æ··åˆæ£€ç´¢æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“ï¼ˆå¦‚æœæä¾›æœ¬åœ°æ–‡æ¡£ï¼‰
        vector_store_path = ""
        content_chunks = []
        if local_docs and local_docs.docs:
            logger.info("æ£€æµ‹åˆ°æœ¬åœ°æ–‡æ¡£ï¼Œä½¿ç”¨ç»Ÿä¸€æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“...")
            vector_store_path, content_chunks = await self._build_project_knowledge_base_unified(
                topic, local_docs, project_repo
            )
            logger.info(f"âœ… é¡¹ç›®çŸ¥è¯†åº“æ„å»ºæˆåŠŸ: {vector_store_path}")
            logger.info("âœ… ç»Ÿä¸€æ£€ç´¢æœåŠ¡å·²å‡†å¤‡å°±ç»ªã€‚")
        
        # 2. ç½‘ç»œç ”ç©¶ (å¦‚æœæœ‰é¡¹ç›®çŸ¥è¯†åº“ï¼Œå°†ç”¨äºRAGå¢å¼º)
        online_research_content = await self._conduct_online_research(
            topic, 
            decomposition_nums, 
            url_per_query,
            project_vector_path=vector_store_path  # ä¼ é€’é¡¹ç›®çŸ¥è¯†åº“è·¯å¾„ç”¨äºRAGå¢å¼º
        )

        # 3. å°†ç½‘ç»œç ”ç©¶å†…å®¹ä¹Ÿæ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“ï¼ˆå®ç°å…±å»ºå…±äº«ï¼‰
        if online_research_content and vector_store_path:
            logger.info("ğŸ”„ å°†ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“...")
            await self._add_online_content_to_project(online_research_content, vector_store_path, topic, project_repo)
        elif online_research_content and not vector_store_path:
            # å¦‚æœæ²¡æœ‰æœ¬åœ°æ–‡æ¡£ï¼Œä¸ºç½‘ç»œå†…å®¹åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“
            logger.info("ğŸ“ ä¸ºç½‘ç»œç ”ç©¶å†…å®¹åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“...")
            vector_store_path, content_chunks = await self._build_project_knowledge_base_unified(
                topic, Documents(), project_repo, online_content=online_research_content
            )

        # 4. æ•´åˆå†…å®¹å¹¶ç”Ÿæˆç ”ç©¶ç®€æŠ¥
        combined_content = online_research_content
        if local_docs and local_docs.docs:
            local_docs_content = "\n\n--- æœ¬åœ°çŸ¥è¯†åº“ ---\n"
            for doc in local_docs.docs:
                local_docs_content += f"### æ–‡æ¡£: {doc.filename}\n{doc.content}\n\n"
            combined_content += local_docs_content

        prompt = GENERATE_RESEARCH_BRIEF_PROMPT.format(content=combined_content, topic=topic)
        brief = await self._aask(prompt, [COMPREHENSIVE_RESEARCH_BASE_SYSTEM])
        
        logger.info(f"ç ”ç©¶ç®€æŠ¥ç”Ÿæˆå®Œæ¯•ã€‚")

        # 5. è·å–æœ€ç»ˆçš„å†…å®¹å—ï¼ˆä»é¡¹ç›®çŸ¥è¯†åº“ï¼‰
        if vector_store_path:
            # ä»ç»Ÿä¸€æœåŠ¡è·å–å†…å®¹å—
            content_chunks = await self._get_content_chunks_from_project(vector_store_path)
        else:
            # å¿…é¡»æœ‰é¡¹ç›®çŸ¥è¯†åº“ï¼Œå¦åˆ™æŠ›å‡ºé”™è¯¯
            raise ValueError("é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç ”ç©¶æµç¨‹")

        # 6. åˆ›å»ºå¹¶è¿”å›ResearchData
        research_data = ResearchData(
            brief=brief, 
            vector_store_path=vector_store_path,
            content_chunks=content_chunks
        )

        if project_repo:
            docs_filename = "research_brief.md"  # ä½¿ç”¨å›ºå®šçš„æ–‡ä»¶å
            await project_repo.docs.save(filename=docs_filename, content=brief)
            brief_path = project_repo.docs.workdir / docs_filename
            logger.info(f"ç ”ç©¶ç®€æŠ¥å·²ä¿å­˜åˆ°: {brief_path}")

        return research_data

    # ========== ğŸš€ æ–°çš„ç»Ÿä¸€çŸ¥è¯†åº“ç®¡ç†æ–¹æ³• ==========
    
    async def _build_project_knowledge_base_unified(
        self, 
        topic: str, 
        local_docs: Documents, 
        project_repo=None, 
        online_content: str = ""
    ) -> tuple[str, List[str]]:
        """
        ğŸš€ ä½¿ç”¨ç»Ÿä¸€çš„æ··åˆæ£€ç´¢æœåŠ¡æ„å»ºé¡¹ç›®çŸ¥è¯†åº“
        """
        try:
            from backend.services.hybrid_search import hybrid_search
            
            # ç¡®å®šé¡¹ç›®ID
            project_id = project_repo.workdir.name if project_repo else f"research_{hash(topic) % 10000}"
            
            # åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“
            project_vector_path = hybrid_search.create_project_knowledge_base(project_id)
            
            # å‡†å¤‡è¦æ·»åŠ çš„å†…å®¹
            contents_to_add = []
            
            # æ·»åŠ æœ¬åœ°æ–‡æ¡£
            if local_docs and local_docs.docs:
                for doc in local_docs.docs:
                    contents_to_add.append({
                        "content": doc.content,
                        "filename": doc.filename
                    })
                logger.info(f"ğŸ“„ å‡†å¤‡æ·»åŠ  {len(local_docs.docs)} ä¸ªæœ¬åœ°æ–‡æ¡£")
            
            # æ·»åŠ ç½‘ç»œç ”ç©¶å†…å®¹
            if online_content:
                contents_to_add.append({
                    "content": online_content,
                    "filename": f"ç½‘ç»œç ”ç©¶_{topic}.md"
                })
                logger.info("ğŸŒ å‡†å¤‡æ·»åŠ ç½‘ç»œç ”ç©¶å†…å®¹")
            
            # æ‰¹é‡æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“
            if contents_to_add:
                success = hybrid_search.add_multiple_contents_to_project(contents_to_add, project_vector_path)
                if not success:
                    logger.warning("âš ï¸ éƒ¨åˆ†å†…å®¹æ·»åŠ å¤±è´¥")
            
            # è·å–å†…å®¹å—
            content_chunks = await self._get_content_chunks_from_project(project_vector_path)
            
            logger.info(f"âœ… ç»Ÿä¸€é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå®Œæˆ: {project_vector_path}")
            return project_vector_path, content_chunks
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€é¡¹ç›®çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")
            # ä¸é™çº§ï¼Œè®©é”™è¯¯æš´éœ²å‡ºæ¥ï¼Œå¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€æ¶æ„
            raise e
    
    async def _add_online_content_to_project(self, online_content: str, project_vector_path: str, topic: str, project_repo=None):
        """å°†ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ åˆ°ç°æœ‰é¡¹ç›®çŸ¥è¯†åº“"""
        try:
            from backend.services.hybrid_search import hybrid_search
            
            success = hybrid_search.add_content_to_project(
                content=online_content,
                filename=f"ç½‘ç»œç ”ç©¶_{topic}.md",
                project_vector_storage_path=project_vector_path
            )
            
            if success:
                logger.info("âœ… ç½‘ç»œç ”ç©¶å†…å®¹å·²æ·»åŠ åˆ°é¡¹ç›®çŸ¥è¯†åº“")
            else:
                logger.warning("âš ï¸ ç½‘ç»œç ”ç©¶å†…å®¹æ·»åŠ å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç½‘ç»œå†…å®¹å¤±è´¥: {e}")
    
    async def _get_content_chunks_from_project(self, project_vector_path: str) -> List[str]:
        """ä»é¡¹ç›®çŸ¥è¯†åº“è·å–å†…å®¹å—"""
        try:
            content_chunks = []
            if Path(project_vector_path).exists():
                for file_path in Path(project_vector_path).glob("*.txt"):
                    try:
                        chunk_content = file_path.read_text(encoding='utf-8')
                        if chunk_content.strip():
                            content_chunks.append(chunk_content.strip())
                    except Exception as e:
                        logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        
            logger.debug(f"ğŸ“Š ä»é¡¹ç›®çŸ¥è¯†åº“è·å–åˆ° {len(content_chunks)} ä¸ªå†…å®¹å—")
            return content_chunks
            
        except Exception as e:
            logger.error(f"âŒ è·å–é¡¹ç›®å†…å®¹å—å¤±è´¥: {e}")
            return []
    
    async def _conduct_online_research(self, topic: str, decomposition_nums: int, url_per_query: int, project_vector_path: str = "") -> str:
        """æ‰§è¡Œåœ¨çº¿ç ”ç©¶"""
        if not self.search_engine:
            logger.error("âŒ æœç´¢å¼•æ“æœªåˆå§‹åŒ–ï¼æ— æ³•è¿›è¡Œåœ¨çº¿ç ”ç©¶")
            raise ValueError("æœç´¢å¼•æ“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œåœ¨çº¿ç ”ç©¶ã€‚è¯·æ£€æŸ¥config/config2.yamlä¸­çš„searché…ç½®")
        
        logger.info("æ­¥éª¤ 1: ç”Ÿæˆæœç´¢å…³é”®è¯")
        keywords_prompt = RESEARCH_TOPIC_SYSTEM.format(topic=topic)
        keywords_str = await self._aask(SEARCH_KEYWORDS_PROMPT, [keywords_prompt])
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(1)
        
        try:
            keywords = OutputParser.extract_struct(keywords_str, list)
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯è§£æå¤±è´¥: {e}")
            raise ValueError(f"LLMå…³é”®è¯è§£æå¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç ”ç©¶: {e}")

        logger.info(f"å…³é”®è¯: {keywords}")

        # ä¸²è¡Œæœç´¢å…³é”®è¯ï¼Œé¿å…å¹¶å‘è¯·æ±‚è§¦å‘é¢‘ç‡é™åˆ¶
        search_results = []
        for i, kw in enumerate(keywords):
            try:
                if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(2)  # æ¯ä¸ªè¯·æ±‚é—´éš”2ç§’
                result = await self.search_engine.run(kw, as_string=False)
                search_results.append(result)
                logger.info(f"æˆåŠŸæœç´¢å…³é”®è¯: {kw}")
            except Exception as e:
                logger.error(f"æœç´¢å…³é”®è¯å¤±è´¥ {kw}: {e}")
                search_results.append([])  # æ·»åŠ ç©ºç»“æœä¿æŒç´¢å¼•ä¸€è‡´
        
        # RAGå¢å¼ºï¼šä½¿ç”¨ç»Ÿä¸€æ··åˆæ£€ç´¢æŸ¥è¯¢é¡¹ç›®çŸ¥è¯†åº“
        rag_results_str = ""
        if project_vector_path:
            try:
                from backend.services.hybrid_search import hybrid_search
                logger.info("...åŒæ—¶æŸ¥è¯¢é¡¹ç›®å‘é‡çŸ¥è¯†åº“...")
                rag_results = await hybrid_search.hybrid_search(
                    query=" ".join(keywords),
                    project_vector_storage_path=project_vector_path,
                    enable_global=True,
                    global_top_k=1,
                    project_top_k=2
                )
                if rag_results:
                    rag_results_str = "\n\n### é¡¹ç›®çŸ¥è¯†åº“ç›¸å…³ä¿¡æ¯\n" + "\n".join(rag_results)
            except Exception as e:
                logger.warning(f"é¡¹ç›®çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {e}")
        
        search_results_str = "\n".join([f"#### å…³é”®è¯: {kw}\n{res}\n" for kw, res in zip(keywords, search_results)])
        
        # å°†RAGç»“æœå’Œç½‘ç»œæœç´¢ç»“æœåˆå¹¶
        combined_search_results = search_results_str + rag_results_str

        logger.info("æ­¥éª¤ 2: åˆ†è§£ç ”ç©¶é—®é¢˜")
        decompose_prompt = DECOMPOSE_RESEARCH_PROMPT.format(
            decomposition_nums=decomposition_nums, 
            search_results=combined_search_results
        )
        queries_str = await self._aask(decompose_prompt, [keywords_prompt])
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(1)
        try:
            queries = OutputParser.extract_struct(queries_str, list)
        except Exception as e:
            logger.warning(f"é—®é¢˜åˆ†è§£å¤±è´¥: {e}, ä½¿ç”¨å…³é”®è¯ä½œä¸ºé—®é¢˜")
            queries = keywords
        
        logger.info(f"ç ”ç©¶é—®é¢˜: {queries}")

        # ä¸²è¡Œå¤„ç†æ¯ä¸ªé—®é¢˜ï¼Œé¿å…å¹¶å‘æœç´¢
        summaries = []
        for i, q in enumerate(queries):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(2)  # æ¯ä¸ªé—®é¢˜å¤„ç†é—´éš”2ç§’ï¼Œå¢åŠ å»¶è¿Ÿ
            summary = await self._search_and_summarize_query(topic, q, url_per_query)
            summaries.append(summary)

        return "\n\n".join(summaries)

    async def _search_and_summarize_query(self, topic: str, query: str, url_per_query: int) -> str:
        """æœç´¢ã€æ’åºå¹¶æ€»ç»“å•ä¸ªé—®é¢˜çš„URL"""
        logger.info(f"å¤„ç†é—®é¢˜: {query}")
        urls = await self._search_and_rank_urls(topic, query, url_per_query)
        
        if not urls:
            return f"### é—®é¢˜: {query}\n\næœªèƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚\n"

        # ä¸²è¡Œæµè§ˆå’Œåˆ†æURLï¼Œé¿å…å¹¶å‘è¯·æ±‚
        contents = []
        for i, url in enumerate(urls):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(1)  # æ¯ä¸ªURLå¤„ç†é—´éš”1ç§’
            content = await self._web_browse_and_summarize(url, query)
            contents.append(content)

        # è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å†…å®¹
        relevant_contents = [c for c in contents if "ä¸ç›¸å…³" not in c]
        
        summary = f"### é—®é¢˜: {query}\n\n" + "\n\n".join(relevant_contents)
        return summary

    async def _search_and_rank_urls(self, topic: str, query: str, num_results: int) -> List[str]:
        """æœç´¢å¹¶æ’åºURL"""
        max_results = max(num_results * 2, 6)
        try:
            results = await self.search_engine.run(query, max_results=max_results, as_string=False)
            if not results:
                logger.error(f"âŒ æœç´¢å¼•æ“æœªè¿”å›ä»»ä½•ç»“æœ: {query}")
                raise ValueError(f"æœç´¢å¼•æ“å¯¹æŸ¥è¯¢'{query}'æœªè¿”å›ä»»ä½•ç»“æœï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé…ç½®é”™è¯¯")
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥ {query}: {e}")
            raise e  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸éšè—
    
        _results_str = "\n".join(f"{i}: {res}" for i, res in enumerate(results))
        time_stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        prompt = RANK_URLS_PROMPT.format(topic=topic, query=query, results=_results_str, time_stamp=time_stamp)
        
        logger.debug(f"URLæ’åºæç¤ºè¯: {prompt}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        indices_str = await self._aask(prompt)
        
        # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        await asyncio.sleep(0.5)
        
        logger.debug(f"LLMè¿”å›çš„æ’åºç»“æœ: {indices_str}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        try:
            indices = OutputParser.extract_struct(indices_str, list)
            if not indices:
                logger.error(f"âŒ LLMè¿”å›ç©ºçš„æ’åºç´¢å¼•: {indices_str}")
                raise ValueError(f"LLM URLæ’åºå¤±è´¥ï¼Œè¿”å›ç©ºç´¢å¼•åˆ—è¡¨")
            ranked_results = [results[i] for i in indices if i < len(results)]
        except Exception as e:
            logger.error(f"âŒ URLæ’åºå¤±è´¥: {e}")
            raise e  # ä¸é™çº§ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
    
        final_urls = [res['link'] for res in ranked_results[:num_results]]
        logger.info(f"æœ€ç»ˆè·å¾— {len(final_urls)} ä¸ªURLç”¨äºæŸ¥è¯¢: {query}")
        
        return final_urls

    async def _web_browse_and_summarize(self, url: str, query: str) -> str:
        """æµè§ˆç½‘é¡µå¹¶æ€»ç»“å†…å®¹"""
        try:
            content = await WebBrowserEngine().run(url)
            prompt = WEB_CONTENT_ANALYSIS_PROMPT.format(content=content, query=query)
            summary = await self._aask(prompt)
            
            # æ·»åŠ LLMè°ƒç”¨åçš„å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
            await asyncio.sleep(1)
            return f"#### æ¥æº: {url}\n{summary}"
        except Exception as e:
            logger.error(f"æµè§ˆURLå¤±è´¥ {url}: {e}")
            return f"#### æ¥æº: {url}\n\næ— æ³•è®¿é—®æˆ–å¤„ç†æ­¤é¡µé¢ã€‚"