#!/usr/bin/env python
"""
SOPæµç¨‹éªŒè¯å™¨
ç›´æ¥åœ¨çœŸå®ç¯å¢ƒä¸­æµ‹è¯•å®Œæ•´çš„SOPæµç¨‹ï¼ŒéªŒè¯æ‰€æœ‰æ™ºèƒ½ä½“æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œ
"""
import asyncio
import time
import os
from pathlib import Path
from typing import Optional, List, Dict, Any

from backend.services.company import Company
from metagpt.environment import Environment
from metagpt.schema import Message


class SOPFlowValidator:
    """SOPæµç¨‹éªŒè¯å™¨"""
    
    def __init__(self):
        self.company = Company()
        self.project_id: Optional[str] = None
        self.validation_results = {
            "start_time": None,
            "end_time": None,
            "duration": 0,
            "agents": {
                "ProductManager": {"executed": False, "output_size": 0, "errors": []},
                "Architect": {"executed": False, "output_size": 0, "errors": []},
                "ProjectManager": {"executed": False, "output_size": 0, "errors": []},
                "WriterExpert": {"executed": False, "output_size": 0, "errors": []}
            },
            "files": {
                "research_brief": {"exists": False, "size": 0, "path": ""},
                "report_structure": {"exists": False, "size": 0, "path": ""},
                "metric_analysis": {"exists": False, "size": 0, "path": ""},
                "final_report": {"exists": False, "size": 0, "path": ""}
            },
            "overall": {
                "success": False,
                "completion_rate": 0.0,
                "file_count": 0,
                "total_errors": 0
            }
        }
    
    async def create_test_document(self) -> str:
        """åˆ›å»ºæµ‹è¯•æ–‡æ¡£"""
        test_content = """# æµ‹è¯•é¡¹ç›®ç»©æ•ˆè¯„ä»·

## é¡¹ç›®æ¦‚è¿°
æœ¬é¡¹ç›®æ—¨åœ¨é€šè¿‡æ•°æ®åˆ†æå’Œç»©æ•ˆè¯„ä¼°ï¼Œæå‡ç»„ç»‡çš„æ•´ä½“è¿è¥æ•ˆç‡ã€‚

## å…³é”®æŒ‡æ ‡
- **ç”¨æˆ·æ´»è·ƒåº¦**: æœˆæ´»è·ƒç”¨æˆ·æ•° (MAU)
- **è½¬åŒ–æ•ˆç‡**: ç”¨æˆ·è½¬åŒ–æ¼æ–—å„é˜¶æ®µè½¬åŒ–ç‡
- **æ”¶å…¥æŒ‡æ ‡**: æœˆåº¦ç»å¸¸æ€§æ”¶å…¥ (MRR)
- **å®¢æˆ·æ»¡æ„åº¦**: å‡€æ¨èå€¼ (NPS)

## æ•°æ®æº
- ç”¨æˆ·è¡Œä¸ºæ•°æ®
- è´¢åŠ¡æ•°æ®
- å®¢æˆ·åé¦ˆæ•°æ®
- å¸‚åœºç«äº‰æ•°æ®

## æœŸæœ›è¾“å‡º
- å…¨é¢çš„ç»©æ•ˆåˆ†ææŠ¥å‘Š
- å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®
- å…·ä½“çš„å…³é”®æŒ‡æ ‡è¿½è¸ªæ–¹æ¡ˆ

## æŠ€æœ¯è¦æ±‚
- ä½¿ç”¨ç°ä»£åŒ–çš„æ•°æ®åˆ†ææ–¹æ³•
- ç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œå¯é æ€§
- æä¾›æ¸…æ™°çš„å¯è§†åŒ–å›¾è¡¨
- ç»™å‡ºå…·ä½“å¯è¡Œçš„è¡ŒåŠ¨å»ºè®®"""
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        import tempfile
        temp_dir = Path(tempfile.gettempdir()) / "sop_validation"
        temp_dir.mkdir(exist_ok=True)
        
        test_file = temp_dir / "test_performance_evaluation.md"
        test_file.write_text(test_content, encoding='utf-8')
        
        print(f"ğŸ“„ åˆ›å»ºæµ‹è¯•æ–‡æ¡£: {test_file}")
        return str(test_file)
    
    def analyze_project_files(self, project_id: str):
        """åˆ†æé¡¹ç›®ç”Ÿæˆçš„æ–‡ä»¶"""
        project_path = Path("workspace") / project_id / "docs"
        
        if not project_path.exists():
            print(f"âš ï¸  é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {project_path}")
            return
        
        print(f"ğŸ“ åˆ†æé¡¹ç›®æ–‡ä»¶: {project_path}")
        
        # æŸ¥æ‰¾ä¸åŒç±»å‹çš„æ–‡ä»¶
        file_patterns = {
            "research_brief": ["research_brief", "ç ”ç©¶ç®€æŠ¥"],
            "report_structure": ["report_structure", "æŠ¥å‘Šç»“æ„", "structure"],
            "metric_analysis": ["metric", "æŒ‡æ ‡", "analysis_table"],
            "final_report": ["final_report", "æœ€ç»ˆæŠ¥å‘Š", "report.md"]
        }
        
        for file_path in project_path.glob("*.md"):
            if file_path.stat().st_size == 0:
                continue
                
            file_name_lower = file_path.name.lower()
            print(f"  ğŸ“„ {file_path.name} ({file_path.stat().st_size} bytes)")
            
            # åˆ†ç±»æ–‡ä»¶
            for file_type, patterns in file_patterns.items():
                if any(pattern in file_name_lower for pattern in patterns):
                    self.validation_results["files"][file_type] = {
                        "exists": True,
                        "size": file_path.stat().st_size,
                        "path": str(file_path)
                    }
                    break
    
    def check_console_logs(self) -> List[str]:
        """æ£€æŸ¥æ§åˆ¶å°æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦å®ç°æ—¥å¿—æ£€æŸ¥é€»è¾‘
        # ç›®å‰è¿”å›ç©ºåˆ—è¡¨
        return []
    
    async def run_validation(self, message: str, file_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡ŒSOPæµç¨‹éªŒè¯"""
        print("ğŸš€ å¼€å§‹SOPæµç¨‹éªŒè¯...")
        print(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {message}")
        
        self.validation_results["start_time"] = time.time()
        
        try:
            # è®¾ç½®ç¯å¢ƒ
            environment = Environment()
            
            # å¤„ç†æ¶ˆæ¯å’Œæ–‡ä»¶
            file_paths = [file_path] if file_path else None
            
            print("â³ æ‰§è¡ŒSOPæµç¨‹...")
            result = await self.company.process_message(
                project_id="sop_validation_test",
                message=message,
                environment=environment,
                file_paths=file_paths
            )
            
            self.project_id = "sop_validation_test"
            
            print(f"âœ… SOPæµç¨‹æ‰§è¡Œå®Œæˆ")
            print(f"ğŸ“„ ç»“æœé¢„è§ˆ: {result[:200]}..." if len(result) > 200 else f"ğŸ“„ ç»“æœ: {result}")
            
            # ç­‰å¾…ä¸€ä¸‹è®©æ–‡ä»¶ç³»ç»ŸåŒæ­¥
            await asyncio.sleep(2)
            
            # åˆ†æç»“æœ
            self.analyze_project_files(self.project_id)
            
            # æ£€æŸ¥é”™è¯¯
            errors = self.check_console_logs()
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            self.validation_results["end_time"] = time.time()
            self.validation_results["duration"] = self.validation_results["end_time"] - self.validation_results["start_time"]
            
            # æ–‡ä»¶ç»Ÿè®¡
            file_count = sum(1 for f in self.validation_results["files"].values() if f["exists"])
            total_file_size = sum(f["size"] for f in self.validation_results["files"].values())
            
            # æ™ºèƒ½ä½“æ‰§è¡Œæ£€æŸ¥ï¼ˆåŸºäºæ–‡ä»¶è¾“å‡ºæ¨æ–­ï¼‰
            if self.validation_results["files"]["research_brief"]["exists"]:
                self.validation_results["agents"]["ProductManager"]["executed"] = True
                self.validation_results["agents"]["ProductManager"]["output_size"] = self.validation_results["files"]["research_brief"]["size"]
            
            if self.validation_results["files"]["report_structure"]["exists"]:
                self.validation_results["agents"]["Architect"]["executed"] = True
                self.validation_results["agents"]["Architect"]["output_size"] = self.validation_results["files"]["report_structure"]["size"]
            
            if self.validation_results["files"]["final_report"]["exists"]:
                self.validation_results["agents"]["WriterExpert"]["executed"] = True
                self.validation_results["agents"]["WriterExpert"]["output_size"] = self.validation_results["files"]["final_report"]["size"]
            
            # æ¨æ–­ProjectManageræ‰§è¡Œï¼ˆå¦‚æœæœ‰ä»»åŠ¡ç›¸å…³çš„è¾“å‡ºï¼‰
            if file_count >= 2:  # å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œè¯´æ˜ProjectManagerå¯èƒ½ä¹Ÿæ‰§è¡Œäº†
                self.validation_results["agents"]["ProjectManager"]["executed"] = True
            
            # è®¡ç®—å®Œæˆç‡
            executed_agents = sum(1 for agent in self.validation_results["agents"].values() if agent["executed"])
            completion_rate = executed_agents / len(self.validation_results["agents"])
            
            # æ•´ä½“æˆåŠŸåˆ¤æ–­
            success = (
                completion_rate >= 0.5 and  # è‡³å°‘50%æ™ºèƒ½ä½“æ‰§è¡Œ
                file_count >= 2 and         # è‡³å°‘ç”Ÿæˆ2ä¸ªæ–‡ä»¶
                total_file_size > 1000      # æ€»æ–‡ä»¶å¤§å°è¶…è¿‡1KB
            )
            
            self.validation_results["overall"] = {
                "success": success,
                "completion_rate": completion_rate,
                "file_count": file_count,
                "total_file_size": total_file_size,
                "total_errors": len(errors)
            }
            
            return self.validation_results
            
        except Exception as e:
            print(f"âŒ SOPéªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            self.validation_results["end_time"] = time.time()
            self.validation_results["duration"] = self.validation_results["end_time"] - self.validation_results["start_time"]
            self.validation_results["overall"]["success"] = False
            
            return self.validation_results
    
    def print_validation_report(self):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š SOPæµç¨‹éªŒè¯æŠ¥å‘Š")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {self.validation_results['duration']:.1f}ç§’")
        print(f"ğŸ¯ æ€»ä½“ç»“æœ: {'âœ… æˆåŠŸ' if self.validation_results['overall']['success'] else 'âŒ å¤±è´¥'}")
        print(f"ğŸ“ˆ å®Œæˆç‡: {self.validation_results['overall']['completion_rate']*100:.1f}%")
        
        # æ™ºèƒ½ä½“æ‰§è¡Œæƒ…å†µ
        print(f"\nğŸ¤– æ™ºèƒ½ä½“æ‰§è¡Œæƒ…å†µ:")
        agent_icons = {
            "ProductManager": "ğŸ“Š",
            "Architect": "ğŸ—ï¸", 
            "ProjectManager": "ğŸ“‹",
            "WriterExpert": "âœï¸"
        }
        
        for agent_name, agent_data in self.validation_results["agents"].items():
            icon = agent_icons.get(agent_name, "ğŸ¤–")
            status = "âœ…" if agent_data["executed"] else "âŒ"
            size = agent_data["output_size"]
            print(f"  {icon} {agent_name}: {status} ({size} bytes)")
        
        # æ–‡ä»¶ç”Ÿæˆæƒ…å†µ  
        print(f"\nğŸ“ æ–‡ä»¶ç”Ÿæˆæƒ…å†µ:")
        file_icons = {
            "research_brief": "ğŸ“Š",
            "report_structure": "ğŸ—ï¸",
            "metric_analysis": "ğŸ“ˆ", 
            "final_report": "ğŸ“„"
        }
        
        for file_type, file_data in self.validation_results["files"].items():
            icon = file_icons.get(file_type, "ğŸ“„")
            status = "âœ…" if file_data["exists"] else "âŒ"
            size = file_data["size"]
            print(f"  {icon} {file_type}: {status} ({size} bytes)")
            if file_data["exists"] and file_data["path"]:
                print(f"      ğŸ“ {file_data['path']}")
        
        # ç»Ÿè®¡æ‘˜è¦
        print(f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
        print(f"  ğŸ“ ç”Ÿæˆæ–‡ä»¶: {self.validation_results['overall']['file_count']} ä¸ª")
        print(f"  ğŸ“ æ€»æ–‡ä»¶å¤§å°: {self.validation_results['overall']['total_file_size']} bytes")
        print(f"  âŒ é”™è¯¯æ•°é‡: {self.validation_results['overall']['total_errors']}")
        
        print("="*60)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª SOPæµç¨‹éªŒè¯å™¨å¯åŠ¨\n")
    
    validator = SOPFlowValidator()
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_doc_path = await validator.create_test_document()
    
    # è¿è¡ŒéªŒè¯
    test_message = "æ ¹æ®ä¸Šä¼ çš„æ–‡æ¡£å†…å®¹ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼ŒåŒæ—¶å¯ä»¥æ£€ç´¢ç½‘ç»œæ¡ˆä¾‹æ‰¾åˆ°åˆé€‚çš„é€‚é…æ­¤é¡¹ç›®çš„ç»©æ•ˆè¯„ä»·æŒ‡æ ‡ï¼Œæ¥è¾…åŠ©æ’°å†™ã€Šæµ‹è¯•é¡¹ç›®ç»©æ•ˆåˆ†ææŠ¥å‘Šã€‹"
    
    results = await validator.run_validation(
        message=test_message,
        file_path=test_doc_path
    )
    
    # æ‰“å°æŠ¥å‘Š
    validator.print_validation_report()
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    try:
        os.unlink(test_doc_path)
        print(f"ğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡æ¡£: {test_doc_path}")
    except:
        pass
    
    return results["overall"]["success"]


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        if success:
            print("\nğŸ‰ SOPæµç¨‹éªŒè¯é€šè¿‡ï¼")
            exit(0)
        else:
            print("\nğŸ’¥ SOPæµç¨‹éªŒè¯å¤±è´¥ï¼")
            exit(1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        exit(1)
    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        exit(1)