#!/usr/bin/env python
"""
æ¶æ„å¸ˆActioné›†åˆ - æŠ¥å‘Šç»“æ„è®¾è®¡å’ŒæŒ‡æ ‡åˆ†æ
é‡æ„å®ç°ä¸‰ç¯èŠ‚é€»è¾‘ï¼šåˆ†æç®€æŠ¥ -> RAGæ£€ç´¢ -> ç»¼åˆè®¾è®¡
é…ç½®é©±åŠ¨ç‰ˆæœ¬ - æ‰€æœ‰ä¸šåŠ¡é€»è¾‘é€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†
"""
import pandas as pd
import json
import re
from typing import List, Tuple, Optional
from pydantic import BaseModel, Field
from metagpt.actions import Action
from metagpt.logs import logger
from backend.actions.research_action import ResearchData
from backend.tools.json_utils import extract_json_from_llm_response
from backend.config.performance_constants import (
    ENV_ARCHITECT_BASE_SYSTEM,
    ENV_RAG_KEYWORDS_GENERATION_PROMPT,
    ENV_SECTION_PROMPT_GENERATION_TEMPLATE,
    ENV_METRICS_DESIGN_PROMPT,
    ENV_REPORT_SECTIONS,
    ENV_GET_SECTION_CONFIG,
    ENV_GET_SECTION_KEY_BY_TITLE,
    ENV_FALLBACK_KEYWORDS,
    ENV_LEVEL1_INDICATORS,
    ENV_EVIDENCE_KEYWORD_MAPPING
)

# é…ç½®é©±åŠ¨ç‰ˆæœ¬ - æ‰€æœ‰ç¡¬ç¼–ç å¸¸é‡å·²ç§»è‡³é…ç½®æ–‡ä»¶
# é€šè¿‡ ENV_* å¸¸é‡è®¿é—®é…ç½®åŒ–çš„ä¸šåŠ¡é€»è¾‘


class Section(BaseModel):
    """æŠ¥å‘Šç« èŠ‚çš„ç»“æ„åŒ–æ¨¡å‹"""
    section_title: str = Field(..., description="ç« èŠ‚æ ‡é¢˜")
    description_prompt: str = Field(..., description="æŒ‡å¯¼æœ¬ç« èŠ‚å†™ä½œçš„æ ¸å¿ƒè¦ç‚¹æˆ–é—®é¢˜")


class ReportStructure(BaseModel):
    """æŠ¥å‘Šæ•´ä½“æ¶æ„çš„ç»“æ„åŒ–æ¨¡å‹"""
    title: str = Field(..., description="æŠ¥å‘Šä¸»æ ‡é¢˜")
    sections: List[Section] = Field(..., description="æŠ¥å‘Šçš„ç« èŠ‚åˆ—è¡¨")


class MetricAnalysisTable(BaseModel):
    """æŒ‡æ ‡åˆ†æè¡¨çš„ç»“æ„åŒ–æ¨¡å‹"""
    data_json: str = Field(..., description="å­˜å‚¨æŒ‡æ ‡åˆ†æç»“æœçš„DataFrame (JSONæ ¼å¼)")


class ArchitectOutput(BaseModel):
    """Architectè¾“å‡ºçš„å¤åˆæ•°æ®ç»“æ„"""
    report_structure: ReportStructure = Field(..., description="æŠ¥å‘Šç»“æ„è®¾è®¡")
    metric_analysis_table: MetricAnalysisTable = Field(..., description="æŒ‡æ ‡åˆ†æè¡¨")


class DesignReportStructure(Action):
    """
    è®¾è®¡æŠ¥å‘Šç»“æ„Action - Architectçš„æ ¸å¿ƒèƒ½åŠ›
    å®ç°ä¸‰ç¯èŠ‚é€»è¾‘ï¼šåˆ†æç®€æŠ¥ -> RAGæ£€ç´¢ -> ç»¼åˆè®¾è®¡
    """
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._research_data: Optional[ResearchData] = None
        # ğŸ¯ ä»app2.pyä¼ å…¥çš„é¡¹ç›®é…ç½®ä¸­è·å–é¡¹ç›®ä¿¡æ¯
        self._project_info: Optional[dict] = None
    
    def set_project_info(self, project_info: dict):
        """è®¾ç½®é¡¹ç›®ä¿¡æ¯ï¼ˆä»é¡¹ç›®é…ç½®æ–‡ä»¶è·å–ï¼‰"""
        self._project_info = project_info
    
    async def run(self, enhanced_research_context: str, research_data: Optional[ResearchData] = None, project_info: dict = None) -> Tuple[ReportStructure, MetricAnalysisTable]:
        """
        ğŸ¯ é…ç½®é©±åŠ¨çš„æŠ¥å‘Šç»“æ„è®¾è®¡ - ç›´æ¥ä½¿ç”¨é¡¹ç›®é…ç½®ï¼Œæ— éœ€LLMæå–
        
        Args:
            enhanced_research_context: å¯èƒ½å·²ç»ç»è¿‡RAGå¢å¼ºçš„ç ”ç©¶ä¸Šä¸‹æ–‡
            research_data: ProductManageræä¾›çš„ç ”ç©¶æ•°æ®ï¼ˆåŒ…å«å‘é‡çŸ¥è¯†åº“ï¼‰
            project_info: ä»é¡¹ç›®é…ç½®æ–‡ä»¶è·å–çš„é¡¹ç›®ä¿¡æ¯
        """
        logger.info("ğŸ—ï¸ å¼€å§‹åŸºäºæ ‡å‡†æ¨¡æ¿çš„æŠ¥å‘Šç»“æ„è®¾è®¡...")
        self._research_data = research_data
        
        # ğŸ¯ ç›´æ¥ä½¿ç”¨é…ç½®åŒ–çš„é¡¹ç›®ä¿¡æ¯ï¼Œæ— éœ€LLMæå–
        if project_info:
            self._project_info = project_info
            logger.info(f"ğŸ“‹ ä½¿ç”¨é…ç½®åŒ–é¡¹ç›®ä¿¡æ¯: {project_info['project_name']}")
        else:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥é¡¹ç›®ä¿¡æ¯ï¼Œå°è¯•ä»ç±»å±æ€§è·å–
            if not self._project_info:
                raise ValueError("é¡¹ç›®ä¿¡æ¯æœªæä¾›ï¼Œæ— æ³•è¿›è¡Œæ¶æ„è®¾è®¡ã€‚è¯·ç¡®ä¿é€šè¿‡é¡¹ç›®é…ç½®æ–‡ä»¶æä¾›é¡¹ç›®ä¿¡æ¯")
            project_info = self._project_info
        
        # æ­¥éª¤ä¸€ï¼šRAGå¢å¼º - æŸ¥è¯¢è¯¦ç»†èµ„æ–™ä¸°å¯Œé¡¹ç›®ä¿¡æ¯
        logger.info("ğŸ” æ­¥éª¤ä¸€ï¼šRAGæ£€ç´¢å¢å¼ºé¡¹ç›®ä¿¡æ¯...")
        enriched_info = await self._enrich_with_rag(project_info)
        
        # æ­¥éª¤äºŒï¼šæ ‡å‡†ç»“æ„å®šåˆ¶ - åŸºäºå›ºå®šæ¨¡æ¿ç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹
        logger.info("ğŸ—ï¸ æ­¥éª¤äºŒï¼šåŸºäºæ ‡å‡†æ¨¡æ¿ç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹...")
        report_structure, metric_table = await self._generate_customized_template(enriched_info)
        
        logger.info(f"âœ… æŠ¥å‘Šè“å›¾è®¾è®¡å®Œæˆ: {report_structure.title}")
        logger.info(f"ğŸ“Š æŒ‡æ ‡ä½“ç³»: {len(json.loads(metric_table.data_json))} ä¸ªæŒ‡æ ‡")
        
        return report_structure, metric_table
    
    # ğŸ¯ ç§»é™¤LLMé¡¹ç›®ä¿¡æ¯æå–é€»è¾‘ - ç›´æ¥ä½¿ç”¨é…ç½®åŒ–é¡¹ç›®ä¿¡æ¯
    
    async def _enrich_with_rag(self, project_info: dict) -> dict:
        """
        æ­¥éª¤äºŒï¼šé€šè¿‡RAGæ£€ç´¢ä¸°å¯Œé¡¹ç›®ä¿¡æ¯ - åŠ¨æ€ç”Ÿæˆæ£€ç´¢å…³é”®è¯
        """
        if not self._research_data or not getattr(self._research_data, 'vector_store_path', ''):
            logger.warning("âš ï¸ æœªæä¾›é¡¹ç›®å‘é‡åº“è·¯å¾„ï¼Œè·³è¿‡RAGå¢å¼ºï¼Œç›´æ¥ä½¿ç”¨ç ”ç©¶ç®€æŠ¥è¿›è¡Œç»“æ„è®¾è®¡ã€‚")
            # ç›´æ¥è¿”å›åŸå§‹é¡¹ç›®ä¿¡æ¯ä½œä¸ºå¯ŒåŒ–ç»“æœçš„åŸºç¡€
            enriched_info = project_info.copy()
            enriched_info["rag_evidence"] = {}
            return enriched_info
        
        # åŠ¨æ€ç”Ÿæˆæ£€ç´¢å…³é”®è¯
        search_keywords = await self._generate_rag_search_keywords(project_info)
        
        enriched_info = project_info.copy()
        enriched_info["rag_evidence"] = {}
        
        logger.info(f"ğŸ” å¼€å§‹å¯¹ {len(search_keywords)} ä¸ªåŠ¨æ€å…³é”®è¯è¿›è¡ŒRAGæ£€ç´¢...")
        
        # é€ä¸ªç±»åˆ«æ£€ç´¢
        for keyword_group in search_keywords:
            category = keyword_group["category"]
            keywords = keyword_group["keywords"]
            
            category_evidence = []
            for keyword in keywords:
                try:
                    relevant_chunks = await self._search_chunks(keyword)
                    if relevant_chunks:
                        category_evidence.extend(relevant_chunks[:2])
                except Exception as e:
                    logger.warning(f"å…³é”®è¯ '{keyword}' æ£€ç´¢å¤±è´¥: {e}")
            
            if category_evidence:
                enriched_info["rag_evidence"][category] = category_evidence
                logger.debug(f"ğŸ“‹ {category}: æ£€ç´¢åˆ° {len(category_evidence)} æ¡ç›¸å…³è¯æ®")
        
        # æœ€åæ¸…ç†é‡å¤å†…å®¹å¹¶é™åˆ¶æ•°é‡
        for category in enriched_info["rag_evidence"]:
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_chunks = list(dict.fromkeys(enriched_info["rag_evidence"][category]))
            enriched_info["rag_evidence"][category] = unique_chunks[:6]  # æ¯ä¸ªç±»åˆ«æœ€å¤š6æ¡
            logger.debug(f"ğŸ“‹ {category}: æœ€ç»ˆæ£€ç´¢åˆ° {len(enriched_info['rag_evidence'][category])} æ¡ç›¸å…³è¯æ®")
        
        logger.info(f"ğŸ“‹ RAGæ£€ç´¢å®Œæˆï¼Œä¸°å¯Œäº† {len(enriched_info['rag_evidence'])} ä¸ªä¿¡æ¯ç±»åˆ«")
        return enriched_info
    
    async def _generate_rag_search_keywords(self, project_info: dict) -> List[dict]:
        """
        åŠ¨æ€ç”ŸæˆRAGæ£€ç´¢å…³é”®è¯ï¼ˆç±»ä¼¼PMçš„å…³é”®è¯ç”Ÿæˆé€»è¾‘ï¼‰
        """
        project_name = project_info['project_name']
        
        keyword_generation_prompt = ENV_RAG_KEYWORDS_GENERATION_PROMPT.format(
            project_info=json.dumps(project_info, ensure_ascii=False, indent=2),
            project_name=project_name
        )
        
        try:
            keywords_result = await self._aask(keyword_generation_prompt)
            
            # ä½¿ç”¨é€šç”¨JSONæå–å·¥å…·
            search_keywords = extract_json_from_llm_response(keywords_result)
            
            logger.info(f"ğŸ” åŠ¨æ€ç”Ÿæˆäº† {len(search_keywords)} ä¸ªå…³é”®è¯ç»„")
            return search_keywords
        except Exception as e:
            logger.warning(f"åŠ¨æ€å…³é”®è¯ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é…ç½®åŒ–å¤‡ç”¨å…³é”®è¯: {e}")
            # ğŸ¯ ä½¿ç”¨é…ç½®åŒ–çš„å¤‡ç”¨å…³é”®è¯
            return ENV_FALLBACK_KEYWORDS
    
    async def _search_chunks(self, query: str) -> List[str]:
        """
        ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢æœåŠ¡è¿›è¡Œå¢å¼ºæ£€ç´¢
        """
        try:
            from backend.services.intelligent_search import intelligent_search
            
            # ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢æœåŠ¡
            if self._research_data and hasattr(self._research_data, 'vector_store_path'):
                search_result = await intelligent_search.intelligent_search(
                    query=query,
                    project_vector_storage_path=self._research_data.vector_store_path,
                    mode="hybrid",  # ä½¿ç”¨æ··åˆæ™ºèƒ½æ£€ç´¢ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•
                    enable_global=True,
                    max_results=5
                )
                
                results = search_result.get("results", [])
                
                # ğŸ§  æ·»åŠ æ™ºèƒ½åˆ†ææ´å¯Ÿåˆ°ç»“æœä¸­
                if search_result.get("insights"):
                    insights_text = "\nğŸ’¡ æ™ºèƒ½åˆ†ææ´å¯Ÿ:\n" + "\n".join(search_result["insights"])
                    if results:
                        results[0] = results[0] + insights_text
                    else:
                        results = [insights_text]
                
                logger.debug(f"ğŸ§  æ™ºèƒ½æ£€ç´¢å®Œæˆï¼ŒæŸ¥è¯¢: '{query}'ï¼Œæ¨¡å¼: {search_result.get('mode_used', 'unknown')}ï¼Œæ‰¾åˆ° {len(results)} æ¡ç›¸å…³å†…å®¹")
                return results
                    
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def _generate_customized_template(self, enriched_info: dict) -> Tuple[ReportStructure, MetricAnalysisTable]:
        """
        æ­¥éª¤ä¸‰ï¼šåŸºäºæ ‡å‡†ç»©æ•ˆè¯„ä»·æ¨¡æ¿ç”Ÿæˆå®šåˆ¶åŒ–å†…å®¹
        """
        # ğŸ¯ ä½¿ç”¨é…ç½®åŒ–çš„æ ‡å‡†ç»©æ•ˆè¯„ä»·æŠ¥å‘Šç»“æ„
        standard_sections = ENV_REPORT_SECTIONS
        
        # åŸºäºé¡¹ç›®ä¿¡æ¯å®šåˆ¶å†…å®¹æè¿°
        customized_sections = await self._customize_section_content(standard_sections, enriched_info)
        
        # ç”Ÿæˆæ ‡å‡†æŒ‡æ ‡ä½“ç³»
        metric_table = await self._generate_standard_metrics(enriched_info)
        
        # æ„é€ ReportStructure
        sections = []
        for section_data in customized_sections:
            section = Section(
                section_title=section_data["title"],
                description_prompt=section_data["description_prompt"]
            )
            sections.append(section)
        
        project_name = enriched_info['project_name']
        report_structure = ReportStructure(
            title=f"{project_name}ç»©æ•ˆè¯„ä»·æŠ¥å‘Š",
            sections=sections
        )
        
        return report_structure, metric_table
    
    async def _customize_section_content(self, standard_sections: List[dict], enriched_info: dict) -> List[dict]:
        """
        ğŸ¯ é…ç½®é©±åŠ¨çš„ç« èŠ‚å†…å®¹å®šåˆ¶ - ç§»é™¤ç¡¬ç¼–ç çš„æŒ‡æ ‡å…³è”
        """
        customized_sections = []
        
        for section in standard_sections:
            # åŸºäºé¡¹ç›®ä¿¡æ¯è°ƒæ•´prompt
            customized_prompt = await self._generate_section_prompt(section, enriched_info)
            
            customized_section = {
                "title": section["title"],
                "description_prompt": customized_prompt
            }
                
            customized_sections.append(customized_section)
        
        return customized_sections
    
    async def _generate_section_prompt(self, section: dict, enriched_info: dict) -> str:
        """
        ğŸ¯ é…ç½®é©±åŠ¨çš„ç« èŠ‚å†™ä½œæŒ‡å¯¼ç”Ÿæˆ - ä½¿ç”¨é…ç½®åŒ–é¡¹ç›®ä¿¡æ¯
        """
        base_prompt = section["prompt_template"]
        # ğŸ¯ ä½¿ç”¨é…ç½®åŒ–çš„é¡¹ç›®åç§°
        project_name = enriched_info['project_name']
        section_title = section["title"]
        
        # æ ¹æ®ç« èŠ‚ç‰¹ç‚¹ç”Ÿæˆå…·ä½“çš„RAGæ£€ç´¢æŒ‡å¯¼
        rag_instructions = await self._generate_chapter_rag_instructions(section_title, enriched_info)
        
        customized_prompt = ENV_SECTION_PROMPT_GENERATION_TEMPLATE.format(
            project_name=project_name,
            base_prompt=base_prompt,
            rag_instructions=rag_instructions
        )
        
        return customized_prompt
    
    async def _generate_chapter_rag_instructions(self, section_title: str, enriched_info: dict) -> str:
        """
        ğŸ¯ é…ç½®é©±åŠ¨çš„ç« èŠ‚RAGæŒ‡å¯¼ç”Ÿæˆ - æ›¿ä»£ç¡¬ç¼–ç æ¡ä»¶åˆ¤æ–­
        """
        rag_evidence = enriched_info.get("rag_evidence", {})
        
        # ğŸ¯ æ ¹æ®ç« èŠ‚æ ‡é¢˜ç¡®å®šå¯¹åº”çš„é…ç½®key
        section_key = self._get_section_key_from_title(section_title)
        
        # ğŸ¯ ä»é…ç½®è·å–ç« èŠ‚ç‰¹å®šçš„RAGæŒ‡å¯¼
        section_config = ENV_GET_SECTION_CONFIG(section_key)
        
        if section_config and 'rag_instructions' in section_config:
            # ä½¿ç”¨é…ç½®ä¸­çš„æŒ‡å¯¼å†…å®¹
            instructions = section_config['rag_instructions']
            
            # åŠ¨æ€æ›¿æ¢è¯æ®æ‘˜è¦å ä½ç¬¦
            for category in section_config.get('keywords', []):
                placeholder = f"{{evidence_summary_{category}}}"
                if placeholder in instructions:
                    evidence_summary = self._get_evidence_summary(rag_evidence, category)
                    instructions = instructions.replace(placeholder, evidence_summary)
        else:
            # é€šç”¨æŒ‡å¯¼ä½œä¸ºå¤‡ç”¨ï¼šæ”¹ä¸ºä»é…ç½®generalç« èŠ‚è¯»å–
            general_cfg = ENV_GET_SECTION_CONFIG('general') or {}
            instructions = general_cfg.get('rag_instructions', '')
        
        return instructions

    def _get_section_key_from_title(self, section_title: str) -> str:
        """
        ğŸ¯ é…ç½®é©±åŠ¨çš„ç« èŠ‚æ ‡é¢˜æ˜ å°„ - å®Œå…¨æ¶ˆé™¤ç¡¬ç¼–ç 
        """
        # ğŸ¯ ä½¿ç”¨é…ç½®ç®¡ç†ç±»çš„è‡ªåŠ¨åŒ¹é…æ–¹æ³•
        return ENV_GET_SECTION_KEY_BY_TITLE(section_title)
    
    def _get_evidence_summary(self, rag_evidence: dict, category: str) -> str:
        """
        ğŸ¯ é…ç½®é©±åŠ¨çš„è¯æ®æ‘˜è¦ç”Ÿæˆ - æ¶ˆé™¤ç¡¬ç¼–ç å…³é”®è¯åˆ¤æ–­
        """
        if category in rag_evidence and rag_evidence[category]:
            # ä»è¯æ®ä¸­æå–å…³é”®è¯ä½œä¸ºæ£€ç´¢æŒ‡å¯¼
            evidence_text = " ".join(rag_evidence[category][:2])  # å–å‰2æ¡è¯æ®
            
            # ğŸ¯ ä½¿ç”¨é…ç½®åŒ–çš„å…³é”®è¯æ˜ å°„ï¼Œæ¶ˆé™¤ç¡¬ç¼–ç 
            keywords = []
            for summary_type, mapping in ENV_EVIDENCE_KEYWORD_MAPPING.items():
                type_keywords = mapping.get('keywords', [])
                # æ£€æŸ¥è¯æ®æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«è¯¥ç±»å‹çš„å…³é”®è¯
                if any(keyword in evidence_text for keyword in type_keywords):
                    keywords.append(summary_type)
            
            return ", ".join(keywords) if keywords else "ç›¸å…³é¡¹ç›®ä¿¡æ¯"
        return "é¡¹ç›®ç›¸å…³ä¿¡æ¯ï¼ˆå¾…æ£€ç´¢ï¼‰"
    
    async def _generate_standard_metrics(self, enriched_info: dict) -> MetricAnalysisTable:
        """
        åŸºäºé¡¹ç›®ç‰¹ç‚¹åŠ¨æ€ç”Ÿæˆç»©æ•ˆæŒ‡æ ‡ä½“ç³»
        ä¸€çº§æŒ‡æ ‡å›ºå®šä¸ºï¼šå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Š
        äºŒçº§ã€ä¸‰çº§æŒ‡æ ‡æ ¹æ®é¡¹ç›®ç‰¹ç‚¹ç”±LLMåŠ¨æ€ç”Ÿæˆ
        """
        
        # æ„é€ æŒ‡æ ‡è®¾è®¡prompt
        metrics_design_prompt = ENV_METRICS_DESIGN_PROMPT.format(
            project_info=json.dumps(enriched_info, ensure_ascii=False, indent=2),
            project_name=enriched_info['project_name'],
            project_type=enriched_info['project_type']
        )
        
        try:
            metrics_result = await self._aask(metrics_design_prompt, [ENV_ARCHITECT_BASE_SYSTEM])

            # ä»LLMå›å¤ä¸­æå–JSONå†…å®¹ï¼ˆé€šç”¨å·¥å…·ï¼‰
            raw = extract_json_from_llm_response(metrics_result)

            # ç»Ÿä¸€ä¾èµ–å·¥å…·è¿”å›ï¼›æ­¤å¤„ä»…ä½œæœ€ç»ˆå…œåº•
            if isinstance(raw, list):
                metrics_data = raw
            elif isinstance(raw, dict):
                metrics_data = [raw]
            else:
                raise ValueError("LLMè¿”å›çš„æŒ‡æ ‡æ•°æ®ä¸æ˜¯å¯è§£æçš„åˆ—è¡¨/å¯¹è±¡")

            # ä»…ä¿ç•™å­—å…¸é¡¹ï¼Œé¿å…å­—ç¬¦ä¸²ç­‰å¼‚å¸¸å…ƒç´ å¯¼è‡´åç»­å¤„ç†æŠ¥é”™
            metrics_data = [m for m in metrics_data if isinstance(m, dict)]

            # éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œä¸€çº§æŒ‡æ ‡åˆ†å¸ƒ
            validated_metrics = self._validate_metrics_structure(metrics_data)
            
            logger.info(f"ğŸ“Š åŠ¨æ€ç”Ÿæˆäº† {len(validated_metrics)} ä¸ªç»©æ•ˆæŒ‡æ ‡")
            logger.info(f"ğŸ“Š æŒ‡æ ‡åˆ†å¸ƒ - å†³ç­–:{self._count_metrics_by_level1(validated_metrics, 'å†³ç­–')}ä¸ª, "
                       f"è¿‡ç¨‹:{self._count_metrics_by_level1(validated_metrics, 'è¿‡ç¨‹')}ä¸ª, "
                       f"äº§å‡º:{self._count_metrics_by_level1(validated_metrics, 'äº§å‡º')}ä¸ª, "
                       f"æ•ˆç›Š:{self._count_metrics_by_level1(validated_metrics, 'æ•ˆç›Š')}ä¸ª")
            
            return MetricAnalysisTable(data_json=json.dumps(validated_metrics, ensure_ascii=False))
            
        except Exception as e:
            logger.error(f"LLMæŒ‡æ ‡ç”Ÿæˆå¤±è´¥ï¼Œæ¡ä»¶ä¸è¶³æ— æ³•æ„å»ºæŒ‡æ ‡ä½“ç³»: {e}")
            # ä¸ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼Œç›´æ¥è¿”å›ç©ºæŒ‡æ ‡è¡¨ç¤ºæ— æ³•æ„å»º
            empty_metrics = {
                "error": "æ¡ä»¶ä¸è¶³ï¼Œæ— æ³•æ„å»ºæŒ‡æ ‡ä½“ç³»",
                "reason": str(e),
                "suggestion": "è¯·ç¡®ä¿é¡¹ç›®ä¿¡æ¯å®Œæ•´åé‡æ–°ç”Ÿæˆ"
            }
            return MetricAnalysisTable(data_json=json.dumps(empty_metrics, ensure_ascii=False))
    
    def _validate_metrics_structure(self, metrics_data: List[dict]) -> List[dict]:
        """
        éªŒè¯æŒ‡æ ‡æ•°æ®ç»“æ„çš„å®Œæ•´æ€§
        """
        validated_metrics = []
        # ğŸ”§ ä¿®å¤ï¼šæ”¯æŒå¤šç§å­—æ®µåæ ¼å¼ï¼Œå…¼å®¹æ–°çš„æŒ‡æ ‡ç»“æ„
        required_fields = ['metric_id', 'name', 'category', 'ä¸€çº§æŒ‡æ ‡', 'äºŒçº§æŒ‡æ ‡', 'ä¸‰çº§æŒ‡æ ‡', 'åˆ†å€¼']
        # å¯é€‰å­—æ®µï¼ˆè¯´æ˜æ€§æ³¨é‡Šï¼Œå·²åœ¨æ ¡éªŒé€»è¾‘ä¸­å†…è”å¤„ç†å¤šç§å‘½åï¼Œä¸å†å•ç‹¬ä½¿ç”¨åˆ—è¡¨ï¼‰
        
        for metric in metrics_data:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if all(field in metric for field in required_fields):
                # ğŸ¯ ä½¿ç”¨é…ç½®åŒ–çš„ä¸€çº§æŒ‡æ ‡éªŒè¯ï¼Œæ¶ˆé™¤ç¡¬ç¼–ç 
                if metric['ä¸€çº§æŒ‡æ ‡'] in ENV_LEVEL1_INDICATORS:
                    # ğŸ”§ æ ‡å‡†åŒ–å­—æ®µåï¼Œç¡®ä¿å…¼å®¹æ€§
                    standardized_metric = metric.copy()
                    
                    # å¤„ç†è¯„åˆ†æ–¹æ³•å­—æ®µçš„å¤šç§æ ¼å¼
                    if 'scoring_method' not in standardized_metric:
                        if 'è¯„åˆ†æ–¹æ³•' in standardized_metric:
                            standardized_metric['scoring_method'] = standardized_metric['è¯„åˆ†æ–¹æ³•']
                        elif 'è¯„åˆ†è§„åˆ™' in standardized_metric:
                            standardized_metric['scoring_method'] = standardized_metric['è¯„åˆ†è§„åˆ™']
                    
                    # ç¡®ä¿æœ‰è¯„åˆ†è¿‡ç¨‹å­—æ®µ
                    if 'è¯„åˆ†è¿‡ç¨‹' not in standardized_metric:
                        if 'evaluation_process' in standardized_metric:
                            standardized_metric['è¯„åˆ†è¿‡ç¨‹'] = standardized_metric['evaluation_process']
                        else:
                            standardized_metric['è¯„åˆ†è¿‡ç¨‹'] = f"å¯¹æŒ‡æ ‡'{metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')}'è¿›è¡Œä¸“ä¸šè¯„ä»·"
                    
                    validated_metrics.append(standardized_metric)
                    logger.debug(f"âœ… éªŒè¯é€šè¿‡æŒ‡æ ‡: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')}")
                else:
                    logger.warning(f"æŒ‡æ ‡ {metric.get('name', 'æœªçŸ¥')} çš„ä¸€çº§æŒ‡æ ‡ä¸ç¬¦åˆè¦æ±‚: {metric.get('ä¸€çº§æŒ‡æ ‡', '')}")
            else:
                missing_fields = [field for field in required_fields if field not in metric]
                logger.warning(f"æŒ‡æ ‡æ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå¤±å­—æ®µ {missing_fields}: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')}")
        
        return validated_metrics
    
    def _count_metrics_by_level1(self, metrics: List[dict], level1: str) -> int:
        """
        ç»Ÿè®¡æŒ‡å®šä¸€çº§æŒ‡æ ‡ä¸‹çš„æŒ‡æ ‡æ•°é‡
        """
        return len([m for m in metrics if m.get('ä¸€çº§æŒ‡æ ‡') == level1])
    
