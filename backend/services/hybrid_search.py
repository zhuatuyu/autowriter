"""
æ··åˆæ£€ç´¢æœåŠ¡
åŒæ—¶æ£€ç´¢å…¨å±€çŸ¥è¯†åº“å’Œé¡¹ç›®çŸ¥è¯†åº“ï¼Œåˆå¹¶ç»“æœ
"""

import asyncio
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple
from metagpt.logs import logger
from metagpt.rag.engines.simple import SimpleEngine
from metagpt.rag.factories.embedding import get_rag_embedding
from metagpt.rag.schema import FAISSRetrieverConfig
from metagpt.config2 import Config
from llama_index.llms.openai import OpenAI as LlamaOpenAI

from .global_knowledge import global_knowledge


class HybridSearchService:
    """æ··åˆæ£€ç´¢æœåŠ¡ï¼šå…¨å±€çŸ¥è¯†åº“ + é¡¹ç›®çŸ¥è¯†åº“"""
    
    def __init__(self):
        self._project_engines = {}  # ç¼“å­˜é¡¹ç›®å¼•æ“
        self._config = None
    
    def _get_config(self) -> Config:
        """è·å–é…ç½®"""
        if self._config is None:
            self._config = Config.from_yaml_file(Path('config/config2.yaml'))
        return self._config
    
    def _create_llm_and_embed_model(self):
        """åˆ›å»ºLLMå’ŒåµŒå…¥æ¨¡å‹"""
        config = self._get_config()
        llm_config = config.llm
        
        llm = LlamaOpenAI(
            api_key=llm_config.api_key,
            base_url=llm_config.base_url,
            model="gpt-3.5-turbo"
        )
        
        embed_model = get_rag_embedding(config=config)
        embed_model.embed_batch_size = 8
        
        return llm, embed_model
    
    def _get_project_vector_index_path(self, project_vector_storage_path: str) -> str:
        """è·å–é¡¹ç›®å‘é‡ç´¢å¼•è·¯å¾„"""
        project_dir = Path(project_vector_storage_path).parent
        return str(project_dir / "vector_index")
    
    def _is_project_index_exists(self, index_path: str) -> bool:
        """æ£€æŸ¥é¡¹ç›®ç´¢å¼•æ˜¯å¦å­˜åœ¨"""
        index_path = Path(index_path)
        # ä½¿ç”¨æ­£ç¡®çš„FAISSç´¢å¼•æ–‡ä»¶å
        index_files = ['default__vector_store.json', 'docstore.json', 'index_store.json']
        return all((index_path / f).exists() for f in index_files)
    
    async def _build_project_index(self, project_vector_storage_path: str) -> bool:
        """æ„å»ºé¡¹ç›®å‘é‡ç´¢å¼•"""
        try:
            # æ”¶é›†é¡¹ç›®æ–‡æ¡£
            project_files = []
            if os.path.isdir(project_vector_storage_path):
                project_files = [
                    os.path.join(project_vector_storage_path, f) 
                    for f in os.listdir(project_vector_storage_path) 
                    if f.endswith(('.md', '.txt'))
                ]
            
            if not project_files:
                logger.warning(f"âš ï¸ é¡¹ç›®çŸ¥è¯†åº“ä¸ºç©º: {project_vector_storage_path}")
                return False
            
            logger.info(f"ğŸ”§ æ„å»ºé¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•: {len(project_files)} ä¸ªæ–‡ä»¶")
            llm, embed_model = self._create_llm_and_embed_model()
            
            # ğŸš€ ä½¿ç”¨FAISS Retrieveræ”¯æŒæŒä¹…åŒ–
            from metagpt.rag.schema import FAISSRetrieverConfig
            
            # æ„å»ºå¼•æ“æ—¶å°±æŒ‡å®šæ”¯æŒæŒä¹…åŒ–çš„é…ç½®
            engine = SimpleEngine.from_docs(
                input_files=project_files,
                llm=llm,
                embed_model=embed_model,
                retriever_configs=[FAISSRetrieverConfig(dimensions=1024)]  # ä½¿ç”¨FAISSæ”¯æŒæŒä¹…åŒ–
            )
            
            index_path = self._get_project_vector_index_path(project_vector_storage_path)
            Path(index_path).mkdir(parents=True, exist_ok=True)
            
            # æŒä¹…åŒ–ç´¢å¼•
            engine.persist(index_path)
            
            logger.info(f"âœ… é¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºé¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    async def _get_project_engine(self, project_vector_storage_path: str) -> SimpleEngine:
        """è·å–æˆ–åˆ›å»ºé¡¹ç›®çŸ¥è¯†åº“å¼•æ“"""
        cache_key = project_vector_storage_path
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._project_engines:
            return self._project_engines[cache_key]
        
        try:
            index_path = self._get_project_vector_index_path(project_vector_storage_path)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æ„å»º
            if not self._is_project_index_exists(index_path):
                logger.info("ğŸ“ é¡¹ç›®ç´¢å¼•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ„å»º...")
                if not await self._build_project_index(project_vector_storage_path):
                    raise Exception("æ„å»ºé¡¹ç›®ç´¢å¼•å¤±è´¥")
            
            # ä»ç´¢å¼•åŠ è½½å¼•æ“
            logger.info(f"ğŸ“– åŠ è½½é¡¹ç›®çŸ¥è¯†åº“ç´¢å¼•: {index_path}")
            llm, embed_model = self._create_llm_and_embed_model()
            
            from metagpt.rag.schema import FAISSIndexConfig
            config = FAISSIndexConfig(
                persist_path=index_path,
                embed_model=embed_model
            )
            engine = SimpleEngine.from_index(
                index_config=config,
                embed_model=embed_model,
                llm=llm,
                retriever_configs=[FAISSRetrieverConfig(dimensions=1024)]
            )
            
            # ç¼“å­˜å¼•æ“
            self._project_engines[cache_key] = engine
            logger.info("âœ… é¡¹ç›®çŸ¥è¯†åº“å¼•æ“åŠ è½½æˆåŠŸ")
            return engine
            
        except Exception as e:
            logger.error(f"âŒ è·å–é¡¹ç›®çŸ¥è¯†åº“å¼•æ“å¤±è´¥: {e}")
            raise
    
    async def _search_project_knowledge(self, query: str, project_vector_storage_path: str, top_k: int = 3) -> List[str]:
        """æœç´¢é¡¹ç›®çŸ¥è¯†åº“"""
        try:
            engine = await self._get_project_engine(project_vector_storage_path)
            results = await engine.aretrieve(query)
            return [result.text.strip() for result in results[:top_k]]
        except Exception as e:
            logger.error(f"âŒ é¡¹ç›®çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}")
            return []
    
    def _merge_search_results(
        self, 
        global_results: List[str], 
        project_results: List[str],
        global_weight: float = 0.3,
        project_weight: float = 0.7
    ) -> List[str]:
        """åˆå¹¶æ£€ç´¢ç»“æœï¼Œé¡¹ç›®çŸ¥è¯†åº“æƒé‡æ›´é«˜"""
        
        merged_results = []
        
        # å…ˆæ·»åŠ é¡¹ç›®ç»“æœï¼ˆæƒé‡æ›´é«˜ï¼‰
        for result in project_results:
            if result and result.strip():
                merged_results.append(f"ğŸ“ [é¡¹ç›®çŸ¥è¯†] {result.strip()}")
        
        # å†æ·»åŠ å…¨å±€ç»“æœ
        for result in global_results:
            if result and result.strip():
                merged_results.append(f"ğŸŒ [å…¨å±€çŸ¥è¯†] {result.strip()}")
        
        # å»é‡å¹¶é™åˆ¶æ€»æ•°
        seen = set()
        unique_results = []
        for result in merged_results:
            if result not in seen:
                seen.add(result)
                unique_results.append(result)
        
        return unique_results[:6]  # é™åˆ¶æ€»ç»“æœæ•°
    
    async def hybrid_search(
        self, 
        query: str, 
        project_vector_storage_path: str,
        enable_global: bool = True,
        global_top_k: int = 2,
        project_top_k: int = 4
    ) -> List[str]:
        """
        æ··åˆæ£€ç´¢ï¼šåŒæ—¶æœç´¢å…¨å±€çŸ¥è¯†åº“å’Œé¡¹ç›®çŸ¥è¯†åº“
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            project_vector_storage_path: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
            enable_global: æ˜¯å¦å¯ç”¨å…¨å±€çŸ¥è¯†åº“æœç´¢
            global_top_k: å…¨å±€çŸ¥è¯†åº“è¿”å›ç»“æœæ•°
            project_top_k: é¡¹ç›®çŸ¥è¯†åº“è¿”å›ç»“æœæ•°
        """
        try:
            tasks = []
            
            # é¡¹ç›®çŸ¥è¯†åº“æœç´¢ï¼ˆæ€»æ˜¯æ‰§è¡Œï¼‰
            project_task = self._search_project_knowledge(
                query, project_vector_storage_path, project_top_k
            )
            tasks.append(project_task)
            
            # å…¨å±€çŸ¥è¯†åº“æœç´¢ï¼ˆå¯é€‰ï¼‰
            if enable_global:
                global_task = global_knowledge.search_global(query, global_top_k)
                tasks.append(global_task)
            else:
                tasks.append(asyncio.coroutine(lambda: [])())
            
            # å¹¶è¡Œæ‰§è¡Œ
            project_results, global_results = await asyncio.gather(*tasks)
            
            logger.info(f"ğŸ” æ··åˆæ£€ç´¢å®Œæˆ - é¡¹ç›®ç»“æœ: {len(project_results)}, å…¨å±€ç»“æœ: {len(global_results)}")
            
            # åˆå¹¶ç»“æœ
            return self._merge_search_results(global_results, project_results)
            
        except Exception as e:
            logger.error(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def invalidate_project_cache(self, project_vector_storage_path: str):
        """æ¸…é™¤é¡¹ç›®å¼•æ“ç¼“å­˜ï¼ˆå½“é¡¹ç›®æ–‡æ¡£æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        cache_key = project_vector_storage_path
        if cache_key in self._project_engines:
            del self._project_engines[cache_key]
            logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤é¡¹ç›®å¼•æ“ç¼“å­˜: {cache_key}")
    
    # ========== ğŸ“ é¡¹ç›®çŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½ ==========
    
    def create_project_knowledge_base(self, project_id: str, workspace_root: str = "workspace") -> str:
        """
        ä¸ºé¡¹ç›®åˆ›å»ºä¸“ç”¨çŸ¥è¯†åº“ç›®å½•
        
        Returns:
            str: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
        """
        project_vector_path = Path(workspace_root) / project_id / "vector_storage" / "project_docs"
        project_vector_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“ é¡¹ç›®çŸ¥è¯†åº“å·²åˆ›å»º: {project_vector_path}")
        return str(project_vector_path)
    
    def add_content_to_project(self, content: str, filename: str, project_vector_storage_path: str) -> bool:
        """
        æ·»åŠ å†…å®¹åˆ°é¡¹ç›®çŸ¥è¯†åº“
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            filename: æ–‡ä»¶å
            project_vector_storage_path: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(project_vector_storage_path).mkdir(parents=True, exist_ok=True)
            
            # å†…å®¹åˆ†å—å¤„ç†
            chunks = self._split_content_to_chunks(content)
            
            # ä¿å­˜åˆ†å—åˆ°æ–‡ä»¶
            for i, chunk in enumerate(chunks):
                chunk_filename = f"{Path(filename).stem}_chunk_{i}.txt"
                chunk_file_path = Path(project_vector_storage_path) / chunk_filename
                chunk_file_path.write_text(chunk.strip(), encoding='utf-8')
            
            # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡å»ºç´¢å¼•
            self.invalidate_project_cache(project_vector_storage_path)
            
            logger.info(f"ğŸ“„ å·²æ·»åŠ  {len(chunks)} ä¸ªå†…å®¹å—åˆ°é¡¹ç›®çŸ¥è¯†åº“: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å†…å®¹åˆ°é¡¹ç›®çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def add_multiple_contents_to_project(self, contents: List[dict], project_vector_storage_path: str) -> bool:
        """
        æ‰¹é‡æ·»åŠ å†…å®¹åˆ°é¡¹ç›®çŸ¥è¯†åº“
        
        Args:
            contents: å†…å®¹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"content": "å†…å®¹", "filename": "æ–‡ä»¶å"}, ...]
            project_vector_storage_path: é¡¹ç›®å‘é‡å­˜å‚¨è·¯å¾„
        """
        try:
            success_count = 0
            for item in contents:
                if self.add_content_to_project(
                    content=item.get("content", ""),
                    filename=item.get("filename", f"content_{success_count}.txt"),
                    project_vector_storage_path=project_vector_storage_path
                ):
                    success_count += 1
            
            logger.info(f"ğŸ“¦ æ‰¹é‡æ·»åŠ å®Œæˆ: {success_count}/{len(contents)} ä¸ªå†…å®¹æˆåŠŸæ·»åŠ ")
            return success_count == len(contents)
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ·»åŠ å†…å®¹å¤±è´¥: {e}")
            return False
    
    def _split_content_to_chunks(self, content: str, max_chunk_size: int = 2000) -> List[str]:
        """
        ğŸš€ æ™ºèƒ½å†…å®¹åˆ†å—é€»è¾‘ - åŸºäºè¯­ä¹‰è¾¹ç•Œçš„ä¼˜åŒ–ç­–ç•¥
        å‚è€ƒä¸šç•Œæœ€ä½³å®è·µï¼š1024 tokens â‰ˆ 2000å­—ç¬¦ ä¸ºæœ€ä¼˜å¹³è¡¡ç‚¹
        """
        # ğŸ¯ ä¼˜åŒ–1: æé«˜æœ€å°æœ‰æ•ˆchunkå¤§å°ï¼Œé¿å…æ— æ„ä¹‰çš„å°ç‰‡æ®µ
        min_chunk_size = 200  # é¿å…äº§ç”Ÿè¿‡å°çš„æ— æ„ä¹‰chunk
        
        if len(content) <= max_chunk_size:
            return [content] if len(content.strip()) >= min_chunk_size else []
        
        chunks = []
        
        # ğŸ¯ ä¼˜åŒ–2: ä¼˜å…ˆä¿æŠ¤è¡¨æ ¼å®Œæ•´æ€§
        if self._contains_table(content):
            table_chunks = self._handle_table_content(content, max_chunk_size)
            if table_chunks:
                return table_chunks
        
        # ğŸ¯ ä¼˜åŒ–3: é€’å½’åˆ†å—ç­–ç•¥ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        # åˆ†éš”ç¬¦ä¼˜å…ˆçº§ï¼šç« èŠ‚ > æ®µè½ > å¥å­ > å¼ºåˆ¶åˆ†å‰²
        separators = [
            '\n\n## ',  # ç« èŠ‚æ ‡é¢˜
            '\n\n# ',   # ä¸»æ ‡é¢˜  
            '\n\n',     # æ®µè½åˆ†éš”
            '\n',       # è¡Œåˆ†éš”
            'ã€‚',       # å¥å­åˆ†éš”
            'ï¼›',       # åˆ†å¥
            'ï¼Œ',       # çŸ­è¯­åˆ†éš”
        ]
        
        # é€’å½’åˆ†å—
        chunks = self._recursive_split(content, max_chunk_size, min_chunk_size, separators)
        
        # ğŸ¯ ä¼˜åŒ–4: æ·»åŠ 10%é‡å ï¼Œé¿å…è¾¹ç•Œä¿¡æ¯ä¸¢å¤±
        overlapped_chunks = self._add_overlap(chunks, overlap_ratio=0.1)
        
        # ğŸ¯ ä¼˜åŒ–5: è¿‡æ»¤è¿‡å°çš„chunksï¼Œé¿å…å™ªå£°
        valid_chunks = [chunk for chunk in overlapped_chunks if len(chunk.strip()) >= min_chunk_size]
        
        logger.debug(f"ğŸ“ æ™ºèƒ½åˆ†å—å®Œæˆ: {len(content)} å­—ç¬¦ -> {len(valid_chunks)} ä¸ªæœ‰æ•ˆå—")
        return valid_chunks
    
    def _contains_table(self, content: str) -> bool:
        """æ£€æµ‹å†…å®¹æ˜¯å¦åŒ…å«è¡¨æ ¼"""
        table_indicators = ['<table>', '<tr>', '<td>', '|---|', '|----']
        return any(indicator in content for indicator in table_indicators)
    
    def _handle_table_content(self, content: str, max_chunk_size: int) -> List[str]:
        """å¤„ç†åŒ…å«è¡¨æ ¼çš„å†…å®¹ï¼Œä¿æŒè¡¨æ ¼å®Œæ•´æ€§"""
        # ç®€å•ç­–ç•¥ï¼šå¦‚æœæ•´ä¸ªå†…å®¹åŒ…å«è¡¨æ ¼ä¸”ä¸è¶…è¿‡æœ€å¤§å¤§å°ï¼Œä¿æŒå®Œæ•´
        if len(content) <= max_chunk_size * 1.5:  # è¡¨æ ¼å…è®¸ç¨å¾®è¶…è¿‡é™åˆ¶
            return [content]
        
        # å¤æ‚è¡¨æ ¼ï¼šå°è¯•æŒ‰è¡¨æ ¼åˆ†å‰²
        table_parts = content.split('<table>')
        if len(table_parts) > 1:
            chunks = []
            current_chunk = table_parts[0]
            
            for i, part in enumerate(table_parts[1:], 1):
                table_content = '<table>' + part
                if len(current_chunk + table_content) <= max_chunk_size:
                    current_chunk += table_content
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = table_content
            
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            return chunks
        
        return []
    
    def _recursive_split(self, text: str, max_size: int, min_size: int, separators: List[str], depth: int = 0) -> List[str]:
        """é€’å½’åˆ†å‰²ç­–ç•¥"""
        if len(text) <= max_size:
            return [text] if len(text.strip()) >= min_size else []
        
        if depth >= len(separators):
            # å¼ºåˆ¶æŒ‰å­—ç¬¦åˆ†å‰²
            return [text[i:i+max_size] for i in range(0, len(text), max_size)]
        
        separator = separators[depth]
        parts = text.split(separator)
        
        if len(parts) == 1:
            # å½“å‰åˆ†éš”ç¬¦æ— æ•ˆï¼Œå°è¯•ä¸‹ä¸€ä¸ª
            return self._recursive_split(text, max_size, min_size, separators, depth + 1)
        
        chunks = []
        current_chunk = ""
        
        for i, part in enumerate(parts):
            # é‡æ–°æ·»åŠ åˆ†éš”ç¬¦
            if i > 0:
                part = separator + part
            
            if len(current_chunk + part) <= max_size:
                current_chunk += part
            else:
                if current_chunk.strip() and len(current_chunk.strip()) >= min_size:
                    chunks.append(current_chunk.strip())
                
                # å¦‚æœå•ä¸ªpartè¿˜æ˜¯å¤ªå¤§ï¼Œé€’å½’åˆ†å‰²
                if len(part) > max_size:
                    chunks.extend(self._recursive_split(part, max_size, min_size, separators, depth + 1))
                    current_chunk = ""
                else:
                    current_chunk = part
        
        if current_chunk.strip() and len(current_chunk.strip()) >= min_size:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _add_overlap(self, chunks: List[str], overlap_ratio: float = 0.1) -> List[str]:
        """æ·»åŠ chunké—´é‡å ï¼Œé¿å…è¾¹ç•Œä¿¡æ¯ä¸¢å¤±"""
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = [chunks[0]]
        
        for i in range(1, len(chunks)):
            prev_chunk = chunks[i-1]
            curr_chunk = chunks[i]
            
            # è®¡ç®—é‡å é•¿åº¦
            overlap_length = int(len(prev_chunk) * overlap_ratio)
            if overlap_length > 0:
                # ä»å‰ä¸€ä¸ªchunkæœ«å°¾å–é‡å å†…å®¹
                overlap_text = prev_chunk[-overlap_length:]
                overlapped_chunk = overlap_text + "\n\n" + curr_chunk
                overlapped_chunks.append(overlapped_chunk)
            else:
                overlapped_chunks.append(curr_chunk)
        
        return overlapped_chunks
    
    def get_project_knowledge_stats(self, project_vector_storage_path: str) -> dict:
        """è·å–é¡¹ç›®çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not Path(project_vector_storage_path).exists():
                return {"exists": False, "file_count": 0, "index_exists": False}
            
            # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
            files = list(Path(project_vector_storage_path).glob("*.txt"))
            file_count = len(files)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            index_path = self._get_project_vector_index_path(project_vector_storage_path)
            index_exists = self._is_project_index_exists(index_path)
            
            return {
                "exists": True,
                "file_count": file_count,
                "index_exists": index_exists,
                "storage_path": project_vector_storage_path,
                "index_path": index_path
            }
        except Exception as e:
            logger.error(f"âŒ è·å–é¡¹ç›®çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥: {e}")
            return {"exists": False, "error": str(e)}


# å…¨å±€å•ä¾‹å®ä¾‹
hybrid_search = HybridSearchService()