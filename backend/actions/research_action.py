#!/usr/bin/env python
"""
ç ”ç©¶Actioné›†åˆ - ProductManagerä¸“ç”¨
æ•´åˆMetaGPTåŸç”ŸProductManagerçš„ä¼˜ç§€å®è·µå’ŒCaseExpertçš„ç ”ç©¶èƒ½åŠ›
å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–æç¤ºè¯å’Œç ”ç©¶é€»è¾‘
"""
import asyncio
from datetime import datetime
from typing import List, Optional, Dict, Tuple
import hashlib
import time
from pathlib import Path

from pydantic import BaseModel, Field, TypeAdapter
from metagpt.actions import Action
from metagpt.logs import logger
from metagpt.tools.search_engine import SearchEngine
from metagpt.tools.web_browser_engine import WebBrowserEngine
from metagpt.utils.project_repo import ProjectRepo
from metagpt.utils.common import OutputParser

# MetaGPT åŸç”Ÿ RAG ç»„ä»¶ - å¼ºåˆ¶ä½¿ç”¨ï¼Œä¸å†æä¾›ç®€åŒ–ç‰ˆæœ¬


# --- æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–æç¤ºè¯ ---
COMPREHENSIVE_RESEARCH_BASE_SYSTEM = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„AIç ”ç©¶åˆ†æå¸ˆå’Œäº§å“ç»ç†ã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼š
1. æ·±å…¥ç†è§£ç”¨æˆ·éœ€æ±‚å’Œä¸šåŠ¡èƒŒæ™¯
2. è¿›è¡Œå…¨é¢çš„å¸‚åœºå’Œæ¡ˆä¾‹ç ”ç©¶
3. ç”Ÿæˆé«˜è´¨é‡çš„ç ”ç©¶ç®€æŠ¥ï¼Œä¸ºåç»­çš„æ¶æ„è®¾è®¡å’Œå†…å®¹ç”Ÿæˆæä¾›åšå®åŸºç¡€
"""

RESEARCH_TOPIC_SYSTEM = "ä½ æ˜¯ä¸€åAIç ”ç©¶åˆ†æå¸ˆï¼Œä½ çš„ç ”ç©¶ä¸»é¢˜æ˜¯:\n#ä¸»é¢˜#\n{topic}"

SEARCH_KEYWORDS_PROMPT = """è¯·æ ¹æ®ä½ çš„ç ”ç©¶ä¸»é¢˜ï¼Œæä¾›æœ€å¤š3ä¸ªå¿…è¦çš„å…³é”®è¯ç”¨äºç½‘ç»œæœç´¢ã€‚
è¿™äº›å…³é”®è¯åº”è¯¥èƒ½å¤Ÿå¸®åŠ©æ”¶é›†åˆ°æœ€ç›¸å…³å’Œæœ€æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚
ä½ çš„å›åº”å¿…é¡»æ˜¯JSONæ ¼å¼ï¼Œä¾‹å¦‚ï¼š["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]ã€‚"""

DECOMPOSE_RESEARCH_PROMPT = """### è¦æ±‚
1. ä½ çš„ç ”ç©¶ä¸»é¢˜å’Œåˆæ­¥æœç´¢ç»“æœå±•ç¤ºåœ¨"å‚è€ƒä¿¡æ¯"éƒ¨åˆ†ã€‚
2. è¯·åŸºäºè¿™äº›ä¿¡æ¯ï¼Œç”Ÿæˆæœ€å¤š {decomposition_nums} ä¸ªä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³çš„ã€æ›´å…·ä½“çš„è°ƒæŸ¥é—®é¢˜ã€‚
3. è¿™äº›é—®é¢˜åº”è¯¥èƒ½å¤Ÿå¸®åŠ©æ·±å…¥äº†è§£ä¸»é¢˜çš„ä¸åŒæ–¹é¢ï¼ŒåŒ…æ‹¬å¸‚åœºè¶‹åŠ¿ã€æœ€ä½³å®è·µã€æŠ€æœ¯æ–¹æ¡ˆç­‰ã€‚
4. ä½ çš„å›åº”å¿…é¡»æ˜¯JSONæ ¼å¼ï¼š["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3", ...]ã€‚

### å‚è€ƒä¿¡æ¯
{search_results}
"""

RANK_URLS_PROMPT = """### ä¸»é¢˜
{topic}
### å…·ä½“é—®é¢˜
{query}
### åŸå§‹æœç´¢ç»“æœ
{results}
### è¦æ±‚
è¯·ç§»é™¤ä¸å…·ä½“é—®é¢˜æˆ–ä¸»é¢˜æ— å…³çš„æœç´¢ç»“æœã€‚
å¦‚æœé—®é¢˜å…·æœ‰æ—¶æ•ˆæ€§ï¼Œè¯·ç§»é™¤è¿‡æ—¶çš„ä¿¡æ¯ã€‚å½“å‰æ—¶é—´æ˜¯ {time_stamp}ã€‚
ç„¶åï¼Œæ ¹æ®é“¾æ¥çš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§å¯¹å‰©ä¸‹çš„ç»“æœè¿›è¡Œæ’åºã€‚
ä¼˜å…ˆè€ƒè™‘ï¼šå®˜æ–¹æ–‡æ¡£ã€æƒå¨æœºæ„ã€çŸ¥åä¼ä¸šæ¡ˆä¾‹ã€å­¦æœ¯ç ”ç©¶ç­‰ã€‚
ä»¥JSONæ ¼å¼æä¾›æ’åºåç»“æœçš„ç´¢å¼•ï¼Œä¾‹å¦‚ [0, 1, 3, 4, ...]ï¼Œä¸è¦åŒ…å«å…¶ä»–ä»»ä½•æ–‡å­—ã€‚
"""

WEB_CONTENT_ANALYSIS_PROMPT = """### è¦æ±‚
1. åˆ©ç”¨"å‚è€ƒä¿¡æ¯"ä¸­çš„æ–‡æœ¬æ¥å›ç­”é—®é¢˜"{query}"ã€‚
2. å¦‚æœæ— æ³•ç›´æ¥å›ç­”ï¼Œä½†å†…å®¹ä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³ï¼Œè¯·å¯¹æ–‡æœ¬è¿›è¡Œå…¨é¢æ€»ç»“ã€‚
3. å¦‚æœå†…å®¹å®Œå…¨ä¸ç›¸å…³ï¼Œè¯·å›å¤"ä¸ç›¸å…³"ã€‚
4. é‡ç‚¹æå–ï¼šå…³é”®æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯ã€æœ€ä½³å®è·µã€æŠ€æœ¯æ–¹æ¡ˆã€å¸‚åœºè¶‹åŠ¿ç­‰ã€‚
5. ä¿æŒå®¢è§‚æ€§ï¼Œæ³¨æ˜ä¿¡æ¯æ¥æºçš„å¯ä¿¡åº¦ã€‚

### å‚è€ƒä¿¡æ¯
{content}
"""

GENERATE_RESEARCH_BRIEF_PROMPT = """### å‚è€ƒä¿¡æ¯
{content}

### æŒ‡ä»¤
ä½ æ˜¯ä¸€åé¡¶çº§çš„AIç ”ç©¶åˆ†æå¸ˆï¼Œä½ çš„ç›®æ ‡æ˜¯ä¸ºä¸‹æ¸¸çš„æŠ¥å‘Šæ¶æ„å¸ˆæä¾›ä¸€ä»½å……æ»¡æ´å¯ŸåŠ›çš„å‰æœŸè°ƒç ”ç®€æŠ¥ã€‚
è¯·æ ¹æ®ä»¥ä¸Šå‚è€ƒä¿¡æ¯ï¼ˆåŒ…å«ç½‘ç»œæ¡ˆä¾‹å’Œæœ¬åœ°çŸ¥è¯†åº“ï¼‰ï¼Œå›´ç»•ä¸»é¢˜â€œ{topic}â€ç”Ÿæˆä¸€ä»½é«˜è´¨é‡çš„ç ”ç©¶ç®€æŠ¥ã€‚

ç®€æŠ¥æ ‡é¢˜æ ¼å¼åº”ä¸ºï¼š**ã€Šå½“å‰é¡¹ç›®ç»©æ•ˆè¯„ä»·æŠ¥å‘Šå‰æœŸè°ƒç ”ç®€æŠ¥ã€‹**

### ç®€æŠ¥ç»“æ„è¦æ±‚ (è¯·ä¸¥æ ¼éµå¾ª)

**1. é¡¹ç›®ç«‹é¡¹èƒŒæ™¯åŠç›®çš„åˆ†æ**
   - **åˆ†æè¦æ±‚**: ç»¼åˆç½‘ç»œæ¡ˆä¾‹å’Œæœ¬åœ°èµ„æ–™ï¼Œåˆ†ææ­¤ç±»é¡¹ç›®é€šå¸¸åœ¨ä½•ç§èƒŒæ™¯ä¸‹ç«‹é¡¹ï¼Œå…¶æ ¸å¿ƒç›®çš„æ˜¯ä»€ä¹ˆã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: æŒ‡å‡ºåœ¨æ’°å†™æœ€ç»ˆæŠ¥å‘Šçš„â€œé¡¹ç›®æ¦‚è¿°â€ç« èŠ‚æ—¶ï¼Œåº”é‡ç‚¹çªå‡ºå“ªäº›èƒŒæ™¯å› ç´ å’Œé¡¹ç›®ç›®çš„ï¼Œä»¥å½°æ˜¾å…¶å¿…è¦æ€§å’Œé‡è¦æ€§ã€‚

**2. é¡¹ç›®ä¸»è¦å†…å®¹æ´å¯Ÿ**
   - **åˆ†æè¦æ±‚**: å‰–æä¸åŒè¡Œä¸šçš„ç±»ä¼¼ç»©æ•ˆæŠ¥å‘Šé€šå¸¸åŒ…å«å“ªäº›æ ¸å¿ƒå†…å®¹ã€‚ç»“åˆæœ¬åœ°èµ„æ–™ï¼Œæç‚¼å‡ºæœ¬æ¬¡æŠ¥å‘Šåº”é‡ç‚¹å…³æ³¨çš„å‡ ä¸ªæ–¹é¢ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºæŠ¥å‘Šçš„â€œé¡¹ç›®æ¦‚è¿°â€éƒ¨åˆ†æä¾›å†…å®¹å»ºè®®ï¼ŒæŒ‡å‡ºå“ªäº›æ˜¯å¿…é¡»åŒ…å«çš„å…³é”®æ¨¡å—ã€‚

**3. èµ„é‡‘æŠ•å…¥å’Œä½¿ç”¨æƒ…å†µåˆ†æ**
   - **åˆ†æè¦æ±‚**: æ€»ç»“ç½‘ç»œæ¡ˆä¾‹ä¸­å…³äºèµ„é‡‘ç®¡ç†å’Œä½¿ç”¨çš„ç»éªŒä¸æ•™è®­ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: æé†’æ¶æ„å¸ˆåœ¨è®¾è®¡â€œèµ„é‡‘ä½¿ç”¨â€ç›¸å…³ç« èŠ‚æ—¶ï¼Œåº”å¼•å¯¼å†™ä½œè€…é‡ç‚¹å…³æ³¨å“ªäº›æ•°æ®ï¼ˆå¦‚é¢„ç®—æ‰§è¡Œç‡ã€èµ„é‡‘åˆ°ä½åŠæ—¶æ€§ç­‰ï¼‰ï¼Œå¹¶å¯ä»¥ä»å“ªäº›è§’åº¦è¿›è¡Œåˆ†æã€‚

**4. é¡¹ç›®å®æ–½ä¸ç»„ç»‡ç®¡ç†ç»éªŒå€Ÿé‰´**
   - **åˆ†æè¦æ±‚**: ä»å‚è€ƒä¿¡æ¯ä¸­æç‚¼å‡ºé¡¹ç›®å®æ–½å’Œç»„ç»‡ç®¡ç†æ–¹é¢çš„æœ€ä½³å®è·µæˆ–å¸¸è§é—®é¢˜ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºæŠ¥å‘Šçš„â€œç»„ç»‡ç®¡ç†â€éƒ¨åˆ†æä¾›å†™ä½œæ–¹å‘ï¼Œä¾‹å¦‚å¯ä»¥å€Ÿé‰´å“ªäº›ç®¡ç†æ¨¡å¼ï¼Œæˆ–è€…éœ€è¦è§„é¿å“ªäº›å¸¸è§é£é™©ã€‚

**5. ç»©æ•ˆç›®æ ‡è®¾å®šè¦ç‚¹**
   - **åˆ†æè¦æ±‚**: åŸºäºå¯¹å››ä¸ªç»´åº¦ï¼ˆå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šï¼‰çš„ç†è§£ï¼Œåˆ†æç½‘ç»œæ¡ˆä¾‹ä¸­æ˜¯å¦‚ä½•è®¾å®šç»©æ•ˆç›®æ ‡çš„ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: æŒ‡å‡ºåœ¨è®¾è®¡æŠ¥å‘Šçš„â€œç»©æ•ˆç›®æ ‡â€éƒ¨åˆ†æ—¶ï¼Œå››ä¸ªç»´åº¦åˆ†åˆ«å¯ä»¥ä»å“ªäº›è§’åº¦è¿›è¡Œç›®æ ‡è®¾å®šï¼Œå¹¶æä¾›å¯ä»¥å€Ÿé‰´çš„ç›®æ ‡æè¿°æ–¹å¼ã€‚**è¿™ä¸æ˜¯æœ€ç»ˆçš„æŒ‡æ ‡ï¼Œè€Œæ˜¯ç›®æ ‡è®¾å®šçš„æ€è·¯å’Œæ–¹å‘**ã€‚

**6. å­˜åœ¨çš„é—®é¢˜å’ŒåŸå› åˆ†æ**
   - **åˆ†æè¦æ±‚**: åŸºäºç½‘ç»œæ¡ˆä¾‹å’Œæœ¬åœ°èµ„æ–™ï¼Œå½’çº³æ€»ç»“ç±»ä¼¼é¡¹ç›®åœ¨å®æ–½è¿‡ç¨‹ä¸­æ™®éå­˜åœ¨çš„é—®é¢˜ï¼ˆå¦‚èµ„é‡‘ä½¿ç”¨æ•ˆç‡ä½ã€ç›®æ ‡è¾¾æˆç‡ä¸è¶³ã€ç›‘ç®¡ç¼ºä½ç­‰ï¼‰ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: é¢„æµ‹å½“å‰é¡¹ç›®å¯èƒ½é¢ä¸´çš„æ½œåœ¨é£é™©å’ŒæŒ‘æˆ˜ã€‚è¿™éƒ¨åˆ†å†…å®¹å°†ä½œä¸ºæŠ¥å‘Šä¸­â€œå­˜åœ¨çš„é—®é¢˜â€ç« èŠ‚çš„é‡è¦å‚è€ƒï¼Œä½¿å¾—æœ€ç»ˆæŠ¥å‘Šä¸ä»…èƒ½æ€»ç»“æˆç»©ï¼Œæ›´èƒ½ä½“ç°å‰ç»æ€§çš„é£é™©æ€è€ƒã€‚

**7. æ”¹è¿›å»ºè®®ä¸ç»éªŒå€Ÿé‰´**
   - **åˆ†æè¦æ±‚**: æ·±å…¥åˆ†ææˆåŠŸæ¡ˆä¾‹æ˜¯å¦‚ä½•è§£å†³ä¸Šè¿°æ™®éé—®é¢˜çš„ï¼Œæç‚¼å‡ºå¯æ“ä½œã€å¯å€Ÿé‰´çš„æ”¹è¿›æªæ–½å’Œç®¡ç†ç»éªŒã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºæŠ¥å‘Šçš„â€œæ”¹è¿›å»ºè®®â€ç« èŠ‚æä¾›ç´ æã€‚è¿™äº›åŸºäºçœŸå®æ¡ˆä¾‹çš„å»ºè®®ï¼Œå°†æ¯”æ³›æ³›è€Œè°ˆçš„é€šç”¨å»ºè®®æ›´å…·è¯´æœåŠ›å’Œå¯æ“ä½œæ€§ã€‚

**8. ç»©æ•ˆè¯„ä»·ä½“ç³»æ¨è (æ ¸å¿ƒå†…å®¹)**
   - **åˆ†æè¦æ±‚**: åŸºäºæ‰€æœ‰ç ”ç©¶ï¼Œä¸ºå½“å‰é¡¹ç›®æ¨èä¸€å¥—ç»©æ•ˆè¯„ä»·æŒ‡æ ‡ä½“ç³»ã€‚
   - **ç»™æ¶æ„å¸ˆçš„å»ºè®®**: ä¸ºâ€œå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šâ€å››ä¸ªç»´åº¦ï¼Œ**æ¯ä¸ªç»´åº¦æ¨è5ä¸ªå…·ä½“ã€å¯è¡¡é‡çš„è¯„ä»·æŒ‡æ ‡**ã€‚æŒ‡æ ‡åº”æ¸…æ™°ï¼Œå¹¶ç®€è¦è¯´æ˜æ¨èç†ç”±ã€‚è¿™å°†æ˜¯æ¶æ„å¸ˆè®¾è®¡æŒ‡æ ‡ä½“ç³»çš„é‡è¦è¾“å…¥ã€‚

### è´¨é‡è¦æ±‚
- **æ·±åº¦æ´å¯Ÿ**: ä¸è¦ç®€å•ç½—åˆ—ä¿¡æ¯ï¼Œè¦æç‚¼è§‚ç‚¹ã€æ€»ç»“ç»éªŒã€ç»™å‡ºå»ºè®®ã€‚
- **å¼ºåŠ›æ”¯æŒ**: ç®€æŠ¥çš„æ¯ä¸€éƒ¨åˆ†éƒ½åº”ä¸ºä¸‹æ¸¸çš„æ¶æ„å¸ˆæä¾›æ¸…æ™°çš„ã€å¯æ“ä½œçš„æŒ‡å¯¼ã€‚
- **æ ¼å¼æ¸…æ™°**: ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æ„åˆ†æ˜ã€‚


**å¼•ç”¨è¦æ±‚ï¼š**
- åœ¨ç®€æŠ¥æœ«å°¾åˆ—å‡ºæ‰€æœ‰ä¿¡æ¯æ¥æºçš„URL
- å¯¹å…³é”®æ•°æ®å’Œè§‚ç‚¹è¿›è¡Œé€‚å½“çš„å¼•ç”¨æ ‡æ³¨
"""


class Document(BaseModel):
    """å•ä¸ªæ–‡æ¡£çš„ç»“æ„åŒ–æ¨¡å‹"""
    filename: str
    content: str


class Documents(BaseModel):
    """æ–‡æ¡£é›†åˆçš„ç»“æ„åŒ–æ¨¡å‹"""
    docs: List[Document] = Field(default_factory=list)


class ResearchData(BaseModel):
    """ç ”ç©¶æˆæœçš„ç»“æ„åŒ–æ•°æ®æ¨¡å‹"""
    brief: str = Field(..., description="åŸºäºç ”ç©¶ç”Ÿæˆçš„ç»¼åˆæ€§ç®€æŠ¥")
    vector_store_path: str = Field(..., description="å­˜å‚¨ç ”ç©¶å†…å®¹å‘é‡ç´¢å¼•çš„è·¯å¾„")
    content_chunks: List[str] = Field(default_factory=list, description="åˆ†å—çš„å†…å®¹åˆ—è¡¨ï¼Œä¾›RAGæ£€ç´¢ä½¿ç”¨")


class PrepareDocuments(Action):
    """æ‰«ææœ¬åœ°ç›®å½•ï¼ŒåŠ è½½ç”¨æˆ·æä¾›çš„æ–‡æ¡£ä½œä¸ºç ”ç©¶ææ–™"""
    
    async def run(self, uploads_path: Path) -> Documents:
        """æ‰«æuploadsç›®å½•ï¼Œè¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹"""
        docs = []
        if not uploads_path.exists():
            logger.warning(f"ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨: {uploads_path}")
            return Documents(docs=docs)

        logger.info(f"å¼€å§‹æ‰«ææ–‡æ¡£ç›®å½•: {uploads_path}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {'.txt', '.md', '.csv', '.json', '.yaml', '.yml'}
        
        for file_path in uploads_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                try:
                    content = file_path.read_text(encoding='utf-8')
                    docs.append(Document(filename=file_path.name, content=content))
                    logger.info(f"æˆåŠŸè¯»å–æ–‡æ¡£: {file_path.name}")
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"æ–‡æ¡£æ‰«æå®Œæˆï¼Œå…±è¯»å– {len(docs)} ä¸ªæ–‡æ¡£")
        return Documents(docs=docs)


class ConductComprehensiveResearch(Action):
    """
    ç»¼åˆç ”ç©¶Action - æ•´åˆæœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œç ”ç©¶
    å®Œå…¨æ•´åˆcase_research.pyä¸­çš„ç²¾ç»†åŒ–ç ”ç©¶é€»è¾‘å’Œæç¤ºè¯
    """
    
    def __init__(self, search_engine: SearchEngine = None, **kwargs):
        super().__init__(**kwargs)
        self.search_engine = search_engine

    async def run(
        self, 
        topic: str,
        decomposition_nums: int = 1,  # æµ‹è¯•æ¨¡å¼: 3->2
        url_per_query: int = 1,       # æµ‹è¯•æ¨¡å¼: 3->2
        project_repo: ProjectRepo = None,
        local_docs: Documents = None
    ) -> ResearchData:
        """æ‰§è¡Œå…¨é¢çš„ç ”ç©¶ï¼Œæ•´åˆç½‘ç»œæœç´¢å’Œæœ¬åœ°æ–‡æ¡£ï¼Œæ„å»ºå‘é‡çŸ¥è¯†åº“"""
        logger.info(f"å¼€å§‹å¯¹ä¸»é¢˜ '{topic}' è¿›è¡Œå…¨é¢ç ”ç©¶ï¼ŒåŒ…æ‹¬å‘é‡åŒ–å¤„ç†...")

        # 1. ã€ä¼˜å…ˆã€‘æ„å»ºåŸºç¡€å‘é‡çŸ¥è¯†åº“ï¼ˆå¦‚æœæä¾›æœ¬åœ°æ–‡æ¡£ï¼‰
        base_engine = None
        if local_docs and local_docs.docs:
            logger.info("æ£€æµ‹åˆ°æœ¬åœ°æ–‡æ¡£ï¼Œä¼˜å…ˆæ„å»ºåŸºç¡€å‘é‡çŸ¥è¯†åº“...")
            vector_store_path, content_chunks, base_engine = await self._build_vector_knowledge_base_native(
                topic, "", local_docs, "", project_repo
            )
            logger.info(f"âœ… åŸºç¡€å‘é‡åº“æ„å»ºæˆåŠŸ: {vector_store_path}")
            logger.info("âœ… å‘é‡æ£€ç´¢å¼•æ“å·²å‡†å¤‡å°±ç»ªã€‚")
        
        # 2. ç½‘ç»œç ”ç©¶ (æ•´åˆRAGæ£€ç´¢)
        online_research_content = await self._conduct_online_research(
            topic, 
            decomposition_nums, 
            url_per_query,
            rag_engine=base_engine  # ä¼ å…¥RAGå¼•æ“
        )

        # 3. æ•´åˆå†…å®¹å¹¶ç”Ÿæˆç ”ç©¶ç®€æŠ¥ (æš‚æ—¶ä¿æŒä¸å˜ï¼Œåç»­ä¼˜åŒ–)
        combined_content = online_research_content
        if local_docs and local_docs.docs:
            local_docs_content = "\n\n--- æœ¬åœ°çŸ¥è¯†åº“ ---\n"
            for doc in local_docs.docs:
                local_docs_content += f"### æ–‡æ¡£: {doc.filename}\n{doc.content}\n\n"
            combined_content += local_docs_content

        prompt = GENERATE_RESEARCH_BRIEF_PROMPT.format(content=combined_content, topic=topic)
        brief = await self._aask(prompt, [COMPREHENSIVE_RESEARCH_BASE_SYSTEM])
        
        logger.info(f"ç ”ç©¶ç®€æŠ¥ç”Ÿæˆå®Œæ¯•ã€‚")

        # 4. ã€æ›´æ–°ã€‘å‘é‡çŸ¥è¯†åº“ (å°†ç½‘ç»œå†…å®¹åŠ å…¥) - å¼ºåˆ¶ä½¿ç”¨åŸç”ŸRAG
        final_vector_store_path, final_content_chunks, final_engine = await self._build_vector_knowledge_base_native(
            topic, online_research_content, local_docs, combined_content, project_repo
        )
        logger.info(f"ğŸ”¥ æœ€ç»ˆå‘é‡åº“å·²æ›´æ–°: {final_vector_store_path}")


        # 5. åˆ›å»ºå¹¶è¿”å›ResearchData
        research_data = ResearchData(
            brief=brief, 
            vector_store_path=final_vector_store_path,
            content_chunks=final_content_chunks
        )

        if project_repo:
            docs_filename = "research_brief.md"  # ä½¿ç”¨å›ºå®šçš„æ–‡ä»¶å
            await project_repo.docs.save(filename=docs_filename, content=brief)
            brief_path = project_repo.docs.workdir / docs_filename
            logger.info(f"ç ”ç©¶ç®€æŠ¥å·²ä¿å­˜åˆ°: {brief_path}")

        return research_data
    async def _build_vector_knowledge_base_native(
        self,
        topic: str,
        online_content: str,
        local_docs: List[Document],
        combined_content: str,
        project_repo: ProjectRepo = None
    ) -> Tuple[str, List[str], "SimpleEngine"]:
        """
        ä½¿ç”¨MetaGPTåŸç”ŸRAGå¼•æ“æ„å»ºå‘é‡çŸ¥è¯†åº“
        
        Returns:
            Tuple[str, List[str], SimpleEngine]: (vector_store_path, content_chunks, engine)
        """
        try:
            from metagpt.rag.engines.simple import SimpleEngine
            # from metagpt.rag.schema import FAISSRetrieverConfig, VectorIndexConfig
            import tempfile
            import os
            
            # åˆ›å»ºä¸´æ—¶å­˜å‚¨ç›®å½•
            if project_repo:
                base_dir = os.path.join(project_repo.workdir, "vector_storage")
            else:
                base_dir = tempfile.mkdtemp(prefix="rag_storage_")
            
            # å®‰å…¨çš„ç›®å½•åç§°
            safe_topic = "".join(c for c in topic if c.isalnum() or c in "()[]{},.!?;:@#$%^&*+=_-")[:100]
            vector_store_path = os.path.join(base_dir, safe_topic)
            os.makedirs(vector_store_path, exist_ok=True)
            
            # å‡†å¤‡æ–‡æ¡£å†…å®¹
            all_content = []
            
            # æ·»åŠ åœ¨çº¿ç ”ç©¶å†…å®¹
            if online_content and online_content.strip():
                all_content.append(("åœ¨çº¿ç ”ç©¶å†…å®¹", online_content))
            
            # æ·»åŠ æœ¬åœ°æ–‡æ¡£å†…å®¹
            if local_docs:  # æ£€æŸ¥local_docsä¸ä¸ºNone
                for doc in local_docs.docs:  # æ­£ç¡®è®¿é—®docså±æ€§
                    if doc.content and doc.content.strip():
                        all_content.append((f"æœ¬åœ°æ–‡æ¡£: {doc.filename}", doc.content))
            
            if not all_content:
                logger.warning("æ²¡æœ‰å¯ç”¨å†…å®¹æ„å»ºå‘é‡çŸ¥è¯†åº“")
                return vector_store_path, [], None
            
            # å°†å†…å®¹è½¬æ¢ä¸ºä¸´æ—¶æ–‡ä»¶
            temp_files = []
            content_chunks = []
            
            for title, content in all_content:
                # å°†å†…å®¹åˆ†å—
                chunks = self._split_content_to_chunks(content)
                content_chunks.extend(chunks)
                
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                temp_file = os.path.join(vector_store_path, f"{len(temp_files)}.txt")
                with open(temp_file, 'w', encoding='utf-8') as f:
                    f.write(f"# {title}\n\n{content}")
                temp_files.append(temp_file)
            
            # ä½¿ç”¨MetaGPTåŸç”Ÿçš„RAG embeddingå·¥å‚ - è¿™æ˜¯æ­£ç¡®çš„æ–¹å¼ï¼
            from llama_index.llms.openai import OpenAI as LlamaOpenAI
            from pathlib import Path
            from metagpt.config2 import Config
            from metagpt.rag.factories.embedding import get_rag_embedding
            
            # æ‰‹åŠ¨åŠ è½½å®Œæ•´é…ç½®ï¼Œç¡®ä¿embeddingé…ç½®è¢«æ­£ç¡®è¯»å–
            full_config = Config.from_yaml_file(Path('config/config2.yaml'))
            
            # è·å–LLMé…ç½® - ä½¿ç”¨å…¼å®¹çš„æ¨¡å‹å
            llm_config = full_config.llm
            llm = LlamaOpenAI(
                api_key=llm_config.api_key,
                base_url=llm_config.base_url,
                model="gpt-3.5-turbo"  # ä½¿ç”¨llama_indexè®¤è¯†çš„æ¨¡å‹åï¼Œå®é™…ä¼šè°ƒç”¨é˜¿é‡Œäº‘API
            )
            
            # ä½¿ç”¨MetaGPTåŸç”Ÿembeddingå·¥å‚ - è¿™ä¼šæ­£ç¡®å¤„ç†model_nameå‚æ•°
            embed_model = get_rag_embedding(config=full_config)
            # é˜¿é‡Œäº‘DashScope embedding APIé™åˆ¶æ‰¹å¤„ç†å¤§å°ä¸èƒ½è¶…è¿‡10
            embed_model.embed_batch_size = 10
            
            engine = SimpleEngine.from_docs(
                input_files=temp_files,  # æä¾›æ–‡ä»¶åˆ—è¡¨
                llm=llm,  # çœŸå®çš„LLMé…ç½®
                embed_model=embed_model  # çœŸå®çš„åµŒå…¥æ¨¡å‹
            )
            
            logger.info(f"âœ… å‘é‡çŸ¥è¯†åº“å·²æ„å»ºï¼Œå…± {len(content_chunks)} ä¸ªå†…å®¹å—")
            logger.info(f"ğŸ“ å‘é‡åº“å­˜å‚¨è·¯å¾„: {vector_store_path}")
            
            return vector_store_path, content_chunks, engine
            
        except Exception as e:
            logger.error(f"åŸç”ŸRAGå¼•æ“æ„å»ºå¤±è´¥: {e}")
            # ä¸å†é™çº§ï¼Œè®©é”™è¯¯æš´éœ²å‡ºæ¥
            raise e
    

    
    def _split_content_to_chunks(self, content: str, max_chunk_size: int = 1000) -> List[str]:
        """å°†å†…å®¹åˆ†å‰²æˆå—"""
        # ç®€å•çš„åˆ†å—ç­–ç•¥ï¼šæŒ‰æ®µè½å’Œé•¿åº¦åˆ†å‰²
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¼šè¶…å‡ºé™åˆ¶ï¼Œä¿å­˜å½“å‰å—
            if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += ("\n\n" if current_chunk else "") + paragraph
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    async def _conduct_online_research(self, topic: str, decomposition_nums: int, url_per_query: int, rag_engine=None) -> str:
        """æ‰§è¡Œåœ¨çº¿ç ”ç©¶"""
        if not self.search_engine:
            logger.warning("æœç´¢å¼•æ“æœªåˆå§‹åŒ–ï¼Œå°†è¿”å›æ¨¡æ‹Ÿç ”ç©¶å†…å®¹")
            return f"""### æ¨¡æ‹Ÿç ”ç©¶å†…å®¹
            
ä¸»é¢˜: {topic}

è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„åœ¨çº¿ç ”ç©¶ç»“æœã€‚ç”±äºæœç´¢å¼•æ“æœªé…ç½®ï¼Œæˆ‘ä»¬æä¾›ä»¥ä¸‹æ¨¡æ‹Ÿå†…å®¹ï¼š

1. **èƒŒæ™¯ä¿¡æ¯**: è¯¥é¡¹ç›®å±äºå†œä¸šé¢†åŸŸçš„ç»©æ•ˆè¯„ä»·é¡¹ç›®
2. **è¡Œä¸šè¶‹åŠ¿**: å½“å‰å†œä¸šé¡¹ç›®è¶Šæ¥è¶Šæ³¨é‡ç§‘å­¦åŒ–ç®¡ç†å’Œç»©æ•ˆè¯„ä¼°
3. **æœ€ä½³å®è·µ**: ç»¼åˆæ€§è¯„ä»·ä½“ç³»åº”åŒ…æ‹¬ç»æµæ•ˆç›Šã€ç¤¾ä¼šæ•ˆç›Šå’Œç¯å¢ƒæ•ˆç›Š
4. **æŠ€æœ¯æ–¹æ¡ˆ**: ä½¿ç”¨æ•°æ®åˆ†æå’Œä¸“ä¸šè¯„ä¼°æ–¹æ³•
5. **å…³é”®æŒ‡æ ‡**: æˆæœ¬æ§åˆ¶ã€é¡¹ç›®å®Œæˆåº¦ã€ç”¨æˆ·æ»¡æ„åº¦ç­‰

è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ç”¨çš„æ¨¡æ‹Ÿç ”ç©¶å†…å®¹ï¼Œå®é™…éƒ¨ç½²æ—¶åº”é…ç½®æœ‰æ•ˆçš„æœç´¢å¼•æ“ã€‚
"""
        
        logger.info("æ­¥éª¤ 1: ç”Ÿæˆæœç´¢å…³é”®è¯")
        keywords_prompt = RESEARCH_TOPIC_SYSTEM.format(topic=topic)
        keywords_str = await self._aask(SEARCH_KEYWORDS_PROMPT, [keywords_prompt])
        try:
            keywords = OutputParser.extract_struct(keywords_str, list)
        except Exception as e:
            logger.warning(f"å…³é”®è¯è§£æå¤±è´¥: {e}, ä½¿ç”¨ä¸»é¢˜ä½œä¸ºå…³é”®è¯")
            keywords = [topic]

        logger.info(f"å…³é”®è¯: {keywords}")

        # ä¸²è¡Œæœç´¢å…³é”®è¯ï¼Œé¿å…å¹¶å‘è¯·æ±‚è§¦å‘é¢‘ç‡é™åˆ¶
        search_results = []
        for i, kw in enumerate(keywords):
            try:
                if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                    await asyncio.sleep(2)  # æ¯ä¸ªè¯·æ±‚é—´éš”2ç§’
                result = await self.search_engine.run(kw, as_string=False)
                search_results.append(result)
                logger.info(f"æˆåŠŸæœç´¢å…³é”®è¯: {kw}")
            except Exception as e:
                logger.error(f"æœç´¢å…³é”®è¯å¤±è´¥ {kw}: {e}")
                search_results.append([])  # æ·»åŠ ç©ºç»“æœä¿æŒç´¢å¼•ä¸€è‡´
        
        # RAGå¢å¼ºï¼šä½¿ç”¨å…³é”®è¯æŸ¥è¯¢æœ¬åœ°å‘é‡åº“
        rag_results_str = ""
        if rag_engine:
            logger.info("...åŒæ—¶æŸ¥è¯¢æœ¬åœ°å‘é‡çŸ¥è¯†åº“...")
            rag_results = await rag_engine.aretrieve(query=" ".join(keywords))
            if rag_results:
                rag_results_str = "\n\n### æœ¬åœ°çŸ¥è¯†åº“ç›¸å…³ä¿¡æ¯\n" + "\n".join([r.text for r in rag_results])
        
        search_results_str = "\n".join([f"#### å…³é”®è¯: {kw}\n{res}\n" for kw, res in zip(keywords, search_results)])
        
        # å°†RAGç»“æœå’Œç½‘ç»œæœç´¢ç»“æœåˆå¹¶
        combined_search_results = search_results_str + rag_results_str

        logger.info("æ­¥éª¤ 2: åˆ†è§£ç ”ç©¶é—®é¢˜")
        decompose_prompt = DECOMPOSE_RESEARCH_PROMPT.format(
            decomposition_nums=decomposition_nums, 
            search_results=combined_search_results
        )
        queries_str = await self._aask(decompose_prompt, [keywords_prompt])
        try:
            queries = OutputParser.extract_struct(queries_str, list)
        except Exception as e:
            logger.warning(f"é—®é¢˜åˆ†è§£å¤±è´¥: {e}, ä½¿ç”¨å…³é”®è¯ä½œä¸ºé—®é¢˜")
            queries = keywords
        
        logger.info(f"ç ”ç©¶é—®é¢˜: {queries}")

        # ä¸²è¡Œå¤„ç†æ¯ä¸ªé—®é¢˜ï¼Œé¿å…å¹¶å‘æœç´¢
        summaries = []
        for i, q in enumerate(queries):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(1)  # æ¯ä¸ªé—®é¢˜å¤„ç†é—´éš”1ç§’
            summary = await self._search_and_summarize_query(topic, q, url_per_query)
            summaries.append(summary)

        return "\n\n".join(summaries)

    async def _search_and_summarize_query(self, topic: str, query: str, url_per_query: int) -> str:
        """æœç´¢ã€æ’åºå¹¶æ€»ç»“å•ä¸ªé—®é¢˜çš„URL"""
        logger.info(f"å¤„ç†é—®é¢˜: {query}")
        urls = await self._search_and_rank_urls(topic, query, url_per_query)
        
        if not urls:
            return f"### é—®é¢˜: {query}\n\næœªèƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚\n"

        # ä¸²è¡Œæµè§ˆå’Œåˆ†æURLï¼Œé¿å…å¹¶å‘è¯·æ±‚
        contents = []
        for i, url in enumerate(urls):
            if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                await asyncio.sleep(1)  # æ¯ä¸ªURLå¤„ç†é—´éš”1ç§’
            content = await self._web_browse_and_summarize(url, query)
            contents.append(content)

        # è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å†…å®¹
        relevant_contents = [c for c in contents if "ä¸ç›¸å…³" not in c]
        
        summary = f"### é—®é¢˜: {query}\n\n" + "\n\n".join(relevant_contents)
        return summary

    async def _search_and_rank_urls(self, topic: str, query: str, num_results: int) -> List[str]:
        """æœç´¢å¹¶æ’åºURL"""
        max_results = max(num_results * 2, 6)
        try:
            results = await self.search_engine.run(query, max_results=max_results, as_string=False)
            if not results:
                logger.warning(f"æœç´¢å¼•æ“æœªè¿”å›ä»»ä½•ç»“æœ: {query}")
                return []
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥ {query}: {e}")
            return []
    
        _results_str = "\n".join(f"{i}: {res}" for i, res in enumerate(results))
        time_stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        prompt = RANK_URLS_PROMPT.format(topic=topic, query=query, results=_results_str, time_stamp=time_stamp)
        
        logger.debug(f"URLæ’åºæç¤ºè¯: {prompt}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        indices_str = await self._aask(prompt)
        logger.debug(f"LLMè¿”å›çš„æ’åºç»“æœ: {indices_str}")  # æ·»åŠ è°ƒè¯•æ—¥å¿—
        
        try:
            indices = OutputParser.extract_struct(indices_str, list)
            if not indices:
                logger.warning(f"LLMè¿”å›ç©ºçš„æ’åºç´¢å¼•ï¼Œä½¿ç”¨åŸå§‹é¡ºåº")
                indices = list(range(min(len(results), num_results)))
            ranked_results = [results[i] for i in indices if i < len(results)]
        except Exception as e:
            logger.warning(f"URLæ’åºå¤±è´¥: {e}, ä½¿ç”¨åŸå§‹é¡ºåº")
            ranked_results = results[:num_results]
    
        final_urls = [res['link'] for res in ranked_results[:num_results]]
        logger.info(f"æœ€ç»ˆè·å¾— {len(final_urls)} ä¸ªURLç”¨äºæŸ¥è¯¢: {query}")
        
        return final_urls

    async def _web_browse_and_summarize(self, url: str, query: str) -> str:
        """æµè§ˆç½‘é¡µå¹¶æ€»ç»“å†…å®¹"""
        try:
            content = await WebBrowserEngine().run(url)
            prompt = WEB_CONTENT_ANALYSIS_PROMPT.format(content=content, query=query)
            summary = await self._aask(prompt)
            return f"#### æ¥æº: {url}\n{summary}"
        except Exception as e:
            logger.error(f"æµè§ˆURLå¤±è´¥ {url}: {e}")
            return f"#### æ¥æº: {url}\n\næ— æ³•è®¿é—®æˆ–å¤„ç†æ­¤é¡µé¢ã€‚"