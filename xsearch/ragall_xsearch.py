#!/usr/bin/env python3
"""
ğŸ§  xsearchä¸“ç”¨å…¨å±€çŸ¥è¯†åº“ç®¡ç†å·¥å…· - æ”¯æŒå‘é‡ç´¢å¼•å’ŒçŸ¥è¯†å›¾è°±
ç”¨æ³•: 
  python ragall_xsearch.py -f file1.md file2.pdf file3.txt                    # æ„å»ºå‘é‡ç´¢å¼•
  python ragall_xsearch.py -f file1.md file2.pdf file3.txt --kg              # æ„å»ºçŸ¥è¯†å›¾è°±
  python ragall_xsearch.py -f file1.md file2.pdf file3.txt --kg --vector     # åŒæ—¶æ„å»º
"""

import asyncio
import argparse
import sys
import json
import re
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    from backend.services.global_knowledge import global_knowledge
    from backend.services.knowledge_graph import performance_kg
    BACKEND_SERVICES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ backendæœåŠ¡ä¸å¯ç”¨: {e}")
    BACKEND_SERVICES_AVAILABLE = False


def _infer_domain_tags(file_name: str) -> list:
    name = file_name.lower()
    tags = []
    if any(k in name for k in ["æ³•", "è§„", "æ¡ä¾‹", "åŠæ³•", "æ”¿ç­–", "æ³•è§„"]):
        tags.append("æ”¿ç­–è§„èŒƒ")
    if any(k in name for k in ["æ ‡å‡†", "è§„èŒƒ", "æŒ‡å—"]):
        tags.append("æ ‡å‡†è§„èŒƒ")
    if any(k in name for k in ["æ–¹æ³•", "æ¨¡å‹", "æµç¨‹", "è¯„ä»·", "æ–¹æ³•è®º"]):
        tags.append("æ–¹æ³•è®º")
    if any(k in name for k in ["æ¨¡æ¿", "æ ·ä¾‹", "ç¤ºä¾‹", "èŒƒæœ¬"]):
        tags.append("æ¨¡æ¿")
    if not tags:
        tags.append("é€šç”¨")
    return tags


def _infer_year_and_version(file_name: str) -> tuple[int | None, str]:
    # å¹´ä»½ï¼šåŒ¹é…4ä½æ•°å­—
    year_match = re.search(r"(20\d{2}|19\d{2})", file_name)
    year = int(year_match.group(1)) if year_match else None
    # ç‰ˆæœ¬ï¼šä¸­æ–‡"ç¬¬Xç‰ˆ"â†’ vX
    version = "v1"
    mapping = {"ç¬¬ä¸€ç‰ˆ": "v1", "ç¬¬äºŒç‰ˆ": "v2", "ç¬¬ä¸‰ç‰ˆ": "v3", "ç¬¬å››ç‰ˆ": "v4", "ç¬¬äº”ç‰ˆ": "v5"}
    for zh, v in mapping.items():
        if zh in file_name:
            version = v
            break
    return year, version


def _write_sidecar_meta(target_path: Path, meta: dict) -> None:
    try:
        meta_path = target_path.with_suffix(target_path.suffix + ".meta.json")
        meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"ğŸ§¾ å…ƒæ•°æ®å·²å†™å…¥: {meta_path}")
    except Exception as e:
        print(f"âš ï¸ å†™å…¥å…ƒæ•°æ®å¤±è´¥ {target_path.name}: {e}")


async def build_global_knowledge_base(file_paths: list, build_vector: bool = True, build_kg: bool = False, chunk_size: int = 512, overlap: int = 50):
    """ğŸ§  ä»æŒ‡å®šæ–‡ä»¶æ„å»ºå…¨å±€çŸ¥è¯†åº“ï¼ˆå‘é‡ç´¢å¼• + çŸ¥è¯†å›¾è°±ï¼‰"""
    if not BACKEND_SERVICES_AVAILABLE:
        print("âŒ backendæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ„å»ºçŸ¥è¯†åº“")
        return False
    
    print(f"ğŸŒ æ„å»ºå…¨å±€çŸ¥è¯†åº“... å‘é‡ç´¢å¼•: {build_vector}, çŸ¥è¯†å›¾è°±: {build_kg}")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    valid_files = []
    for file_path in file_paths:
        path = Path(file_path)
        if path.exists():
            valid_files.append(str(path.absolute()))
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file_path}")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if not valid_files:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶ï¼Œé€€å‡º")
        return False
    
    success_vector = True
    success_kg = True
    
    # æ„å»ºå‘é‡ç´¢å¼•
    if build_vector:
        print("\nğŸ“Š æ„å»ºå‘é‡ç´¢å¼•çŸ¥è¯†åº“...")
        # æ·»åŠ æ–‡ä»¶åˆ°å…¨å±€çŸ¥è¯†åº“
        for file_path in valid_files:
            file_name = Path(file_path).name
            # æ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨åˆ†ç±»
            if any(keyword in file_name.lower() for keyword in ['æ³•', 'è§„', 'æ¡ä¾‹', 'åŠæ³•']):
                category = "laws"
            elif any(keyword in file_name.lower() for keyword in ['æ ‡å‡†', 'è§„èŒƒ', 'æŒ‡å—']):
                category = "standards"  
            elif any(keyword in file_name.lower() for keyword in ['æ¨¡æ¿', 'æ ·ä¾‹', 'ç¤ºä¾‹']):
                category = "templates"
            else:
                category = "general"
            
            # ç”Ÿæˆå…ƒæ•°æ®å¹¶å†™å…¥æ—è½¦æ–‡ä»¶ï¼ˆä¸åŸæ–‡ä»¶åŒç›®å½•ï¼‰
            stem = Path(file_path).stem
            domain_tags = _infer_domain_tags(file_name)
            year, version = _infer_year_and_version(file_name)
            meta = {
                "source": "global",
                "doc_id": stem,
                "domain_tags": domain_tags,
                "year": year,
                "version": version,
                "chunk_size": chunk_size,
                "overlap": overlap,
                "category": category,
            }
            _write_sidecar_meta(Path(file_path), meta)

            success = global_knowledge.add_global_document(file_path, category)
            if success:
                print(f"ğŸ“„ å·²æ·»åŠ : {file_name} -> {category}")
            else:
                print(f"âŒ æ·»åŠ å¤±è´¥: {file_name}")
        
        # æ„å»ºç´¢å¼•
        print("\nğŸ”§ æ„å»ºå…¨å±€å‘é‡ç´¢å¼•...")
        try:
            # è‹¥å…¨å±€çŸ¥è¯†å®ç°æ”¯æŒï¼Œä¼ å…¥åˆ‡åˆ†å‚æ•°ï¼›å¦åˆ™å›é€€
            success_vector = await global_knowledge.build_global_index(force_rebuild=True, chunk_size=chunk_size, overlap=overlap)
        except TypeError:
            success_vector = await global_knowledge.build_global_index(force_rebuild=True)
        
        if success_vector:
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = global_knowledge.get_global_stats()
            print(f"âœ… å…¨å±€å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ!")
            print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
            print(f"ğŸ“ åˆ†ç±»ç»Ÿè®¡: {stats['categories']}")
        else:
            print("âŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥")
    
    # æ„å»ºçŸ¥è¯†å›¾è°±
    if build_kg:
        print("\nğŸ§  æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # ä¸ºçŸ¥è¯†å›¾è°±å‡†å¤‡å…¨å±€å­˜å‚¨è·¯å¾„
        global_kg_path = "workspace/vector_storage/global"
        Path(global_kg_path).mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶åˆ°å…¨å±€çŸ¥è¯†å›¾è°±ç›®å½•
        for file_path in valid_files:
            file_name = Path(file_path).name
            target_path = Path(global_kg_path) / file_name
            
            try:
                import shutil
                shutil.copy2(file_path, target_path)
                print(f"ğŸ“„ å·²å¤åˆ¶æ–‡ä»¶åˆ°çŸ¥è¯†å›¾è°±ç›®å½•: {file_name}")
                # åŒæ­¥å†™å…¥KGä¾§å…ƒæ•°æ®æ—è½¦æ–‡ä»¶
                stem = Path(file_path).stem
                domain_tags = _infer_domain_tags(file_name)
                year, version = _infer_year_and_version(file_name)
                meta = {
                    "source": "global",
                    "doc_id": stem,
                    "domain_tags": domain_tags,
                    "year": year,
                    "version": version,
                }
                _write_sidecar_meta(target_path, meta)
            except Exception as e:
                print(f"âŒ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {file_name}: {e}")
        
        # æ„å»ºçŸ¥è¯†å›¾è°±
        success_kg = await performance_kg.build_knowledge_graph(global_kg_path)
        
        if success_kg:
            print("âœ… å…¨å±€çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ!")
            
            # ğŸ§  æ¼”ç¤ºçŸ¥è¯†å›¾è°±çš„æ¨ç†èƒ½åŠ›
            print("\nğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†èƒ½åŠ›æ¼”ç¤º:")
            test_queries = [
                "ç»©æ•ˆè¯„ä»·æŒ‡æ ‡ä½“ç³»åº”è¯¥åŒ…å«å“ªäº›ç»´åº¦ï¼Ÿ",
                "é¡¹ç›®å®æ–½è¿‡ç¨‹ä¸­å¸¸è§çš„é£é™©æœ‰å“ªäº›ï¼Ÿ",
                "å¦‚ä½•ç¡®ä¿é¡¹ç›®èµ„é‡‘ä½¿ç”¨çš„åˆè§„æ€§ï¼Ÿ"
            ]
            
            for query in test_queries:
                print(f"\nğŸ¤” æŸ¥è¯¢: {query}")
                try:
                    result = await performance_kg.query_knowledge_graph(query, mode="keyword", max_knowledge_sequence=3)
                    print(f"ğŸ§  æ¨ç†ç»“æœ: {result[:200]}..." if len(result) > 200 else f"ğŸ§  æ¨ç†ç»“æœ: {result}")
                except Exception as e:
                    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        else:
            print("âŒ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥")
    
    return success_vector and success_kg


async def main():
    parser = argparse.ArgumentParser(
        description="ğŸ§  xsearchä¸“ç”¨å…¨å±€çŸ¥è¯†åº“ç®¡ç†å·¥å…· - æ”¯æŒå‘é‡ç´¢å¼•å’ŒçŸ¥è¯†å›¾è°±",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸš€ ä½¿ç”¨ç¤ºä¾‹:
  # é»˜è®¤æ„å»ºå‘é‡ç´¢å¼•
  python ragall_xsearch.py -f workspace/vector_storage/*.md
  
  # æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆæ¨èï¼ï¼‰
  python ragall_xsearch.py -f workspace/vector_storage/*.md --kg
  
  # åŒæ—¶æ„å»ºå‘é‡ç´¢å¼•å’ŒçŸ¥è¯†å›¾è°±
  python ragall_xsearch.py -f workspace/vector_storage/*.md --kg --vector
  
  # ä»…æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œä¸æ„å»ºå‘é‡ç´¢å¼•
  python ragall_xsearch.py -f workspace/vector_storage/*.md --kg --no-vector
        """
    )
    
    parser.add_argument(
        '-f', '--files', 
        nargs='+', 
        required=True,
        help='è¦æ·»åŠ åˆ°å…¨å±€çŸ¥è¯†åº“çš„æ–‡ä»¶åˆ—è¡¨'
    )
    
    parser.add_argument(
        '--kg', '--knowledge-graph',
        action='store_true',
        help='ğŸ§  æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆæ¨èï¼æ”¯æŒæ¨ç†å¼æŸ¥è¯¢ï¼‰'
    )
    
    parser.add_argument(
        '--vector',
        action='store_true',
        help='ğŸ“Š æ„å»ºå‘é‡ç´¢å¼•ï¼ˆä¼ ç»ŸRAGæ£€ç´¢ï¼‰'
    )
    
    parser.add_argument(
        '--no-vector',
        action='store_true',
        help='ğŸš« ä¸æ„å»ºå‘é‡ç´¢å¼•ï¼ˆä»…çŸ¥è¯†å›¾è°±ï¼‰'
    )
    
    parser.add_argument(
        '--chunk-size', type=int, default=512,
        help='å‘é‡ç´¢å¼•åˆ‡åˆ†å—å¤§å°ï¼Œé»˜è®¤512'
    )
    parser.add_argument(
        '--overlap', type=int, default=50,
        help='å‘é‡ç´¢å¼•åˆ‡åˆ†é‡å ï¼Œé»˜è®¤50'
    )

    args = parser.parse_args()
    
    # ç¡®å®šæ„å»ºé€‰é¡¹
    if args.no_vector:
        build_vector = False
        build_kg = args.kg or True  # å¦‚æœç¦ç”¨vectorï¼Œé»˜è®¤å¯ç”¨kg
    elif args.vector and args.kg:
        build_vector = True
        build_kg = True
    elif args.kg:
        build_vector = False
        build_kg = True
    else:
        # é»˜è®¤è¡Œä¸ºï¼šä»…æ„å»ºå‘é‡ç´¢å¼•
        build_vector = True
        build_kg = False
    
    print(f"ğŸ¯ æ„å»ºé…ç½®: å‘é‡ç´¢å¼•={build_vector}, çŸ¥è¯†å›¾è°±={build_kg}")
    
    success = await build_global_knowledge_base(
        args.files,
        build_vector=build_vector,
        build_kg=build_kg,
        chunk_size=args.chunk_size,
        overlap=args.overlap,
    )
    
    if success:
        print("\nğŸ‰ å…¨å±€çŸ¥è¯†åº“å·²å‡†å¤‡å°±ç»ªï¼")
        if build_vector:
            print("ğŸ“Š å‘é‡æ£€ç´¢åŠŸèƒ½å¯ç”¨")
        if build_kg:
            print("ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†åŠŸèƒ½å¯ç”¨")
        sys.exit(0)
    else:
        print("\nğŸ’¥ æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å’Œé…ç½®")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
