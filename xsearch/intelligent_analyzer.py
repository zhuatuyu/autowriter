#!/usr/bin/env python3
"""
æ™ºèƒ½åˆ†æå™¨æ ¸å¿ƒç±»
å®ç°å®Œæ•´çš„æ™ºèƒ½åˆ†ææµç¨‹ï¼šæ„å›¾ç†è§£ -> ç­–ç•¥ç”Ÿæˆ -> æ£€ç´¢æå– -> LLMè¯„ä»·
"""

import asyncio
import json
import os
from pathlib import Path
from typing import Dict, Any, List

# Google APIç¯å¢ƒä»…åœ¨LangExtractä½¿ç”¨æ—¶è®¾ç½®

try:
    import langextract as lx
    LANGEXTRACT_AVAILABLE = True
except ImportError:
    LANGEXTRACT_AVAILABLE = False
    print("âŒ LangExtractæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install langextract")

from .llm_client import LLMClient
from .vector_searcher import VectorSearcher
from .knowledge_graph_client import KnowledgeGraphClient

# å¯¼å…¥é…ç½®å¸¸é‡
from .config_constants import (
    LANGEXTRACT_CONFIG,
    VECTOR_SEARCH_CONFIG,
    EVALUATION_CONFIG,
    INTENT_ANALYSIS_CONFIG
)


class IntelligentAnalyzer:
    """æ™ºèƒ½åˆ†æå™¨"""
    
    def __init__(self, project_config: Dict[str, Any]):
        self.project_config = project_config
        self.llm_client = LLMClient(project_config)
        self.vector_searcher = VectorSearcher(project_config)
        self.kg_client = KnowledgeGraphClient(project_config)
        
        # åœ¨åˆå§‹åŒ–æ—¶æ„å»ºæœ¬åœ°å‘é‡ç´¢å¼•
        self._init_local_vector_index()
    
    def _init_local_vector_index(self):
        """åˆå§‹åŒ–æœ¬åœ°å‘é‡ç´¢å¼•"""
        try:
            print("ğŸ”§ åˆå§‹åŒ–æœ¬åœ°å‘é‡ç´¢å¼•...")
            # è¿™é‡Œåªæ˜¯åˆå§‹åŒ–ï¼Œå®é™…çš„æ„å»ºä¼šåœ¨ç¬¬ä¸€æ¬¡æœç´¢æ—¶è¿›è¡Œ
            stats = self.vector_searcher.local_vector_service.get_stats()
            print(f"ğŸ“Š æœ¬åœ°å‘é‡åŒ–æœåŠ¡çŠ¶æ€:")
            print(f"   æ–‡æ¡£ç›®å½•: {stats['doc_dir']}")
            print(f"   ç´¢å¼•ç›®å½•: {stats['index_dir']}")
            print(f"   æ–‡æ¡£æ•°é‡: {stats['doc_count']}")
            print(f"   ç´¢å¼•å­˜åœ¨: {stats['index_exists']}")
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ–æœ¬åœ°å‘é‡ç´¢å¼•å¤±è´¥: {e}")
        
    async def analyze_query(self, user_query: str) -> Dict[str, Any]:
        """å®Œæ•´çš„æ™ºèƒ½åˆ†ææµç¨‹"""
        
        print("ğŸ” é˜¶æ®µ1: åŠ¨æ€ç†è§£æŸ¥è¯¢æ„å›¾...")
        intent_analysis = await self._analyze_query_intent(user_query)
        
        print("ğŸ”§ é˜¶æ®µ2: åŠ¨æ€ç”Ÿæˆæ£€ç´¢ç­–ç•¥...")
        search_strategy = await self._generate_dynamic_search_strategy(intent_analysis)
        
        print("ğŸ“Š é˜¶æ®µ3: åŠ¨æ€æ‰§è¡Œæ£€ç´¢ä¸æå–...")
        project_data, global_methods, extracted_data = await self._dynamic_hybrid_search(search_strategy)
        
        print("ğŸ§  é˜¶æ®µ4: åŠ¨æ€ç”Ÿæˆè¯„ä»·æç¤ºè¯...")
        evaluation_prompt = await self._generate_dynamic_evaluation_prompt(
            extracted_data, global_methods, search_strategy, user_query
        )
        
        print("ğŸ’¬ é˜¶æ®µ5: LLMç”Ÿæˆè¯„ä»·ç»“æœ...")
        evaluation_result = await self.llm_client.generate_evaluation(evaluation_prompt)
        
        # è¿”å›å®Œæ•´ç»“æœ
        return {
            "user_query": user_query,
            "intent_analysis": intent_analysis,
            "search_strategy": search_strategy,
            "extracted_data": extracted_data.extractions[0].attributes if extracted_data.extractions else {},
            "evaluation_result": evaluation_result,
            "data_sources": {
                "project_docs": len(project_data),
                "global_methods": len(global_methods)
            },
            "evaluation_prompt": evaluation_prompt  # ç”¨äºè°ƒè¯•
        }
    
    async def _analyze_query_intent(self, user_query: str) -> Dict[str, Any]:
        """åŠ¨æ€åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼ŒåŸºäºå…·ä½“é¡¹ç›®èƒŒæ™¯"""
        
        # è·å–é¡¹ç›®ä¿¡æ¯
        project_name = self.project_config.get('project_name', 'æœªçŸ¥é¡¹ç›®')
        project_type = self.project_config.get('project_type', 'æœªçŸ¥ç±»å‹')
        province = self.project_config.get('province', '')
        city = self.project_config.get('city', '')
        county = self.project_config.get('county', '')
        project_description = self.project_config.get('project_description', '')
        
        intent_analysis_prompt = f"""
        è¯·åŸºäºä»¥ä¸‹å…·ä½“é¡¹ç›®èƒŒæ™¯ï¼Œåˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„åˆ†æç»“æœï¼š

        ## é¡¹ç›®èƒŒæ™¯ä¿¡æ¯
        - é¡¹ç›®åç§°ï¼š{project_name}
        - é¡¹ç›®ç±»å‹ï¼š{project_type}
        - åœ°ç†ä½ç½®ï¼š{province}{city}{county}
        - é¡¹ç›®æè¿°ï¼š{project_description}

        ## ç”¨æˆ·æŸ¥è¯¢
        {user_query}

        è¯·ç»“åˆä¸Šè¿°é¡¹ç›®èƒŒæ™¯ï¼Œä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š
        1. æŸ¥è¯¢çš„æ ¸å¿ƒä¸»é¢˜ï¼ˆè¦ç»“åˆå…·ä½“é¡¹ç›®ç±»å‹å’Œç‰¹ç‚¹ï¼‰
        2. éœ€è¦çš„æ•°æ®ç±»å‹ï¼ˆè¦ç»“åˆé¡¹ç›®å®é™…ï¼‰
        3. æœŸæœ›çš„åˆ†æç»´åº¦ï¼ˆè¦ç»“åˆé¡¹ç›®ç±»å‹å’Œæè¿°ï¼‰
        4. é€‚åˆçš„è¯„ä»·ç»“æ„ï¼ˆè¦ç»“åˆé¡¹ç›®ç‰¹ç‚¹ï¼‰
        5. æ£€ç´¢å…³é”®è¯ï¼ˆè¦ç»“åˆé¡¹ç›®å…·ä½“å†…å®¹ï¼‰
        6. æå–å­—æ®µï¼ˆè¦ç»“åˆé¡¹ç›®å®é™…ï¼‰

        è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ ¼å¼å®Œå…¨æ­£ç¡®ï¼š
        {{
            "core_topic": "æ ¸å¿ƒä¸»é¢˜",
            "data_requirements": ["éœ€è¦çš„æ•°æ®ç±»å‹1", "éœ€è¦çš„æ•°æ®ç±»å‹2"],
            "analysis_dimensions": ["åˆ†æç»´åº¦1", "åˆ†æç»´åº¦2"],
            "evaluation_structure": ["è¯„ä»·è¦ç‚¹1", "è¯„ä»·è¦ç‚¹2"],
            "search_keywords": ["æ£€ç´¢å…³é”®è¯1", "æ£€ç´¢å…³é”®è¯2"],
            "extraction_fields": ["æå–å­—æ®µ1", "æå–å­—æ®µ2"]
        }}
        
        åˆ†ææ—¶è¦å……åˆ†è€ƒè™‘é¡¹ç›®èƒŒæ™¯ï¼Œä½¿ç»“æœæ›´åŠ ç²¾å‡†å’Œå®ç”¨ã€‚
        """
        
        intent_result = await self.llm_client.analyze_intent(intent_analysis_prompt)
        
        try:
            # å°è¯•è§£æJSON
            if isinstance(intent_result, str):
                intent_analysis = json.loads(intent_result)
            else:
                intent_analysis = intent_result
                
            print(f"âœ… æŸ¥è¯¢æ„å›¾åˆ†æå®Œæˆï¼š{intent_analysis['core_topic']}")
            return intent_analysis
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„: {e}")
            # è¿”å›é»˜è®¤ç»“æ„
            return {
                "core_topic": INTENT_ANALYSIS_CONFIG["DEFAULT_CORE_TOPIC"],
                "data_requirements": INTENT_ANALYSIS_CONFIG["DEFAULT_DATA_REQUIREMENTS"],
                "analysis_dimensions": INTENT_ANALYSIS_CONFIG["DEFAULT_ANALYSIS_DIMENSIONS"],
                "evaluation_structure": EVALUATION_CONFIG["DEFAULT_STRUCTURE"],
                "search_keywords": EVALUATION_CONFIG["DEFAULT_SEARCH_KEYWORDS"],
                "extraction_fields": EVALUATION_CONFIG["DEFAULT_EXTRACTION_FIELDS"]
            }
    
    async def _generate_dynamic_search_strategy(self, intent_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºæ„å›¾åˆ†æåŠ¨æ€ç”Ÿæˆæ£€ç´¢ç­–ç•¥"""
        
        # åŠ¨æ€ç”Ÿæˆé¡¹ç›®æ£€ç´¢æŸ¥è¯¢
        project_query = " ".join(intent_analysis["search_keywords"])
        
        # åŠ¨æ€ç”Ÿæˆå…¨å±€æ£€ç´¢æŸ¥è¯¢
        global_query = f"{intent_analysis['core_topic']} è¯„ä»·è§„åˆ™è¦ç‚¹ æŒ‡æ ‡è®¾è®¡ æ”¿ç­–æ³•è§„ä¾æ®"
        
        # åŠ¨æ€ç”ŸæˆLangExtractæç¤ºè¯
        extraction_prompt = f"""
        ä»é¡¹ç›®æ–‡æ¡£ä¸­æå–ä¸"{intent_analysis['core_topic']}"ç›¸å…³çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        {chr(10).join([f"- {field}" for field in intent_analysis['extraction_fields']])}
        
        è¯·ç¡®ä¿æå–çš„ä¿¡æ¯å‡†ç¡®ã€å®Œæ•´ï¼Œèƒ½å¤Ÿæ”¯æŒåç»­çš„åˆ†æè¯„ä»·ã€‚
        """
        
        # åŠ¨æ€ç”Ÿæˆè¯„ä»·ç»“æ„
        evaluation_structure = intent_analysis['evaluation_structure']
        
        return {
            "project_query": project_query,
            "global_query": global_query,
            "extraction_prompt": extraction_prompt,
            "evaluation_structure": evaluation_structure
        }
    
    async def _dynamic_hybrid_search(
        self, 
        search_strategy: Dict[str, Any]
    ):
        """æŒ‰éœ€å‡†ç¡®æœç´¢ï¼šç›´æ¥è°ƒç”¨ç›¸åº”çš„æœç´¢æœåŠ¡"""
        
        # 1. é¡¹ç›®æ•°æ®æ£€ç´¢ï¼ˆç›´æ¥æœç´¢ï¼‰
        project_data = await self.vector_searcher.search_project(
            search_strategy["project_query"]
        )
        
        # 2. å…¨å±€æ–¹æ³•æ£€ç´¢ï¼ˆç›´æ¥æœç´¢å·²æ„å»ºçš„ç´¢å¼•ï¼‰
        global_methods = await self.vector_searcher.search_global(
            search_strategy["global_query"]
        )
        
        # 3. åŠ¨æ€LangExtractæå–
        if LANGEXTRACT_AVAILABLE and project_data:
            extracted_data = await self._extract_with_langextract(
                project_data, search_strategy["extraction_prompt"]
            )
        else:
            # å¦‚æœæ²¡æœ‰LangExtractæˆ–æ²¡æœ‰é¡¹ç›®æ•°æ®ï¼Œåˆ›å»ºç©ºç»“æœ
            extracted_data = self._create_empty_extraction_result()
        
        return project_data, global_methods, extracted_data
    
    async def _extract_with_langextract(self, project_data: List[str], extraction_prompt: str):
        """ä½¿ç”¨LangExtractè¿›è¡Œä¿¡æ¯æå–"""
        
        try:
            # ä»…åœ¨LangExtractä½¿ç”¨æ—¶è®¾ç½®Google APIç¯å¢ƒ
            os.environ['GOOGLE_API_KEY'] = 'AIzaSyA-gjWRxk6Y4DUQxIuKtF3R_tp8cjF28gs'
            os.environ['GOOGLE_GENERATIVE_AI_API_KEY'] = 'AIzaSyA-gjWRxk6Y4DUQxIuKtF3R_tp8cjF28gs'
            os.environ['GEMINI_API_KEY'] = 'AIzaSyA-gjWRxk6Y4DUQxIuKtF3R_tp8cjF28gs'
            
            # åˆå¹¶é¡¹ç›®æ•°æ®
            combined_text = "\n".join(project_data[:LANGEXTRACT_CONFIG["MAX_DOCS_TO_PROCESS"]])  # ä½¿ç”¨é…ç½®å¸¸é‡æ§åˆ¶å¤„ç†æ–‡æ¡£æ•°
            
            # åˆ›å»ºç¤ºä¾‹
            examples = self._create_dynamic_examples(extraction_prompt)
            
            # è°ƒç”¨LangExtract
            result = lx.extract(
                text_or_documents=combined_text,
                prompt_description=extraction_prompt,
                examples=examples,
                model_id=LANGEXTRACT_CONFIG["MODEL_ID"],
                extraction_passes=LANGEXTRACT_CONFIG["EXTRACTION_PASSES"],
                max_workers=LANGEXTRACT_CONFIG["MAX_WORKERS"],
                max_char_buffer=LANGEXTRACT_CONFIG["MAX_CHAR_BUFFER"]
            )
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ LangExtractæå–å¤±è´¥: {e}")
            return self._create_empty_extraction_result()
    
    def _create_dynamic_examples(self, extraction_prompt: str):
        """åŠ¨æ€åˆ›å»ºç¤ºä¾‹"""
        return [
            lx.data.ExampleData(
                text="è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æ¡£ï¼ŒåŒ…å«é¡¹ç›®çš„åŸºæœ¬ä¿¡æ¯å’Œå…³é”®æŒ‡æ ‡ã€‚",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="document_summary",
                        extraction_text="ç¤ºä¾‹æ‘˜è¦",
                        attributes={
                            "åŸºæœ¬ä¿¡æ¯": "ç¤ºä¾‹åŸºæœ¬ä¿¡æ¯",
                            "å…³é”®æŒ‡æ ‡": "ç¤ºä¾‹å…³é”®æŒ‡æ ‡",
                            "é—®é¢˜æè¿°": "ç¤ºä¾‹é—®é¢˜æè¿°"
                        }
                    )
                ]
            )
        ]
    
    def _create_empty_extraction_result(self):
        """åˆ›å»ºç©ºçš„æå–ç»“æœ"""
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„AnnotatedDocumentå¯¹è±¡
        class MockExtraction:
            def __init__(self):
                self.extractions = []
        
        return MockExtraction()
    
    async def _generate_dynamic_evaluation_prompt(
        self,
        extracted_data: Any,
        global_methods: List[str],
        search_strategy: Dict[str, Any],
        original_query: str
    ):
        """åŠ¨æ€ç”Ÿæˆè¯„ä»·æç¤ºè¯"""
        
        # æ„å»ºè¯„ä»·æç¤ºè¯
        evaluation_prompt = f"""
        åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·æŸ¥è¯¢ï¼š{original_query}
        
        ## æå–çš„é¡¹ç›®æ•°æ®
        {json.dumps(extracted_data.extractions[0].attributes if extracted_data.extractions else {}, ensure_ascii=False, indent=2)}
        
        ## ç›¸å…³è¯„ä»·æ ‡å‡†å’Œæ–¹æ³•
        {chr(10).join(global_methods[:VECTOR_SEARCH_CONFIG["GLOBAL_METHODS_DISPLAY_LIMIT"]])}
        
        è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç»™å‡ºåˆ†æè¯„ä»·ï¼š
        {chr(10).join([f"{i+1}. {point}" for i, point in enumerate(search_strategy['evaluation_structure'])])}
        
        è¦æ±‚ï¼š
        1. åˆ†æè¦åŸºäºæå–çš„å®é™…æ•°æ®
        2. è¯„ä»·è¦ç»“åˆç›¸å…³æ ‡å‡†å’Œæ–¹æ³•
        3. ç»™å‡ºå…·ä½“çš„è¯„ä»·æ„è§å’Œä¾æ®
        4. æä¾›å¯æ“ä½œçš„æ”¹è¿›å»ºè®®
        """
        
        return evaluation_prompt
