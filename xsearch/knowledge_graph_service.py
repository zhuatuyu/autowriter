#!/usr/bin/env python3
"""
xsearchä¸“ç”¨çŸ¥è¯†å›¾è°±æœåŠ¡
åŸºäºLlamaIndex KnowledgeGraphIndexï¼Œæ„å»ºé¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    from llama_index.core import KnowledgeGraphIndex, Settings
    from llama_index.core.storage.storage_context import StorageContext
    from llama_index.core.graph_stores import SimpleGraphStore
    from llama_index.core.query_engine import KnowledgeGraphQueryEngine
    KG_AVAILABLE = True
    print("âœ… çŸ¥è¯†å›¾è°±ä¾èµ–åŠ è½½æˆåŠŸ")
except ImportError as e:
    KG_AVAILABLE = False
    print(f"âš ï¸ çŸ¥è¯†å›¾è°±ä¾èµ–ä¸å¯ç”¨: {e}ï¼Œå°†ä½¿ç”¨ä¼ ç»ŸRAGæ£€ç´¢")

try:
    from llama_index.llms.openai_like import OpenAILike
    from llama_index.embeddings.dashscope import DashScopeEmbedding
    MODELS_AVAILABLE = True
except ImportError as e:
    MODELS_AVAILABLE = False
    print(f"âš ï¸ æ¨¡å‹ä¾èµ–ä¸å¯ç”¨: {e}")


class XSearchKnowledgeGraph:
    """xsearchä¸“ç”¨çŸ¥è¯†å›¾è°±æœåŠ¡"""
    
    def __init__(self):
        self._kg_index = None
        self._kg_storage_path = "workspace/vector_storage/global_graph"
        
        # ğŸ¯ ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„å®ä½“/å…³ç³»ï¼ˆä»global_promptså¤åˆ¶ï¼‰
        self.entity_types = {
            "é¡¹ç›®": ["é¡¹ç›®åç§°", "é¡¹ç›®ç±»å‹", "å®æ–½åœ°ç‚¹", "èµ„é‡‘è§„æ¨¡"],
            "æŒ‡æ ‡ä½“ç³»": ["å†³ç­–æŒ‡æ ‡", "è¿‡ç¨‹æŒ‡æ ‡", "äº§å‡ºæŒ‡æ ‡", "æ•ˆç›ŠæŒ‡æ ‡"],
            "å…·ä½“æŒ‡æ ‡": ["æŒ‡æ ‡åç§°", "è®¡ç®—æ–¹æ³•", "ç›®æ ‡å€¼", "æƒé‡"],
            "æ”¿ç­–æ³•è§„": ["æ³•è§„åç§°", "é€‚ç”¨èŒƒå›´", "å‘å¸ƒæœºæ„", "ç”Ÿæ•ˆæ—¶é—´"],
            "æœ€ä½³å®è·µ": ["å®è·µåç§°", "é€‚ç”¨åœºæ™¯", "å®æ–½è¦ç‚¹", "é¢„æœŸæ•ˆæœ"],
            "é—®é¢˜æ¡ˆä¾‹": ["é—®é¢˜ç±»å‹", "åŸå› åˆ†æ", "è§£å†³æ–¹æ¡ˆ", "æ”¹è¿›å»ºè®®"],
            "è¡Œä¸šç±»å‹": ["åŸºç¡€è®¾æ–½", "å…¬ç›Šäº‹ä¸š", "æ°‘ç”Ÿä¿éšœ", "ç¯å¢ƒæ²»ç†"],
        }
        
        self.relation_types = [
            "åŒ…å«", "å±äº", "é€‚ç”¨äº", "éµå¾ª", "å‚è€ƒ",
            "å¯¼è‡´", "è§£å†³", "æ”¹è¿›", "å…³è”", "å½±å“"
        ]
    
    def _get_config(self) -> Dict[str, Any]:
        """è·å–é…ç½®"""
        try:
            config_path = Path(project_root) / 'config' / 'config2.yaml'
            if config_path.exists():
                import yaml
                with open(config_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
            else:
                print("âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
                return {}
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def _create_kg_llm_and_embed_model(self):
        """åˆ›å»ºçŸ¥è¯†å›¾è°±ä¸“ç”¨çš„LLMå’ŒåµŒå…¥æ¨¡å‹"""
        if not MODELS_AVAILABLE:
            print("âš ï¸ æ¨¡å‹ä¾èµ–ä¸å¯ç”¨")
            return None, None
        
        try:
            config = self._get_config()
            kg_config = config.get('knowledge_graph', {})
            embed_config = config.get('embedding', {})
            
            # åˆ›å»ºçŸ¥è¯†å›¾è°±ä¸“ç”¨LLM
            llm = OpenAILike(
                model=kg_config.get('model', 'qwen-flash'),
                api_base=kg_config.get('base_url', 'https://dashscope.aliyuncs.com/compatible-mode/v1'),
                api_key=kg_config.get('api_key', ''),
                is_chat_model=True
            )
            
            # åˆ›å»ºEmbeddingæ¨¡å‹
            embed_model = DashScopeEmbedding(
                model_name=embed_config.get('model', 'text-embedding-v3'),
                api_key=embed_config.get('api_key', ''),
                dashscope_api_key=embed_config.get('api_key', '')
            )
            embed_model.embed_batch_size = 8
            
            return llm, embed_model
            
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    async def build_knowledge_graph(self, project_vector_storage_path: str) -> bool:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        if not KG_AVAILABLE:
            print("âš ï¸ çŸ¥è¯†å›¾è°±åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡æ„å»º")
            return False
            
        try:
            print("ğŸ§  å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            
            # 1. æ”¶é›†æ–‡æ¡£
            documents = self._collect_documents(project_vector_storage_path)
            if not documents:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºçŸ¥è¯†å›¾è°±")
                return False
            
            # 2. åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = self._create_storage_context()
            
            # 3. è®¾ç½®ä¸“ç”¨çš„çŸ¥è¯†å›¾è°±LLMå’Œembeddingæ¨¡å‹
            llm, embed_model = self._create_kg_llm_and_embed_model()
            if not llm or not embed_model:
                print("âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥")
                return False
            
            Settings.llm = llm
            Settings.embed_model = embed_model
            Settings.chunk_size = 512
            
            print(f"ğŸ§  çŸ¥è¯†å›¾è°±ä½¿ç”¨LLM: {type(llm).__name__}")
            print(f"ğŸ§  çŸ¥è¯†å›¾è°±ä½¿ç”¨Embedding: {type(embed_model).__name__}")
            
            # 4. æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•
            self._kg_index = KnowledgeGraphIndex.from_documents(
                documents=documents,
                storage_context=storage_context,
                max_triplets_per_chunk=10,
                show_progress=True,
            )
            
            # 5. ä¿å­˜çŸ¥è¯†å›¾è°±
            self._kg_index.storage_context.persist(self._kg_storage_path)
            
            print(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {self._kg_storage_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return False
    
    async def query_knowledge_graph(
        self, 
        query: str, 
        mode: str = "keyword",
        max_knowledge_sequence: int = 5
    ) -> str:
        """æŸ¥è¯¢çŸ¥è¯†å›¾è°±"""
        if not KG_AVAILABLE:
            print("âš ï¸ çŸ¥è¯†å›¾è°±åŠŸèƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°å‘é‡æ£€ç´¢")
            return "çŸ¥è¯†å›¾è°±åŠŸèƒ½ä¸å¯ç”¨"
            
        try:
            if not self._kg_index:
                print("âš ï¸ çŸ¥è¯†å›¾è°±æœªæ„å»ºï¼Œå°è¯•åŠ è½½...")
                if not await self._load_knowledge_graph():
                    return "çŸ¥è¯†å›¾è°±ä¸å¯ç”¨ï¼Œè¯·å…ˆæ„å»ºçŸ¥è¯†å›¾è°±"
            
            # åˆ›å»ºçŸ¥è¯†å›¾è°±æŸ¥è¯¢å¼•æ“
            kg_query_engine = self._kg_index.as_query_engine(
                include_text=True,
                retriever_mode=mode,
                response_mode="tree_summarize",
                similarity_top_k=max_knowledge_sequence,
                verbose=True,
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = await kg_query_engine.aquery(query)
            
            # åå¤„ç†
            enhanced_response = await self._post_process_kg_response(str(response), query)
            
            print(f"ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†æŸ¥è¯¢å®Œæˆ: {query}")
            return enhanced_response
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†å›¾è°±æŸ¥è¯¢å¤±è´¥: {e}")
            return f"æŸ¥è¯¢å¤±è´¥: {e}"
    
    def _collect_documents(self, project_vector_storage_path: str) -> List[Any]:
        """æ”¶é›†é¡¹ç›®æ–‡æ¡£"""
        from llama_index.core import Document
        
        documents = []
        project_path = Path(project_vector_storage_path)
        
        if not project_path.exists():
            return documents
        
        # æ”¶é›†æ‰€æœ‰.mdå’Œ.txtæ–‡ä»¶
        for pattern in ["*.md", "*.txt"]:
            for file_path in project_path.glob(pattern):
                try:
                    content = file_path.read_text(encoding='utf-8')
                    doc = Document(text=content, metadata={"filename": file_path.name})
                    documents.append(doc)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        print(f"ğŸ“š æ”¶é›†åˆ° {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±")
        return documents
    
    def _create_storage_context(self):
        """åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡"""
        if not KG_AVAILABLE:
            return None
            
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        Path(self._kg_storage_path).mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå›¾å­˜å‚¨
        graph_store = SimpleGraphStore()
        storage_context = StorageContext.from_defaults(graph_store=graph_store)
        
        return storage_context
    
    async def _load_knowledge_graph(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„çŸ¥è¯†å›¾è°±"""
        if not KG_AVAILABLE:
            return False
            
        try:
            if not Path(self._kg_storage_path).exists():
                return False
            
            # é…ç½®å…¨å±€è®¾ç½®
            llm, embed_model = self._create_kg_llm_and_embed_model()
            if not llm or not embed_model:
                return False
            
            Settings.llm = llm
            Settings.embed_model = embed_model
            Settings.chunk_size = 512
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(persist_dir=self._kg_storage_path)
            
            # åŠ è½½çŸ¥è¯†å›¾è°±
            from llama_index.core import load_index_from_storage
            self._kg_index = load_index_from_storage(
                storage_context=storage_context,
            )
            
            print(f"âœ… çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸ: {self._kg_storage_path}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return False
    
    async def _post_process_kg_response(self, response: str, original_query: str) -> str:
        """çŸ¥è¯†å›¾è°±å“åº”çš„æ™ºèƒ½åå¤„ç†"""
        # æ·»åŠ æ¨ç†è·¯å¾„è¯´æ˜
        enhanced_response = f"### ğŸ§  çŸ¥è¯†å›¾è°±æ¨ç†ç»“æœ\n\n{response}\n\n"
        
        # å°è¯•æå–å®ä½“å…³ç³»
        entities = self.extract_domain_entities(response)
        if entities:
            enhanced_response += "### ğŸ•¸ï¸ å‘ç°çš„å®ä½“å…³ç³»\n"
            for entity_type, entity_list in entities.items():
                enhanced_response += f"- **{entity_type}**: {', '.join(entity_list)}\n"
        
        # ç”Ÿæˆé¢†åŸŸæ´å¯Ÿ
        insights = self.generate_performance_insights(entities)
        if insights:
            enhanced_response += "\n### ğŸ’¡ ç»©æ•ˆåˆ†ææ´å¯Ÿ\n"
            for insight in insights:
                enhanced_response += f"- {insight}\n"
        
        return enhanced_response
    
    def extract_domain_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–ç»©æ•ˆåˆ†ææŠ¥å‘Šé¢†åŸŸçš„ç‰¹å®šå®ä½“"""
        entities = {}
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        for entity_type, keywords in self.entity_types.items():
            found_entities = []
            for keyword in keywords:
                if keyword in text:
                    found_entities.append(keyword)
            if found_entities:
                entities[entity_type] = found_entities
        
        return entities
    
    def generate_performance_insights(self, entities: Dict[str, List[str]]) -> List[str]:
        """åŸºäºæå–çš„å®ä½“ç”Ÿæˆç»©æ•ˆåˆ†ææ´å¯Ÿ"""
        insights = []
        
        # ç”Ÿæˆé¢†åŸŸç‰¹å®šçš„æ´å¯Ÿ
        if "é¡¹ç›®" in entities and "æŒ‡æ ‡ä½“ç³»" in entities:
            insights.append("ğŸ¯ è¯¥é¡¹ç›®åº”å»ºç«‹å®Œæ•´çš„ç»©æ•ˆæŒ‡æ ‡ä½“ç³»ï¼Œæ¶µç›–å†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šå››ä¸ªç»´åº¦")
        
        if "æ”¿ç­–æ³•è§„" in entities:
            insights.append("ğŸ“‹ é¡¹ç›®å®æ–½åº”ä¸¥æ ¼éµå¾ªç›¸å…³æ”¿ç­–æ³•è§„è¦æ±‚")
        
        if "æœ€ä½³å®è·µ" in entities:
            insights.append("â­ å¯å€Ÿé‰´ç›¸å…³æœ€ä½³å®è·µç»éªŒï¼Œæå‡é¡¹ç›®å®æ–½æ•ˆæœ")
        
        return insights


# å…¨å±€å®ä¾‹
xsearch_kg = XSearchKnowledgeGraph()
