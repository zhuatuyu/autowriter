#!/usr/bin/env python3
"""
ç®€å•æµ‹è¯•å…¨å±€çŸ¥è¯†åº“åŸºæœ¬åŠŸèƒ½
"""

import asyncio
import sys
sys.path.append('.')

from backend.services.global_knowledge import global_knowledge


async def simple_test():
    """ç®€å•æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹ç®€å•æµ‹è¯•...")
    
    # ç›´æ¥åœ¨å†…å­˜ä¸­æ„å»ºï¼Œä¸æŒä¹…åŒ–
    try:
        # ä¿®æ”¹å…¨å±€çŸ¥è¯†åº“ç®¡ç†å™¨ï¼Œæš‚æ—¶è·³è¿‡æŒä¹…åŒ–
        global_knowledge._global_engine = None
        
        # è·å–å…¨å±€æ–‡ä»¶
        global_files = global_knowledge.collect_global_files()
        print(f"ğŸ“ æ‰¾åˆ° {len(global_files)} ä¸ªå…¨å±€æ–‡ä»¶")
        
        if global_files:
            print("âœ… å…¨å±€çŸ¥è¯†åº“æ–‡ä»¶æ”¶é›†æˆåŠŸ")
            for f in global_files:
                print(f"  - {f}")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å…¨å±€çŸ¥è¯†åº“æ–‡ä»¶")
            
        # æµ‹è¯•é…ç½®
        config = global_knowledge._get_config()
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {type(config)}")
        
        # æµ‹è¯•LLMå’Œembedæ¨¡å‹åˆ›å»º
        llm, embed_model = global_knowledge._create_llm_and_embed_model()
        print(f"âœ… LLMå’ŒåµŒå…¥æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"  - LLM: {type(llm)}")
        print(f"  - åµŒå…¥æ¨¡å‹: {type(embed_model)}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(simple_test())