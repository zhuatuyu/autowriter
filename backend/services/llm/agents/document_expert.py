"""
æ–‡æ¡£ä¸“å®¶Agent - æå¿ƒæ‚¦
è´Ÿè´£æ–‡æ¡£ç®¡ç†ã€æ ¼å¼åŒ–å’Œæ‘˜è¦
"""
import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from metagpt.actions import Action
from metagpt.schema import Message
from metagpt.logs import logger

from .base import BaseAgent
from backend.services.llm_provider import llm
from backend.tools.mineru_api_tool import mineru_tool
# å¯¼å…¥æ–°çš„æ‘˜è¦å·¥å…·
from backend.tools.summary_tool import summary_tool

# å¯¼å…¥æ–°çš„Promptæ¨¡å—
from backend.services.llm.prompts import document_expert_prompts

class DocumentProcessAction(Action):
    """æ–‡æ¡£å¤„ç†åŠ¨ä½œ"""
    
    async def run(self, file_path: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£æ–‡ä»¶"""
        try:
            # ä½¿ç”¨MinerUå·¥å…·å¤„ç†æ–‡æ¡£
            result = await mineru_tool.process_file(file_path)
            return result
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return {'error': str(e)}


class DocumentSummaryAction(Action):
    """æ–‡æ¡£æ‘˜è¦åŠ¨ä½œ"""
    
    async def run(self, filename: str, content: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        # ä½¿ç”¨æ–°çš„Promptæ¨¡å—
        prompt = document_expert_prompts.get_document_summary_prompt(filename, content, "æå¿ƒæ‚¦")
        
        try:
            summary = await llm.acreate_text(prompt)
            return summary.strip()
        except Exception as e:
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"


class DocumentExpertAgent(BaseAgent):
    """
    æ–‡æ¡£ä¸“å®¶Agent - æå¿ƒæ‚¦
    
    è§’è‰²å®šä½ï¼šåŠå…¬å®¤æ–‡ç§˜ï¼Œè´Ÿè´£æ‰€æœ‰æ–‡æ¡£çš„ç®¡ç†ã€å½’æ¡£ã€æ ¼å¼åŒ–å’Œæ‘˜è¦
    å°±åƒçœŸå®åŠå…¬å®¤ä¸­çš„æ–‡ç§˜ä¸€æ ·ï¼Œå¥¹è´Ÿè´£ï¼š
    - æ¥æ”¶å’Œæ•´ç†æ‰€æœ‰å®¢æˆ·æä¾›çš„æ–‡æ¡£
    - å°†å„ç§æ ¼å¼çš„æ–‡æ¡£è½¬æ¢ä¸ºæ ‡å‡†çš„Markdownæ ¼å¼
    - å»ºç«‹æ–‡æ¡£ç´¢å¼•å’Œåˆ†ç±»ä½“ç³»
    - æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆæ‘˜è¦
    - ä¸ºå…¶ä»–åŒäº‹æä¾›æ ¼å¼åŒ–çš„æ–‡æ¡£å†…å®¹
    """

    def __init__(self, agent_id: str, session_id: str, workspace_path: str, memory_manager=None):
        super().__init__(
            agent_id=agent_id,
            session_id=session_id,
            workspace_path=workspace_path,
            memory_manager=memory_manager,
            profile="æ–‡æ¡£ä¸“å®¶",
            goal="é«˜æ•ˆç®¡ç†ã€å¤„ç†å’Œåˆ†ææ‰€æœ‰é¡¹ç›®ç›¸å…³æ–‡æ¡£"
        )
        
        # è®¾ç½®ä¸“å®¶ä¿¡æ¯
        self.name = "æå¿ƒæ‚¦"
        self.avatar = "ğŸ“„"
        self.expertise = "æ–‡æ¡£ç®¡ç†ä¸å¤„ç†"
        
        # è®¾ç½®åŠ¨ä½œ
        self.set_actions([DocumentProcessAction, DocumentSummaryAction])
        
        # åˆ›å»ºæå¿ƒæ‚¦çš„å·¥ä½œåŒºç›®å½•ç»“æ„ï¼ˆå°±åƒå¥¹çš„æ–‡ä»¶æŸœï¼‰
        self.upload_path = self.agent_workspace / "uploads"        # åŸå§‹æ–‡ä»¶å­˜æ”¾
        self.processed_path = self.agent_workspace / "processed"   # å¤„ç†åçš„MDæ–‡ä»¶
        self.summaries_path = self.agent_workspace / "summaries"   # æ–‡æ¡£æ‘˜è¦
        self.extracts_path = self.agent_workspace / "extracts"     # å…³é”®ä¿¡æ¯æå–
        
        # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
        for path in [self.upload_path, self.processed_path, self.summaries_path, self.extracts_path]:
            path.mkdir(exist_ok=True)
        
        # æ–‡æ¡£ç´¢å¼•æ–‡ä»¶
        self.index_file = self.agent_workspace / "index.json"
        self.document_index = self._load_document_index()
        
        logger.info(f"ğŸ“„ æ–‡æ¡£ä¸“å®¶ {self.name} åˆå§‹åŒ–å®Œæˆ")

    def _load_document_index(self) -> Dict:
        """åŠ è½½æ–‡æ¡£ç´¢å¼•"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {
            "documents": {},
            "categories": {},
            "last_updated": datetime.now().isoformat()
        }

    def _save_document_index(self):
        """ä¿å­˜æ–‡æ¡£ç´¢å¼•"""
        self.document_index["last_updated"] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.document_index, f, ensure_ascii=False, indent=2)

    def _get_summarize_prompt(self, filename: str, content: str) -> str:
        """æ„å»ºæ–‡æ¡£æ‘˜è¦æç¤ºè¯"""
        # ä½¿ç”¨æ–°çš„Promptæ¨¡å—
        return document_expert_prompts.get_document_summary_prompt(filename, content, self.name)

    def _get_key_info_prompt(self, filename: str, content: str) -> str:
        """æ„å»ºå…³é”®ä¿¡æ¯æå–æç¤ºè¯"""
        # ä½¿ç”¨æ–°çš„Promptæ¨¡å—
        return document_expert_prompts.get_key_info_extraction_prompt(filename, content, self.name)

    async def _execute_specific_task_with_messages(self, task: "Task", history_messages: List[Message]) -> Dict[str, Any]:
        """ä½¿ç”¨MetaGPTæ ‡å‡†çš„Messageå†å²æ‰§è¡Œæ–‡æ¡£å¤„ç†ä»»åŠ¡"""
        logger.info(f"ğŸ“„ {self.name} å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.description}")

        # ä»Messageå†å²ä¸­æå–å†…å®¹å’Œæ–‡ä»¶ä¿¡æ¯
        file_path = ""
        doc_id = ""
        
        if history_messages:
            for msg in history_messages:
                if hasattr(msg, 'content') and msg.content:
                    # å°è¯•ä»æ¶ˆæ¯å†…å®¹ä¸­æå–æ–‡ä»¶è·¯å¾„æˆ–æ–‡æ¡£ID
                    if "file_path" in msg.content:
                        # ç®€å•çš„æ–‡ä»¶è·¯å¾„æå–é€»è¾‘
                        import re
                        path_match = re.search(r'file_path[\'"]?\s*:\s*[\'"]?([^\'"]+)', msg.content)
                        if path_match:
                            file_path = path_match.group(1)
                    elif "document_id" in msg.content:
                        # ç®€å•çš„æ–‡æ¡£IDæå–é€»è¾‘
                        import re
                        id_match = re.search(r'document_id[\'"]?\s*:\s*[\'"]?([^\'"]+)', msg.content)
                        if id_match:
                            doc_id = id_match.group(1)

        # ç®€å•çš„åŸºäºå…³é”®è¯çš„ä»»åŠ¡è·¯ç”±
        if "å¤„ç†" in task.description and "æ–‡ä»¶" in task.description:
            if file_path:
                return await self.process_uploaded_file(file_path)
            else:
                return {"status": "error", "result": "æœªåœ¨Messageå†å²ä¸­æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„"}

        elif "æå–" in task.description and "ä¿¡æ¯" in task.description:
            if not doc_id:
                # å°è¯•ä»å†å²æ¶ˆæ¯ä¸­è·å–æœ€è¿‘åˆ›å»ºçš„æ–‡ä»¶ä½œä¸ºdoc_id
                for msg in reversed(history_messages):
                    if hasattr(msg, 'content') and "files_created" in msg.content:
                        import re
                        files_match = re.search(r'files_created[\'"]?\s*:\s*\[\'"]?([^\'"]+)', msg.content)
                        if files_match:
                            doc_id = files_match.group(1)
                            break
            
            if not doc_id:
                return {"status": "error", "result": "æœªåœ¨Messageå†å²ä¸­æ‰¾åˆ°å¯ä¾›æå–ä¿¡æ¯çš„æ–‡ä»¶"}

            return await self._extract_key_information_by_doc_id(doc_id)
            
        elif "æ‘˜è¦" in task.description:
            if not doc_id:
                return {"status": "error", "result": "æœªåœ¨Messageå†å²ä¸­æ‰¾åˆ°éœ€è¦æ‘˜è¦çš„æ–‡æ¡£"}
            return await self.create_summary(doc_id)

        else:
            # é»˜è®¤å¤„ç†
            return {"status": "completed", "result": f"å·²å®Œæˆé€šç”¨æ–‡æ¡£ä»»åŠ¡: {task.description}"}

    async def _process_document_with_mineru(self, file_path: Path) -> str:
        """ä½¿ç”¨MinerU APIå¤„ç†æ–‡æ¡£"""
        try:
            # è°ƒç”¨MinerUå·¥å…·å¤„ç†æ–‡æ¡£
            result = await mineru_tool.process_file(str(file_path))
            if result and 'markdown_content' in result:
                return result['markdown_content']
            return None
        except Exception as e:
            print(f"âŒ MinerUå¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            # å¦‚æœMinerUå¤±è´¥ï¼Œå°è¯•ç®€å•çš„æ–‡æœ¬æå–
            return await self._fallback_text_extraction(file_path)

    async def _fallback_text_extraction(self, file_path: Path) -> str:
        """å¤‡ç”¨æ–‡æœ¬æå–æ–¹æ³•"""
        try:
            if file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return f"# {file_path.name}\n\n{content}"
            elif file_path.suffix.lower() == '.md':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                return f"# {file_path.name}\n\n[æ–‡æ¡£æ ¼å¼æš‚ä¸æ”¯æŒç›´æ¥è½¬æ¢ï¼Œè¯·ä½¿ç”¨ä¸“ä¸šå·¥å…·å¤„ç†]"
        except:
            return f"# {file_path.name}\n\n[æ–‡æ¡£è¯»å–å¤±è´¥]"

    async def _generate_summary(self, filename: str, content: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦ (ç°åœ¨ä½¿ç”¨SummaryTool)"""
        logger.info(f"ä½¿ç”¨SummaryToolä¸º {filename} ç”Ÿæˆæ‘˜è¦...")
        return await summary_tool.run(content)

    async def _extract_key_information_by_doc_id(self, doc_id: str) -> Dict[str, Any]:
        """æ ¹æ®æ–‡æ¡£IDï¼ˆæ–‡ä»¶åï¼‰æå–å…³é”®ä¿¡æ¯"""
        try:
            # ä»ç´¢å¼•ä¸­è·å–æ–‡ä»¶å†…å®¹
            content = self.get_document_content(doc_id)
            if "å†…å®¹è·å–å¤±è´¥" in content:
                # å°è¯•ç›´æ¥è¯»å–æ–‡ä»¶
                file_path = self.processed_path / doc_id
                if not file_path.exists():
                     file_path = self.upload_path / doc_id # å†æ¬¡å°è¯•åŸå§‹ä¸Šä¼ ç›®å½•
                
                if file_path.exists():
                     with open(file_path, 'r', encoding='utf-8') as f:
                         content = f.read()
                else:
                    return {"status": "error", "result": f"æ‰¾ä¸åˆ°æ–‡æ¡£ {doc_id} æˆ–å…¶å†…å®¹"}

            key_info = await self._extract_key_information(doc_id, content)
            
            # ä¿å­˜æå–ç»“æœ
            extract_file = self.extracts_path / f"extract_{Path(doc_id).stem}.md"
            with open(extract_file, 'w', encoding='utf-8') as f:
                f.write(f"# æ–‡æ¡£å…³é”®ä¿¡æ¯æå–: {doc_id}\n\n{key_info}")

            return {
                "status": "completed",
                "result": f"å·²ä» {doc_id} ä¸­æå–å…³é”®ä¿¡æ¯ã€‚",
                "files_created": [extract_file.name],
                "extracted_info": key_info,
            }
        except Exception as e:
            logger.error(f"æå–å…³é”®ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            return {"status": "error", "result": f"æå–å…³é”®ä¿¡æ¯å¤±è´¥: {e}"}


    async def _extract_key_information(self, filename: str, content: str) -> str:
        """æå–å…³é”®ä¿¡æ¯"""
        try:
            prompt = document_expert_prompts.get_key_info_extraction_prompt(filename, content, self.name)
            key_info = await llm.acreate_text(prompt)
            return key_info.strip()
        except Exception as e:
            return f"å…³é”®ä¿¡æ¯æå–å¤±è´¥ï¼š{str(e)}"

    def _update_document_index(self, filename: str, doc_info: Dict):
        """æ›´æ–°æ–‡æ¡£ç´¢å¼•"""
        self.document_index["documents"][filename] = doc_info
        
        # ç®€å•çš„åˆ†ç±»é€»è¾‘ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        file_ext = Path(filename).suffix.lower()
        if file_ext not in self.document_index["categories"]:
            self.document_index["categories"][file_ext] = []
        self.document_index["categories"][file_ext].append(filename)

    def get_document_summary(self, filename: str) -> str:
        """è·å–æŒ‡å®šæ–‡æ¡£çš„æ‘˜è¦"""
        if filename in self.document_index["documents"]:
            return self.document_index["documents"][filename]["summary"]
        return "æ–‡æ¡£æœªæ‰¾åˆ°æˆ–æœªå¤„ç†"

    def list_processed_documents(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²å¤„ç†çš„æ–‡æ¡£"""
        return list(self.document_index["documents"].keys())

    def get_document_content(self, filename: str) -> str:
        """è·å–å¤„ç†åçš„æ–‡æ¡£å†…å®¹"""
        if filename in self.document_index["documents"]:
            processed_file = self.document_index["documents"][filename]["processed_file"]
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    return f.read()
            except:
                pass
        return "æ–‡æ¡£å†…å®¹è·å–å¤±è´¥"
    
    async def get_work_summary(self) -> str:
        """è·å–å·¥ä½œæ‘˜è¦"""
        try:
            doc_count = len(self.document_index["documents"])
            categories = self.document_index.get("categories", {})
            
            summary = f"ğŸ“„ {self.name} å·¥ä½œæ‘˜è¦:\n"
            summary += f"â€¢ å·²å¤„ç†æ–‡æ¡£: {doc_count} ä¸ª\n"
            
            if categories:
                summary += f"â€¢ æ–‡æ¡£ç±»å‹: {', '.join(categories.keys())}\n"
            
            summary += f"â€¢ å½“å‰çŠ¶æ€: {self.status}\n"
            
            if self.current_task:
                summary += f"â€¢ å½“å‰ä»»åŠ¡: {self.current_task}\n"
            
            return summary
            
        except Exception as e:
            return f"ğŸ“„ {self.name}: å·¥ä½œæ‘˜è¦è·å–å¤±è´¥ - {str(e)}"

    async def upload_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°å·¥ä½œåŒº"""
        try:
            # å°†æ–‡ä»¶å¤åˆ¶åˆ°uploadsç›®å½•
            target_path = self.upload_path / filename
            
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ–‡ä»¶å¤åˆ¶é€»è¾‘
            # æš‚æ—¶åˆ›å»ºä¸€ä¸ªå ä½ç¬¦
            with open(target_path, 'w', encoding='utf-8') as f:
                f.write(f"# {filename}\n\n[æ–‡ä»¶å·²ä¸Šä¼ ï¼Œç­‰å¾…å¤„ç†]")
            
            logger.info(f"ğŸ“„ {self.name} æ¥æ”¶æ–‡ä»¶: {filename}")
            
            return {
                'status': 'success',
                'message': f'æ–‡ä»¶ {filename} å·²ä¸Šä¼ åˆ°æå¿ƒæ‚¦çš„å·¥ä½œåŒº',
                'file_path': str(target_path)
            }
            
        except Exception as e:
            logger.error(f"âŒ {self.name} æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}'
            } 