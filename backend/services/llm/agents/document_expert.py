"""
æ–‡æ¡£ä¸“å®¶Agent - æå¿ƒæ‚¦
è´Ÿè´£æ–‡æ¡£ç®¡ç†ã€æ ¼å¼åŒ–å’Œæ‘˜è¦
"""
import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from metagpt.actions import Action
from metagpt.schema import Message
from metagpt.logs import logger

from .base import BaseAgent
from backend.services.llm_provider import llm
from backend.tools.mineru_api_tool import mineru_tool

class DocumentProcessAction(Action):
    """æ–‡æ¡£å¤„ç†åŠ¨ä½œ"""
    
    async def run(self, file_path: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£æ–‡ä»¶"""
        try:
            # ä½¿ç”¨MinerUå·¥å…·å¤„ç†æ–‡æ¡£
            result = await mineru_tool.process_file(file_path)
            return result
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return {'error': str(e)}


class DocumentSummaryAction(Action):
    """æ–‡æ¡£æ‘˜è¦åŠ¨ä½œ"""
    
    async def run(self, filename: str, content: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        prompt = f"""
ä½ æ˜¯æå¿ƒæ‚¦ï¼Œä¸€ä½ä¸“ä¸šçš„åŠå…¬å®¤æ–‡ç§˜å’Œæ–‡æ¡£ç®¡ç†ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼š

æ–‡æ¡£åç§°ï¼š{filename}
æ–‡æ¡£å†…å®¹ï¼ˆå‰4000å­—ç¬¦ï¼‰ï¼š
---
{content[:4000]}
---

è¯·ç”¨1-2å¥è¯æ€»ç»“è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯ã€‚
"""
        try:
            summary = await llm.acreate_text(prompt)
            return summary.strip()
        except Exception as e:
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"


class DocumentExpertAgent(BaseAgent):
    """
    æ–‡æ¡£ä¸“å®¶Agent - æå¿ƒæ‚¦
    
    è§’è‰²å®šä½ï¼šåŠå…¬å®¤æ–‡ç§˜ï¼Œè´Ÿè´£æ‰€æœ‰æ–‡æ¡£çš„ç®¡ç†ã€å½’æ¡£ã€æ ¼å¼åŒ–å’Œæ‘˜è¦
    å°±åƒçœŸå®åŠå…¬å®¤ä¸­çš„æ–‡ç§˜ä¸€æ ·ï¼Œå¥¹è´Ÿè´£ï¼š
    - æ¥æ”¶å’Œæ•´ç†æ‰€æœ‰å®¢æˆ·æä¾›çš„æ–‡æ¡£
    - å°†å„ç§æ ¼å¼çš„æ–‡æ¡£è½¬æ¢ä¸ºæ ‡å‡†çš„Markdownæ ¼å¼
    - å»ºç«‹æ–‡æ¡£ç´¢å¼•å’Œåˆ†ç±»ä½“ç³»
    - æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆæ‘˜è¦
    - ä¸ºå…¶ä»–åŒäº‹æä¾›æ ¼å¼åŒ–çš„æ–‡æ¡£å†…å®¹
    """

    def __init__(self, agent_id: str, session_id: str, workspace_path: str, memory_manager=None):
        super().__init__(
            agent_id=agent_id,
            session_id=session_id,
            workspace_path=workspace_path,
            memory_manager=memory_manager,
            profile="æ–‡æ¡£ä¸“å®¶",
            goal="é«˜æ•ˆç®¡ç†ã€å¤„ç†å’Œåˆ†ææ‰€æœ‰é¡¹ç›®ç›¸å…³æ–‡æ¡£"
        )
        
        # è®¾ç½®ä¸“å®¶ä¿¡æ¯
        self.name = "æå¿ƒæ‚¦"
        self.avatar = "ğŸ“„"
        self.expertise = "æ–‡æ¡£ç®¡ç†ä¸å¤„ç†"
        
        # è®¾ç½®åŠ¨ä½œ
        self.set_actions([DocumentProcessAction, DocumentSummaryAction])
        
        # åˆ›å»ºæå¿ƒæ‚¦çš„å·¥ä½œåŒºç›®å½•ç»“æ„ï¼ˆå°±åƒå¥¹çš„æ–‡ä»¶æŸœï¼‰
        self.upload_path = self.agent_workspace / "uploads"        # åŸå§‹æ–‡ä»¶å­˜æ”¾
        self.processed_path = self.agent_workspace / "processed"   # å¤„ç†åçš„MDæ–‡ä»¶
        self.summaries_path = self.agent_workspace / "summaries"   # æ–‡æ¡£æ‘˜è¦
        self.extracts_path = self.agent_workspace / "extracts"     # å…³é”®ä¿¡æ¯æå–
        
        # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
        for path in [self.upload_path, self.processed_path, self.summaries_path, self.extracts_path]:
            path.mkdir(exist_ok=True)
        
        # æ–‡æ¡£ç´¢å¼•æ–‡ä»¶
        self.index_file = self.agent_workspace / "index.json"
        self.document_index = self._load_document_index()
        
        logger.info(f"ğŸ“„ æ–‡æ¡£ä¸“å®¶ {self.name} åˆå§‹åŒ–å®Œæˆ")

    def _load_document_index(self) -> Dict:
        """åŠ è½½æ–‡æ¡£ç´¢å¼•"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {
            "documents": {},
            "categories": {},
            "last_updated": datetime.now().isoformat()
        }

    def _save_document_index(self):
        """ä¿å­˜æ–‡æ¡£ç´¢å¼•"""
        self.document_index["last_updated"] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.document_index, f, ensure_ascii=False, indent=2)

    def _get_summarize_prompt(self, filename: str, content: str) -> str:
        """æ„å»ºæ–‡æ¡£æ‘˜è¦æç¤ºè¯"""
        return f"""
ä½ æ˜¯æå¿ƒæ‚¦ï¼Œä¸€ä½ä¸“ä¸šçš„åŠå…¬å®¤æ–‡ç§˜å’Œæ–‡æ¡£ç®¡ç†ä¸“å®¶ã€‚ä½ éœ€è¦ä¸ºé¡¹ç›®æ€»ç›‘å¿«é€Ÿæ€»ç»“è¿™ä»½æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹ã€‚

æ–‡æ¡£åç§°ï¼š{filename}
æ–‡æ¡£å†…å®¹ï¼ˆå‰4000å­—ç¬¦ï¼‰ï¼š
---
{content[:4000]}
---

è¯·ä»¥ä¸“ä¸šæ–‡ç§˜çš„è§’åº¦ï¼Œç”¨1-2å¥è¯æ€»ç»“è¿™ä»½æ–‡æ¡£çš„ï¼š
1. ä¸»è¦å†…å®¹æˆ–ç›®çš„
2. å…³é”®æ•°æ®æˆ–é‡è¦ä¿¡æ¯ç‚¹

æ ¼å¼ç¤ºä¾‹ï¼š
"è¿™æ˜¯ä¸€ä»½å…³äºXXé¡¹ç›®çš„å®æ–½æ–¹æ¡ˆï¼Œè¯¦ç»†è§„å®šäº†ä¸‰ä¸ªé˜¶æ®µçš„å·¥ä½œå†…å®¹å’Œæ—¶é—´å®‰æ’ï¼Œé¢„ç®—æ€»é¢ä¸ºXXä¸‡å…ƒã€‚"

è¯·ç›´æ¥ç»™å‡ºæ‘˜è¦ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜ã€‚
"""

    def _get_key_info_prompt(self, filename: str, content: str) -> str:
        """æ„å»ºå…³é”®ä¿¡æ¯æå–æç¤ºè¯"""
        return f"""
ä½ æ˜¯æå¿ƒæ‚¦ï¼ŒåŠå…¬å®¤æ–‡æ¡£ç®¡ç†ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œä¸ºé¡¹ç›®å›¢é˜Ÿæä¾›ç»“æ„åŒ–çš„ä¿¡æ¯æ‘˜è¦ã€‚

æ–‡æ¡£ï¼š{filename}
å†…å®¹ï¼š
---
{content[:6000]}
---

è¯·æå–ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼ˆå¦‚æœæ–‡æ¡£ä¸­åŒ…å«çš„è¯ï¼‰ï¼š
1. é‡è¦æ—¥æœŸå’Œæ—¶é—´èŠ‚ç‚¹
2. å…³é”®æ•°å­—å’Œæ•°æ®
3. ä¸»è¦è´Ÿè´£äººæˆ–è”ç³»æ–¹å¼
4. é‡è¦æ”¿ç­–æ¡æ¬¾æˆ–è§„å®š
5. é¢„ç®—æˆ–è´¹ç”¨ä¿¡æ¯
6. å·¥ä½œæµç¨‹æˆ–æ­¥éª¤

è¯·ä»¥æ¸…æ™°çš„åˆ—è¡¨æ ¼å¼è¾“å‡ºï¼Œå¦‚æœæŸé¡¹ä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·æ ‡æ³¨"æœªæåŠ"ã€‚
"""

    async def _execute_specific_task(self, task: Dict[str, Any], context: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–‡æ¡£å¤„ç†ä»»åŠ¡
        
        æå¿ƒæ‚¦çš„å·¥ä½œæµç¨‹ï¼š
        1. æ£€æŸ¥uploadsæ–‡ä»¶å¤¹ä¸­çš„æ–°æ–‡æ¡£
        2. ä½¿ç”¨MinerU APIå°†æ–‡æ¡£è½¬æ¢ä¸ºMarkdown
        3. ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å’Œå…³é”®ä¿¡æ¯æå–
        4. æ›´æ–°æ–‡æ¡£ç´¢å¼•
        5. å‘é¡¹ç›®æ€»ç›‘æŠ¥å‘Šå¤„ç†ç»“æœ
        """
        try:
            self.status = 'working'
            self.current_task = "æ­£åœ¨æ•´ç†å’Œå¤„ç†æ–‡æ¡£..."
            
            processed_files = []
            upload_files = list(self.upload_path.iterdir())
            
            if not upload_files:
                return {
                    'agent_id': self.agent_id,
                    'status': 'completed',
                    'result': 'æå¿ƒæ‚¦ï¼šç›®å‰æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°æ–‡æ¡£ã€‚å¦‚æœ‰æ–‡æ¡£éœ€è¦å¤„ç†ï¼Œè¯·ç›´æ¥ä¸Šä¼ åˆ°æˆ‘çš„å·¥ä½œåŒºã€‚',
                    'files_processed': 0
                }
            
            for file_path in upload_files:
                if file_path.is_file() and not file_path.name.startswith('.'):
                    try:
                        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
                        if file_path.name in self.document_index["documents"]:
                            continue
                        
                        self.current_task = f"æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼š{file_path.name}"
                        
                        # ä½¿ç”¨MinerU APIå¤„ç†æ–‡æ¡£
                        markdown_content = await self._process_document_with_mineru(file_path)
                        
                        if markdown_content:
                            # ä¿å­˜å¤„ç†åçš„Markdownæ–‡ä»¶
                            md_filename = f"{file_path.stem}.md"
                            md_file_path = self.processed_path / md_filename
                            
                            with open(md_file_path, 'w', encoding='utf-8') as f:
                                f.write(markdown_content)
                            
                            # ç”Ÿæˆæ‘˜è¦
                            summary = await self._generate_summary(file_path.name, markdown_content)
                            summary_file = self.summaries_path / f"{file_path.stem}_summary.txt"
                            with open(summary_file, 'w', encoding='utf-8') as f:
                                f.write(summary)
                            
                            # æå–å…³é”®ä¿¡æ¯
                            key_info = await self._extract_key_information(file_path.name, markdown_content)
                            key_info_file = self.extracts_path / f"{file_path.stem}_keyinfo.txt"
                            with open(key_info_file, 'w', encoding='utf-8') as f:
                                f.write(key_info)
                            
                            # æ›´æ–°æ–‡æ¡£ç´¢å¼•
                            self._update_document_index(file_path.name, {
                                'original_file': str(file_path),
                                'processed_file': str(md_file_path),
                                'summary_file': str(summary_file),
                                'key_info_file': str(key_info_file),
                                'processed_at': datetime.now().isoformat(),
                                'file_size': file_path.stat().st_size,
                                'summary': summary[:200] + "..." if len(summary) > 200 else summary
                            })
                            
                            processed_files.append(file_path.name)
                            
                    except Exception as e:
                        print(f"âŒ å¤„ç†æ–‡æ¡£ {file_path.name} æ—¶å‡ºé”™: {e}")
                        continue
            
            # ä¿å­˜ç´¢å¼•
            self._save_document_index()
            
            # ç”Ÿæˆå·¥ä½œæŠ¥å‘Š
            if processed_files:
                result_message = f"æå¿ƒæ‚¦ï¼šæˆ‘å·²ç»å¤„ç†å®Œæˆ {len(processed_files)} ä¸ªæ–‡æ¡£ï¼š\n"
                for filename in processed_files:
                    doc_info = self.document_index["documents"][filename]
                    result_message += f"â€¢ {filename} - {doc_info['summary'][:100]}...\n"
                result_message += "\næ‰€æœ‰æ–‡æ¡£å·²è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶ç”Ÿæˆäº†æ‘˜è¦å’Œå…³é”®ä¿¡æ¯æå–ã€‚é¡¹ç›®æ€»ç›‘å¯ä»¥éšæ—¶è°ƒç”¨è¿™äº›èµ„æ–™ã€‚"
            else:
                result_message = "æå¿ƒæ‚¦ï¼šæ‰€æœ‰æ–‡æ¡£éƒ½å·²ç»å¤„ç†è¿‡äº†ï¼Œæ²¡æœ‰æ–°çš„æ–‡æ¡£éœ€è¦å¤„ç†ã€‚"
            
            self.status = 'completed'
            return {
                'agent_id': self.agent_id,
                'status': 'completed',
                'result': result_message,
                'files_processed': len(processed_files),
                'processed_files': processed_files
            }
            
        except Exception as e:
            self.status = 'error'
            return {
                'agent_id': self.agent_id,
                'status': 'error',
                'result': f"æå¿ƒæ‚¦ï¼šå¤„ç†æ–‡æ¡£æ—¶é‡åˆ°é—®é¢˜ï¼š{str(e)}",
                'error': str(e)
            }

    async def _process_document_with_mineru(self, file_path: Path) -> str:
        """ä½¿ç”¨MinerU APIå¤„ç†æ–‡æ¡£"""
        try:
            # è°ƒç”¨MinerUå·¥å…·å¤„ç†æ–‡æ¡£
            result = await mineru_tool.process_file(str(file_path))
            if result and 'markdown_content' in result:
                return result['markdown_content']
            return None
        except Exception as e:
            print(f"âŒ MinerUå¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            # å¦‚æœMinerUå¤±è´¥ï¼Œå°è¯•ç®€å•çš„æ–‡æœ¬æå–
            return await self._fallback_text_extraction(file_path)

    async def _fallback_text_extraction(self, file_path: Path) -> str:
        """å¤‡ç”¨æ–‡æœ¬æå–æ–¹æ³•"""
        try:
            if file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return f"# {file_path.name}\n\n{content}"
            elif file_path.suffix.lower() == '.md':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                return f"# {file_path.name}\n\n[æ–‡æ¡£æ ¼å¼æš‚ä¸æ”¯æŒç›´æ¥è½¬æ¢ï¼Œè¯·ä½¿ç”¨ä¸“ä¸šå·¥å…·å¤„ç†]"
        except:
            return f"# {file_path.name}\n\n[æ–‡æ¡£è¯»å–å¤±è´¥]"

    async def _generate_summary(self, filename: str, content: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        try:
            prompt = self._get_summarize_prompt(filename, content)
            summary = await llm.acreate_text(prompt)
            return summary.strip()
        except Exception as e:
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"

    async def _extract_key_information(self, filename: str, content: str) -> str:
        """æå–å…³é”®ä¿¡æ¯"""
        try:
            prompt = self._get_key_info_prompt(filename, content)
            key_info = await llm.acreate_text(prompt)
            return key_info.strip()
        except Exception as e:
            return f"å…³é”®ä¿¡æ¯æå–å¤±è´¥ï¼š{str(e)}"

    def _update_document_index(self, filename: str, doc_info: Dict):
        """æ›´æ–°æ–‡æ¡£ç´¢å¼•"""
        self.document_index["documents"][filename] = doc_info
        
        # ç®€å•çš„åˆ†ç±»é€»è¾‘ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        file_ext = Path(filename).suffix.lower()
        if file_ext not in self.document_index["categories"]:
            self.document_index["categories"][file_ext] = []
        self.document_index["categories"][file_ext].append(filename)

    def get_document_summary(self, filename: str) -> str:
        """è·å–æŒ‡å®šæ–‡æ¡£çš„æ‘˜è¦"""
        if filename in self.document_index["documents"]:
            return self.document_index["documents"][filename]["summary"]
        return "æ–‡æ¡£æœªæ‰¾åˆ°æˆ–æœªå¤„ç†"

    def list_processed_documents(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²å¤„ç†çš„æ–‡æ¡£"""
        return list(self.document_index["documents"].keys())

    def get_document_content(self, filename: str) -> str:
        """è·å–å¤„ç†åçš„æ–‡æ¡£å†…å®¹"""
        if filename in self.document_index["documents"]:
            processed_file = self.document_index["documents"][filename]["processed_file"]
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    return f.read()
            except:
                pass
        return "æ–‡æ¡£å†…å®¹è·å–å¤±è´¥"
    
    async def get_work_summary(self) -> str:
        """è·å–å·¥ä½œæ‘˜è¦"""
        try:
            doc_count = len(self.document_index["documents"])
            categories = self.document_index.get("categories", {})
            
            summary = f"ğŸ“„ {self.name} å·¥ä½œæ‘˜è¦:\n"
            summary += f"â€¢ å·²å¤„ç†æ–‡æ¡£: {doc_count} ä¸ª\n"
            
            if categories:
                summary += f"â€¢ æ–‡æ¡£ç±»å‹: {', '.join(categories.keys())}\n"
            
            summary += f"â€¢ å½“å‰çŠ¶æ€: {self.status}\n"
            
            if self.current_task:
                summary += f"â€¢ å½“å‰ä»»åŠ¡: {self.current_task}\n"
            
            return summary
            
        except Exception as e:
            return f"ğŸ“„ {self.name}: å·¥ä½œæ‘˜è¦è·å–å¤±è´¥ - {str(e)}"

    async def upload_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°å·¥ä½œåŒº"""
        try:
            # å°†æ–‡ä»¶å¤åˆ¶åˆ°uploadsç›®å½•
            target_path = self.upload_path / filename
            
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ–‡ä»¶å¤åˆ¶é€»è¾‘
            # æš‚æ—¶åˆ›å»ºä¸€ä¸ªå ä½ç¬¦
            with open(target_path, 'w', encoding='utf-8') as f:
                f.write(f"# {filename}\n\n[æ–‡ä»¶å·²ä¸Šä¼ ï¼Œç­‰å¾…å¤„ç†]")
            
            logger.info(f"ğŸ“„ {self.name} æ¥æ”¶æ–‡ä»¶: {filename}")
            
            return {
                'status': 'success',
                'message': f'æ–‡ä»¶ {filename} å·²ä¸Šä¼ åˆ°æå¿ƒæ‚¦çš„å·¥ä½œåŒº',
                'file_path': str(target_path)
            }
            
        except Exception as e:
            logger.error(f"âŒ {self.name} æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}'
            } 