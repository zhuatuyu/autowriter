#!/usr/bin/env python3
"""
ðŸ“„ æ–‡æ¡£ç»“æž„åŒ–ä¿¡æ¯æå–å·¥å…· - åŸºäºŽLangExtract
ä¸“é—¨ç”¨äºŽä»Žç»©æ•ˆè¯„ä»·æŠ¥å‘Šä¸­æå–æŒ‡æ ‡ä½“ç³»ä¿¡æ¯

ç”¨æ³•: 
  python extradoc.py -f ZYCASE2024çœçº§èŒä¸šæŠ€èƒ½ç«žèµ›ç»è´¹é¡¹ç›®ç»©æ•ˆè¯„ä»·æŠ¥å‘Š.md
  python extradoc.py -f æ–‡æ¡£1.md æ–‡æ¡£2.pdf --output æå–ç»“æžœ.json
  python extradoc.py -f æ–‡æ¡£.md --visualize  # ç”Ÿæˆå¯è§†åŒ–HTML
"""

import asyncio
import argparse
import sys
import json
import textwrap
import os
import yaml
from pathlib import Path
from typing import List, Dict, Any

try:
    import langextract as lx
    LANGEXTRACT_AVAILABLE = True
except ImportError:
    LANGEXTRACT_AVAILABLE = False
    print("âŒ LangExtractæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install langextract")


def load_config() -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path('config/config2.yaml')
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    return {}


def setup_ollama_environment():
    """è®¾ç½®OllamaçŽ¯å¢ƒå˜é‡"""
    # è®¾ç½®Ollamaç›¸å…³çŽ¯å¢ƒå˜é‡
    os.environ['OLLAMA_HOST'] = 'http://localhost:11434'
    os.environ['OLLAMA_MODEL'] = 'gpt-oss:20b'
    
    print("ðŸ”§ å·²è®¾ç½®OllamaçŽ¯å¢ƒå˜é‡:")
    print(f"   OLLAMA_HOST: {os.environ['OLLAMA_HOST']}")
    print(f"   OLLAMA_MODEL: {os.environ['OLLAMA_MODEL']}")
    
    # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
    try:
        import requests
        response = requests.get(f"{os.environ['OLLAMA_HOST']}/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸")
            # æ˜¾ç¤ºå¯ç”¨æ¨¡åž‹
            models = response.json().get('models', [])
            if models:
                print("ðŸ“‹ å¯ç”¨æ¨¡åž‹:")
                for model in models:
                    print(f"   - {model.get('name', 'Unknown')}")
        else:
            print("âš ï¸ OllamaæœåŠ¡å“åº”å¼‚å¸¸")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¿žæŽ¥åˆ°OllamaæœåŠ¡: {e}")
        print("ðŸ’¡ è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")


def create_performance_metrics_extraction_prompt() -> str:
    """åˆ›å»ºç»©æ•ˆæŒ‡æ ‡æå–çš„æç¤ºè¯"""
    return textwrap.dedent("""
    ä»Žç»©æ•ˆè¯„ä»·æŠ¥å‘Šä¸­æå–å®Œæ•´çš„æŒ‡æ ‡ä½“ç³»ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š
    
    1. ç»©æ•ˆæŒ‡æ ‡åç§°å’Œå®šä¹‰
    2. æŒ‡æ ‡æ‰€å±žç»´åº¦ï¼ˆå†³ç­–ã€è¿‡ç¨‹ã€äº§å‡ºã€æ•ˆç›Šï¼‰
    3. æŒ‡æ ‡æƒé‡å’Œåˆ†å€¼
    4. è¯„ä»·æ ‡å‡†å’Œè®¡åˆ†æ–¹æ³•
    5. å®žé™…å¾—åˆ†å’Œè¯„ä»·æ„è§
    6. æŒ‡æ ‡å®Œæˆæƒ…å†µ
    
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ¯ä¸ªæŒ‡æ ‡åŒ…å«å®Œæ•´ä¿¡æ¯ï¼š
    {
      "metric_id": "å”¯ä¸€æ ‡è¯†ç¬¦",
      "level1_name": "ä¸€çº§ç»´åº¦åç§°",
      "level2_name": "äºŒçº§åˆ†ç±»åç§°", 
      "name": "æŒ‡æ ‡åç§°",
      "weight": "æƒé‡åˆ†å€¼",
      "evaluation_type": "è¯„ä»·ç±»åž‹",
      "evaluation_points": "è¯„ä»·è¦ç‚¹",
      "opinion": "è¯„ä»·æ„è§",
      "score": "å®žé™…å¾—åˆ†"
    }
    
    ä½¿ç”¨ç²¾ç¡®æ–‡æœ¬ï¼Œä¸è¦æ”¹å†™æˆ–é‡å å®žä½“ã€‚å¦‚æžœæ–‡æ¡£ä¸­æ²¡æœ‰æŸä¸ªå­—æ®µçš„ä¿¡æ¯ï¼Œè¯·æ ‡æ³¨ä¸ºnullã€‚
    """)


def create_performance_metrics_examples() -> List[lx.data.ExampleData]:
    """åˆ›å»ºç»©æ•ˆæŒ‡æ ‡æå–çš„ç¤ºä¾‹æ•°æ®"""
    return [
        lx.data.ExampleData(
            text="å†³ç­–æŒ‡æ ‡ï¼šé¡¹ç›®ç«‹é¡¹ç¬¦åˆå›½å®¶åŠåœ°æ–¹æ”¿ç­–è¦æ±‚ï¼Œæƒé‡10åˆ†ï¼Œè¯„ä»·è¦ç‚¹ï¼šâ‘ é¡¹ç›®ç«‹é¡¹æ–‡ä»¶å¼•ç”¨ç›¸å…³æ”¿ç­–ï¼›â‘¡é¡¹ç›®ç›®æ ‡ä¸Žç®¡ç†åŠžæ³•ä¸€è‡´ï¼›â‘¢é¡¹ç›®é¢„ç®—ç¼–åˆ¶ç¬¦åˆé¢„ç®—æ³•åŽŸåˆ™ã€‚æ»¡è¶³å…¨éƒ¨ä¸‰é¡¹æ¡ä»¶ï¼Œå¾—æ»¡åˆ†ï¼›æ¯ç¼ºå°‘ä¸€é¡¹æ‰£3åˆ†ï¼Œæ‰£å®Œä¸ºæ­¢ã€‚",
            extractions=[
                lx.data.Extraction(
                    extraction_class="metric",
                    extraction_text="é¡¹ç›®ç«‹é¡¹ç¬¦åˆå›½å®¶åŠåœ°æ–¹æ”¿ç­–è¦æ±‚",
                    attributes={
                        "level1_name": "å†³ç­–",
                        "level2_name": "æ”¿ç­–ç¬¦åˆæ€§", 
                        "weight": "10",
                        "evaluation_type": "condition",
                        "evaluation_points": "â‘ é¡¹ç›®ç«‹é¡¹æ–‡ä»¶å¼•ç”¨ç›¸å…³æ”¿ç­–ï¼›â‘¡é¡¹ç›®ç›®æ ‡ä¸Žç®¡ç†åŠžæ³•ä¸€è‡´ï¼›â‘¢é¡¹ç›®é¢„ç®—ç¼–åˆ¶ç¬¦åˆé¢„ç®—æ³•åŽŸåˆ™ã€‚æ»¡è¶³å…¨éƒ¨ä¸‰é¡¹æ¡ä»¶ï¼Œå¾—æ»¡åˆ†ï¼›æ¯ç¼ºå°‘ä¸€é¡¹æ‰£3åˆ†ï¼Œæ‰£å®Œä¸ºæ­¢ã€‚"
                    }
                )
            ]
        ),
        lx.data.ExampleData(
            text="è¿‡ç¨‹æŒ‡æ ‡ï¼šè´¢æ”¿å¥–è¡¥èµ„é‡‘ä¸“æ¬¾ä¸“ç”¨ã€ä½¿ç”¨åˆè§„ï¼Œæƒé‡9åˆ†ï¼Œè¯„ä»·è¦ç‚¹ï¼šâ‘ è®¾ç«‹ä¸“é¡¹èµ„é‡‘è´¦æˆ·æˆ–å®žè¡Œä¸“è´¦æ ¸ç®—ï¼›â‘¡èµ„é‡‘æ”¯å‡ºä¸Žé¡¹ç›®å†…å®¹ç›´æŽ¥å¯¹åº”ï¼›â‘¢æ— æˆªç•™ã€æŒªç”¨ã€å¥—å–èµ„é‡‘è¡Œä¸ºã€‚æ»¡è¶³å…¨éƒ¨ä¸‰é¡¹å¾—æ»¡åˆ†ï¼›å‘çŽ°ä¸€é¡¹è¿è§„ï¼Œå¾—åˆ†=0ã€‚",
            extractions=[
                lx.data.Extraction(
                    extraction_class="metric",
                    extraction_text="è´¢æ”¿å¥–è¡¥èµ„é‡‘ä¸“æ¬¾ä¸“ç”¨ã€ä½¿ç”¨åˆè§„",
                    attributes={
                        "level1_name": "è¿‡ç¨‹",
                        "level2_name": "èµ„é‡‘ç®¡ç†",
                        "weight": "9", 
                        "evaluation_type": "condition",
                        "evaluation_points": "â‘ è®¾ç«‹ä¸“é¡¹èµ„é‡‘è´¦æˆ·æˆ–å®žè¡Œä¸“è´¦æ ¸ç®—ï¼›â‘¡èµ„é‡‘æ”¯å‡ºä¸Žé¡¹ç›®å†…å®¹ç›´æŽ¥å¯¹åº”ï¼›â‘¢æ— æˆªç•™ã€æŒªç”¨ã€å¥—å–èµ„é‡‘è¡Œä¸ºã€‚æ»¡è¶³å…¨éƒ¨ä¸‰é¡¹å¾—æ»¡åˆ†ï¼›å‘çŽ°ä¸€é¡¹è¿è§„ï¼Œå¾—åˆ†=0ã€‚"
                    }
                )
            ]
        )
    ]


async def extract_performance_metrics_from_document(
    document_path: str,
    output_path: str = None,
    visualize: bool = False
) -> Dict[str, Any]:
    """ä»Žæ–‡æ¡£ä¸­æå–ç»©æ•ˆæŒ‡æ ‡ä½“ç³»"""
    
    if not LANGEXTRACT_AVAILABLE:
        return {"error": "LangExtractæœªå®‰è£…"}
    
    try:
        print(f"ðŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {document_path}")
        
        # è¯»å–æ–‡æ¡£å†…å®¹
        with open(document_path, 'r', encoding='utf-8') as f:
            document_content = f.read()
        
        print(f"ðŸ“Š æ–‡æ¡£é•¿åº¦: {len(document_content)} å­—ç¬¦")
        
        # åˆ›å»ºæå–æç¤ºè¯å’Œç¤ºä¾‹
        prompt = create_performance_metrics_extraction_prompt()
        examples = create_performance_metrics_examples()
        
        print("ðŸ§  å¼€å§‹ä½¿ç”¨LangExtractæå–ç»©æ•ˆæŒ‡æ ‡...")
        print("ðŸ”§ ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡åž‹: gpt-oss:20b")
        
        # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡åž‹
        try:
            result = lx.extract(
                text_or_documents=document_content,
                prompt_description=prompt,
                examples=examples,
                model_id="gpt-oss:20b",  # ç›´æŽ¥ä½¿ç”¨Ollamaæ¨¡åž‹åç§°
            )
            print("âœ… æå–å®Œæˆï¼")
        except Exception as e:
            # å¦‚æžœç›´æŽ¥ä½¿ç”¨æ¨¡åž‹åç§°å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Ollamaçš„å®Œæ•´é…ç½®
            print(f"âš ï¸ ç›´æŽ¥ä½¿ç”¨æ¨¡åž‹åç§°å¤±è´¥ï¼Œå°è¯•Ollamaé…ç½®: {e}")
            try:
                result = lx.extract(
                    text_or_documents=document_content,
                    prompt_description=prompt,
                    examples=examples,
                    model_id="ollama/gpt-oss:20b",  # ä½¿ç”¨ollama/å‰ç¼€
                )
                print("âœ… ä½¿ç”¨Ollamaé…ç½®æå–å®Œæˆï¼")
            except Exception as e2:
                print(f"âš ï¸ Ollamaé…ç½®ä¹Ÿå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼: {e2}")
                # æœ€åŽå°è¯•ä½¿ç”¨çŽ¯å¢ƒå˜é‡ä¸­çš„æ¨¡åž‹
                result = lx.extract(
                    text_or_documents=document_content,
                    prompt_description=prompt,
                    examples=examples,
                    model_id=os.environ.get('OLLAMA_MODEL', 'gpt-oss:20b'),
                )
                print("âœ… ä½¿ç”¨çŽ¯å¢ƒå˜é‡æ¨¡åž‹æå–å®Œæˆï¼")
        
        # ä¿å­˜ç»“æžœåˆ°JSONæ–‡ä»¶
        if output_path:
            output_file = Path(output_path)
        else:
            output_file = Path(document_path).with_suffix('.extracted_metrics.json')
        
        # ä¿å­˜æå–ç»“æžœ
        lx.io.save_annotated_documents([result], output_name=output_file.name, output_dir=output_file.parent)
        print(f"ðŸ’¾ æå–ç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç”Ÿæˆå¯è§†åŒ–HTMLï¼ˆå¦‚æžœéœ€è¦ï¼‰
        html_file = None
        if visualize:
            try:
                html_content = lx.visualize(str(output_file))
                html_file = output_file.with_suffix('.html')
                with open(html_file, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"ðŸŒ å¯è§†åŒ–HTMLå·²ç”Ÿæˆ: {html_file}")
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–HTMLå¤±è´¥: {e}")
        
        # è¿”å›žæå–ç»“æžœæ‘˜è¦
        extraction_summary = {
            "document": document_path,
            "extractions_count": len(result.extractions) if hasattr(result, 'extractions') else 0,
            "output_file": str(output_file),
            "visualization_file": str(html_file) if html_file else None,
            "extractions": []
        }
        
        # æ·»åŠ æå–çš„æŒ‡æ ‡ä¿¡æ¯
        if hasattr(result, 'extractions'):
            for extraction in result.extractions:
                extraction_summary["extractions"].append({
                    "class": extraction.extraction_class,
                    "text": extraction.extraction_text,
                    "attributes": extraction.attributes
                })
        
        return extraction_summary
        
    except Exception as e:
        error_msg = f"âŒ æå–å¤±è´¥: {str(e)}"
        print(error_msg)
        return {"error": error_msg}


async def batch_extract_documents(
    document_paths: List[str],
    output_dir: str = None,
    visualize: bool = False
) -> List[Dict[str, Any]]:
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£"""
    
    results = []
    
    for doc_path in document_paths:
        print(f"\n{'='*60}")
        print(f"ðŸ“„ å¤„ç†æ–‡æ¡£: {doc_path}")
        print(f"{'='*60}")
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if output_dir:
            output_path = Path(output_dir) / f"{Path(doc_path).stem}_extracted.json"
        else:
            output_path = None
        
        # æå–æŒ‡æ ‡
        result = await extract_performance_metrics_from_document(
            doc_path, 
            output_path, 
            visualize
        )
        
        results.append(result)
        
        if "error" not in result:
            print(f"âœ… æ–‡æ¡£ {doc_path} å¤„ç†å®Œæˆ")
        else:
            print(f"âŒ æ–‡æ¡£ {doc_path} å¤„ç†å¤±è´¥")
    
    return results


async def main():
    parser = argparse.ArgumentParser(
        description="ðŸ“„ æ–‡æ¡£ç»“æž„åŒ–ä¿¡æ¯æå–å·¥å…· - åŸºäºŽLangExtract + æœ¬åœ°Ollama",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ðŸš€ ä½¿ç”¨ç¤ºä¾‹:
  # æå–å•ä¸ªæ–‡æ¡£çš„ç»©æ•ˆæŒ‡æ ‡
  python extradoc.py -f ZYCASE2024çœçº§èŒä¸šæŠ€èƒ½ç«žèµ›ç»è´¹é¡¹ç›®ç»©æ•ˆè¯„ä»·æŠ¥å‘Š.md
  
  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python extradoc.py -f æ–‡æ¡£.md --output æå–ç»“æžœ.json
  
  # ç”Ÿæˆå¯è§†åŒ–HTML
  python extradoc.py -f æ–‡æ¡£.md --visualize
  
  # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£
  python extradoc.py -f æ–‡æ¡£1.md æ–‡æ¡£2.pdf æ–‡æ¡£3.txt
  
  # æ‰¹é‡å¤„ç†å¹¶æŒ‡å®šè¾“å‡ºç›®å½•
  python extradoc.py -f æ–‡æ¡£1.md æ–‡æ¡£2.pdf --output-dir ./æå–ç»“æžœ --visualize
  
ðŸ’¡ å‰ç½®æ¡ä»¶:
  - ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve
  - ç¡®ä¿å·²æ‹‰å–æ¨¡åž‹: ollama pull gpt-oss:20b
        """
    )
    
    parser.add_argument(
        '-f', '--files', 
        nargs='+', 
        required=True,
        help='è¦æå–ä¿¡æ¯çš„æ–‡æ¡£æ–‡ä»¶åˆ—è¡¨'
    )
    
    parser.add_argument(
        '--output', 
        type=str,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸ªæ–‡æ¡£æ—¶ä½¿ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--output-dir', 
        type=str,
        help='è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡å¤„ç†æ—¶ä½¿ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--visualize',
        action='store_true',
        help='ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–HTMLæ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥LangExtractæ˜¯å¦å¯ç”¨
    if not LANGEXTRACT_AVAILABLE:
        print("âŒ LangExtractæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install langextract")
        sys.exit(1)
    
    # è®¾ç½®OllamaçŽ¯å¢ƒå˜é‡
    print("ðŸ”§ è®¾ç½®OllamaçŽ¯å¢ƒå˜é‡...")
    setup_ollama_environment()
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    valid_files = []
    for file_path in args.files:
        if Path(file_path).exists():
            valid_files.append(file_path)
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file_path}")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if not valid_files:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶ï¼Œé€€å‡º")
        sys.exit(1)
    
    print(f"\nðŸŽ¯ å¼€å§‹å¤„ç† {len(valid_files)} ä¸ªæ–‡æ¡£...")
    
    # å¤„ç†æ–‡æ¡£
    if len(valid_files) == 1:
        # å•ä¸ªæ–‡æ¡£
        result = await extract_performance_metrics_from_document(
            valid_files[0],
            args.output,
            args.visualize
        )
        results = [result]
    else:
        # æ‰¹é‡å¤„ç†
        results = await batch_extract_documents(
            valid_files,
            args.output_dir,
            args.visualize
        )
    
    # è¾“å‡ºå¤„ç†ç»“æžœæ‘˜è¦
    print(f"\n{'='*60}")
    print("ðŸ“Š å¤„ç†ç»“æžœæ‘˜è¦")
    print(f"{'='*60}")
    
    success_count = 0
    for i, result in enumerate(results):
        if "error" not in result:
            success_count += 1
            print(f"âœ… æ–‡æ¡£ {i+1}: æˆåŠŸæå– {result.get('extractions_count', 0)} ä¸ªæŒ‡æ ‡")
            print(f"   è¾“å‡ºæ–‡ä»¶: {result.get('output_file', 'N/A')}")
            if args.visualize:
                print(f"   å¯è§†åŒ–æ–‡ä»¶: {result.get('visualization_file', 'N/A')}")
        else:
            print(f"âŒ æ–‡æ¡£ {i+1}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print(f"\nðŸŽ‰ å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}/{len(results)}")
    
    if success_count > 0:
        print("\nðŸ’¡ æç¤º:")
        print("   - æå–çš„æŒ‡æ ‡å·²ä¿å­˜ä¸ºJSONæ ¼å¼")
        if args.visualize:
            print("   - å¯è§†åŒ–HTMLæ–‡ä»¶å·²ç”Ÿæˆï¼Œå¯åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹")
        print("   - å¯ä»¥ä½¿ç”¨æå–çš„æŒ‡æ ‡æ•°æ®æž„å»ºçŸ¥è¯†å›¾è°±æˆ–è¿›è¡Œè¿›ä¸€æ­¥åˆ†æž")
    
    sys.exit(0 if success_count > 0 else 1)


if __name__ == "__main__":
    asyncio.run(main())
