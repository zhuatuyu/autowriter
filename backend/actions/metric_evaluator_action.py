#!/usr/bin/env python
"""
æŒ‡æ ‡è¯„åˆ†Action - ä»writer_actionä¸­å½»åº•ç‹¬ç«‹ï¼ˆSOP1 æŒ‡æ ‡è¯„ä»·ï¼Œå†…è”å®ç°ï¼‰
"""
from metagpt.actions import Action
from metagpt.logs import logger
from backend.tools.json_utils import extract_json_from_llm_response

from backend.config.evaluation_standards import EVALUATION_TYPES as ENV_EVALUATION_TYPES
from backend.config.evaluator_prompts import (
    EVALUATOR_BASE_SYSTEM as ENV_EVALUATOR_BASE_SYSTEM,
    EVALUATION_PROMPT_TEMPLATE as ENV_EVALUATOR_PROMPT_TEMPLATE,
    METRIC_PROMPT_SPEC as ENV_METRIC_PROMPT_SPEC,
)
from pathlib import Path
import re
from backend.tools.project_info import get_project_info_text


class EvaluateMetrics(Action):
    """
    æŒ‡æ ‡è¯„åˆ†Action - æŒ‰ç…§æ ‡å‡†åŒ–è¯„ä»·ç±»å‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†
    ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆè¯„ä»·æ„è§å’Œå…·ä½“å¾—åˆ†
    """

    async def run(self, metric_table_json: str, vector_store_path: str, metric_table_md_path: str | None = None) -> dict:
        """
        å¯¹æ‰€æœ‰æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼Œè¿”å›è¯„åˆ†ç»“æœ

        Returns:
            dict: {
                "metrics_scores": [{"metric": {}, "score": 85.5, "opinion": "è¯„ä»·æ„è§"}],
                "level1_summary": {"å†³ç­–": 12.5, "è¿‡ç¨‹": 22.3, "äº§å‡º": 30.1, "æ•ˆç›Š": 20.6},
                "total_score": 85.5,
            }
        """
        logger.info("ğŸ“Š å¼€å§‹è¿›è¡ŒæŒ‡æ ‡è¯„åˆ†...")

        try:
            # ä¼˜å…ˆä»æ–‡ä»¶è¯»å–ï¼Œé¿å…ç»ç”±æ¶ˆæ¯ä¸Šä¸‹æ–‡ä¼ é€’é€ æˆæ±¡æŸ“
            metrics_data = None
            if metric_table_md_path:
                path = Path(metric_table_md_path)
                if path.exists():
                    text = path.read_text(encoding="utf-8")
                    m = re.search(r"```json\s*(.*?)\s*```", text, flags=re.DOTALL)
                    if m:
                        metrics_data = extract_json_from_llm_response(m.group(1))
            if metrics_data is None:
                # å›é€€ï¼šè§£æä¼ å…¥çš„ JSON å­—ç¬¦ä¸²ï¼ˆå…¼å®¹ä»£ç å—/æ¾æ•£JSONï¼‰
                metrics_data = extract_json_from_llm_response(metric_table_json)

            if isinstance(metrics_data, dict) and "error" in metrics_data:
                return {"error": "æŒ‡æ ‡ä½“ç³»æ„å»ºå¤±è´¥", "details": metrics_data}

            # extract_json_from_llm_response å·²åšå½’ä¸€åŒ–ï¼›æ­¤å¤„åªåšæœ€ç»ˆå…œåº•
            if isinstance(metrics_data, dict):
                metrics_data = [metrics_data]
            elif not isinstance(metrics_data, list):
                metrics_data = []

            # ä¸ºæ¯ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼ˆä¸¥æ ¼å¯¹é½ SOP1 æ‰å¹³è‹±æ–‡å­—æ®µç»“æ„ï¼‰
            metrics_scores = []

            for metric in metrics_data:
                try:
                    # æ‰§è¡Œå•ä¸ªæŒ‡æ ‡è¯„åˆ†
                    score, opinion = await self._evaluate_single_metric(metric, vector_store_path)

                    metrics_scores.append({
                        "metric": metric,
                        "score": score,
                        "opinion": opinion,
                    })

                    logger.info(f"âœ… å®ŒæˆæŒ‡æ ‡è¯„åˆ†: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {score}åˆ†")

                except Exception as e:
                    logger.error(f"æŒ‡æ ‡è¯„åˆ†å¤±è´¥: {metric.get('name', 'æœªçŸ¥æŒ‡æ ‡')} - {e}")


            # å°†æ„è§ä¸å¾—åˆ†å›å†™è‡³æŒ‡æ ‡è¡¨mdï¼ˆè‹¥æä¾›è·¯å¾„ï¼‰
            if metric_table_md_path:
                try:
                    self._update_metric_table_md(metric_table_md_path, metrics_scores)
                    logger.info(f"ğŸ“ å·²å›å†™è¯„åˆ†ä¸æ„è§è‡³: {metric_table_md_path}")
                except Exception as e:
                    logger.error(f"å›å†™metric_analysis_table.mdå¤±è´¥: {e}")

            result = {"metrics_scores": metrics_scores}
            logger.info("ğŸ“Š æŒ‡æ ‡è¯„åˆ†å®Œæˆï¼Œå·²å›å†™æ¯é¡¹ score/opinion")
            return result

        except Exception as e:
            logger.error(f"æŒ‡æ ‡è¯„åˆ†è¿‡ç¨‹å¤±è´¥: {e}")
            return {"error": "æŒ‡æ ‡è¯„åˆ†å¤±è´¥", "details": str(e)}

    def _update_metric_table_md(self, md_path: str, metrics_scores: list[dict]) -> None:
        """åœ¨ metric_analysis_table.md çš„ JSON é‡Œä¸ºåŒ¹é…çš„æŒ‡æ ‡è¡¥å…… opinion ä¸ scoreï¼ˆä¼˜å…ˆæŒ‰ metric_idï¼Œå…¶æ¬¡æŒ‰ nameï¼‰ã€‚"""
        path = Path(md_path)
        if not path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶: {md_path}")

        text = path.read_text(encoding="utf-8")

        # æå– ```json ... ``` ä»£ç å—
        code_block_match = re.search(r"```json\s*(.*?)\s*```", text, flags=re.DOTALL)
        if not code_block_match:
            raise ValueError("metric_analysis_table.md å†…æœªæ‰¾åˆ°JSONä»£ç å—")

        json_str = code_block_match.group(1)
        data = extract_json_from_llm_response(json_str)

        # å½’ä¸€ä¸ºåˆ—è¡¨
        if isinstance(data, dict):
            metrics_list = [data]
        elif isinstance(data, list):
            metrics_list = data
        else:
            raise ValueError("metric_analysis_table.md ä¸­JSONä¸æ˜¯å¯¹è±¡æˆ–æ•°ç»„")

        # å»ºç«‹è¯„åˆ†æ˜ å°„ï¼ˆä¼˜å…ˆ metric_idï¼Œå…¶æ¬¡ nameï¼‰
        score_map_id: dict[str, dict] = {}
        score_map_name: dict[str, dict] = {}
        for item in metrics_scores:
            metric = item.get("metric", {})
            mid = metric.get("metric_id")
            mname = metric.get("name")
            payload = {"score": item.get("score", 0), "opinion": item.get("opinion", "")}
            if mid:
                score_map_id[mid] = payload
            if mname:
                score_map_name[mname] = payload

        # å›å†™è‡³åŸåˆ—è¡¨ï¼ˆæŒ‰ metric_idï¼‰
        matched_ids = set()
        matched_names = set()
        for m in metrics_list:
            if not isinstance(m, dict):
                continue
            key_id = m.get("metric_id")
            key_name = m.get("name")
            if key_id and key_id in score_map_id:
                m["score"] = score_map_id[key_id]["score"]
                m["opinion"] = score_map_id[key_id]["opinion"]
                matched_ids.add(key_id)
            elif key_name and key_name in score_map_name:
                m["score"] = score_map_name[key_name]["score"]
                m["opinion"] = score_map_name[key_name]["opinion"]
                matched_names.add(key_name)

        # ä¸å†è¿½åŠ æ–°è®°å½•ï¼šä»…æ›´æ–°åŒ¹é…é¡¹ï¼Œä¿æŒåˆå§‹ç»“æ„ç¨³å®š

        # å†™å› mdï¼ˆä¿æŒ```json å—ï¼‰
        from json import dumps
        new_json = dumps(metrics_list, ensure_ascii=False, indent=2)
        new_text = text[:code_block_match.start(1)] + new_json + text[code_block_match.end(1):]
        path.write_text(new_text, encoding="utf-8")

    async def _evaluate_single_metric(self, metric: dict, vector_store_path: str) -> tuple:
        """ç»Ÿä¸€çš„é…ç½®é©±åŠ¨è¯„ä»·ï¼šæ ¹æ®è¯„ä»·ç±»å‹ä»é…ç½®è¯»å–æ¨¡æ¿å¹¶æ‰§è¡Œ"""
        metric_name = metric.get("name", "æœªçŸ¥æŒ‡æ ‡")
        evaluation_type = metric.get("evaluation_type", "")
        evaluation_points = metric.get("evaluation_points", "")
        scoring_method = metric.get("scoring_method", "")

        logger.info(f"ğŸ” å¼€å§‹è¯„ä»·æŒ‡æ ‡: {metric_name} (ç±»å‹: {evaluation_type})")

        # RAGæ£€ç´¢äº‹å®
        facts = await self._retrieve_metric_facts(metric_name, vector_store_path)
        logger.info(f"ğŸ“š æ£€ç´¢åˆ°äº‹å®ä¾æ®: {len(facts)} å­—ç¬¦")

        # ä»é…ç½®è·å–è¯¥è¯„ä»·ç±»å‹çš„è¯¦ç»†è¯´æ˜
        eval_cfg = ENV_EVALUATION_TYPES.get(evaluation_type, {})
        type_description = eval_cfg.get("description", "")
        scoring_guidance = eval_cfg.get("scoring_guidance", "")
        opinion_requirements = eval_cfg.get("opinion_requirements", "")

        logger.info(f"ğŸ“‹ è¯„ä»·ç±»å‹é…ç½®: {type_description}")
        logger.info(f"ğŸ“ è¯„ä»·æ„è§è¦æ±‚: {len(opinion_requirements)} å­—ç¬¦")

        # æŒ‡æ ‡çº§æç¤ºè¯ç»„åˆè§„èŒƒ
        spec_default = ENV_METRIC_PROMPT_SPEC.get('default', {})
        spec_type = ENV_METRIC_PROMPT_SPEC.get('by_evaluation_type', {}).get(evaluation_type, {})
        points_intro = spec_type.get('points_intro', spec_default.get('points_intro', 'è¯„ä»·è¦ç‚¹ï¼š'))
        point_bullet = spec_type.get('point_bullet', spec_default.get('point_bullet', 'â‘  '))
        scoring_method_intro = spec_type.get('scoring_method_intro', spec_default.get('scoring_method_intro', 'è¯„åˆ†æ–¹æ³•ï¼š'))
        max_points = int(spec_default.get('max_points', 10))

        # è§„èŒƒåŒ–è¯„ä»·è¦ç‚¹ - evaluation_pointsç°åœ¨æ˜¯åŒ…å«è¯„ä»·è¦ç´ å’Œè®¡åˆ†è§„åˆ™çš„æ–‡æœ¬
        if evaluation_points:
            # ç›´æ¥ä½¿ç”¨evaluation_pointsæ–‡æœ¬ï¼Œå› ä¸ºå®ƒå·²ç»åŒ…å«äº†å®Œæ•´çš„è¯„ä»·è¦ç´ å’Œè®¡åˆ†è§„åˆ™
            points_str = points_intro + "\n" + evaluation_points
            logger.info(f"ğŸ“Š è¯„ä»·è¦ç‚¹æ ¼å¼åŒ–å®Œæˆ: {len(points_str)} å­—ç¬¦")
        else:
            points_str = points_intro + " æ— "
            logger.warning("âš ï¸ è¯„ä»·è¦ç‚¹ä¸ºç©º")

        # è§„èŒƒåŒ–è¯„åˆ†æ–¹æ³• - å¦‚æœscoring_methodä¸ºç©ºï¼Œå¯ä»¥ä»evaluation_pointsä¸­æå–
        if scoring_method:
            scoring_method = f"{scoring_method_intro}{scoring_method}"
        else:
            # å¦‚æœscoring_methodä¸ºç©ºï¼Œå°è¯•ä»evaluation_pointsä¸­æå–è®¡åˆ†è§„åˆ™
            if "å¾—åˆ†" in evaluation_points or "åˆ†å€¼" in evaluation_points:
                scoring_method = f"{scoring_method_intro}è®¡åˆ†è§„åˆ™å·²åœ¨è¯„ä»·è¦ç‚¹ä¸­æ˜ç¡®"
            else:
                scoring_method = f"{scoring_method_intro}æ— "

        if ENV_EVALUATOR_PROMPT_TEMPLATE:
            prompt = ENV_EVALUATOR_PROMPT_TEMPLATE.format(
                evaluation_type=evaluation_type or "æ ‡å‡†è¯„ä»·",
                evaluation_points=points_str,
                facts=facts,
                scoring_method=scoring_method,
                type_description=type_description,
                scoring_guidance=scoring_guidance,
                opinion_requirements=opinion_requirements,
            )
            logger.info(f"ğŸ“ æç¤ºè¯æ¨¡æ¿ç”Ÿæˆå®Œæˆ: {len(prompt)} å­—ç¬¦")
        else:
            # å…œåº•æ¨¡æ¿
            prompt = f"è¯·åŸºäºä»¥ä¸‹äº‹å®ï¼ŒæŒ‰{evaluation_type or 'æ ‡å‡†è¯„ä»·'}è¿›è¡Œè¯„åˆ†ã€‚\n\nè¦ç‚¹:\n{points_str}\n\näº‹å®:\n{facts}\n\nè¯„åˆ†æ–¹æ³•:{scoring_method}"
            logger.warning("âš ï¸ ä½¿ç”¨å…œåº•æç¤ºè¯æ¨¡æ¿")

        try:
            project_info_text = get_project_info_text()
            logger.info("ğŸ¤– å¼€å§‹è°ƒç”¨LLMè¿›è¡Œè¯„åˆ†...")
            result = await self._aask(prompt, [ENV_EVALUATOR_BASE_SYSTEM, project_info_text])
            logger.info(f"ğŸ¤– LLMå“åº”å®Œæˆ: {len(result)} å­—ç¬¦")
            
            # è§£æJSONå“åº”
            parsed = extract_json_from_llm_response(result)
            logger.info(f"ğŸ” JSONè§£æç»“æœç±»å‹: {type(parsed)}")
            
            # åªæ¥å—å•å¯¹è±¡ï¼›è‹¥è¿”å›åˆ—è¡¨/åŒ…è£¹ï¼Œåˆ™åˆ¤å®šä¸ºéæ³•ï¼Œç›´æ¥ç½®0å¹¶å›å†™è§£æå¤±è´¥è¯´æ˜
            if isinstance(parsed, dict):
                item = parsed
                logger.info(f"âœ… æˆåŠŸè§£æJSONå¯¹è±¡ï¼ŒåŒ…å«é”®: {list(item.keys())}")
            else:
                raise ValueError(f"è¯„åˆ†ç»“æœå¿…é¡»ä¸ºå•ä¸ªJSONå¯¹è±¡ï¼Œä¸”ä»…åŒ…å«score/opinion ä¸¤ä¸ªé”®ï¼Œå®é™…ç±»å‹: {type(parsed)}")
            
            score_val = item.get("score")
            if not isinstance(score_val, (int, float)):
                raise ValueError(f"scoreå¿…é¡»ä¸ºæ•°å€¼(0-100)ï¼Œç¦æ­¢å­—ç¬¦ä¸²/ç™¾åˆ†å·/æ–‡å­—ï¼Œå®é™…å€¼: {score_val}")
            
            score = float(score_val)
            opinion = item.get("opinion") or ""
            
            logger.info(f"âœ… æŒ‡æ ‡è¯„åˆ†æˆåŠŸ: {metric_name} - {score}åˆ†")
            logger.info(f"ğŸ“ è¯„ä»·æ„è§: {opinion[:100]}...")
            
            return score, opinion
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€è¯„ä»·å¤±è´¥: {e}")
            logger.error(f"ğŸ“ åŸå§‹LLMå“åº”: {result[:200]}...")
            return 0, f"ç»Ÿä¸€è¯„ä»·è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"

    async def _retrieve_metric_facts(self, metric_name: str, vector_store_path: str) -> str:
        """ğŸ§  ä½¿ç”¨æ™ºèƒ½æ£€ç´¢æœåŠ¡ä¸ºæŒ‡æ ‡æ£€ç´¢ç›¸å…³äº‹å®ä¾æ®"""
        try:
            from backend.services.intelligent_search import intelligent_search

            primary_query = f"{metric_name} çš„å…·ä½“æ•°æ®ã€å®Œæˆæƒ…å†µå’Œå®æ–½æ•ˆæœ"

            search_result = await intelligent_search.intelligent_search(
                query=primary_query,
                project_vector_storage_path=vector_store_path,
                mode="hybrid",
                enable_global=True,
                max_results=5,
            )

            if search_result.get("results"):
                facts = "\n\n".join(search_result["results"])
                if search_result.get("insights"):
                    facts += "\n\nğŸ’¡ æ™ºèƒ½åˆ†æ:\n" + "\n".join(search_result["insights"])
                return facts
            return f"æœªèƒ½æ£€ç´¢åˆ°å…³äº'{metric_name}'çš„ç›¸å…³äº‹å®ä¾æ®ã€‚"
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ£€ç´¢æŒ‡æ ‡äº‹å®å¤±è´¥: {e}")
            return f"æ£€ç´¢å¤±è´¥ï¼Œæ— æ³•è·å–å…³äº'{metric_name}'çš„äº‹å®ä¾æ®ã€‚"


